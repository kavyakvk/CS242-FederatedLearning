{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":" Eric-Kavya-Jazz-CS242-Final Project_3 Archs New.ipynb","provenance":[{"file_id":"1y3uHf2wzC8Hl2Ya485o32mFwOrbmcWlJ","timestamp":1589074792462},{"file_id":"1Ocy7sJRiA9z9KdtIgKuRYHkHLum3Z-o-","timestamp":1589058415677},{"file_id":"https://github.com/kavyakvk/CS242-FederatedLearning/blob/master/Eric_Kavya_Jazz_CS242_Assignment_2_copy.ipynb","timestamp":1587496718015}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"1fb5479b83494b0e94e0f826ebe97fa0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_1a66812d274d45aa9efa4e879be7a183","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_2a46b32e087a447e9d10e1dfc9e4e879","IPY_MODEL_9a39e283c1254efc9c9a71659c54e7e9"]}},"1a66812d274d45aa9efa4e879be7a183":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2a46b32e087a447e9d10e1dfc9e4e879":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_bf6930b97ae34335881898b7181503b8","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_501cfc369c4142b689b07ee9f62c4c29"}},"9a39e283c1254efc9c9a71659c54e7e9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_29f12b49edd84b4aab3936b548eefcb7","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"â€‹","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 170500096/? [00:08&lt;00:00, 19355481.37it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ab1f9bcf67d84a738182f4d13ff656f2"}},"bf6930b97ae34335881898b7181503b8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"501cfc369c4142b689b07ee9f62c4c29":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"29f12b49edd84b4aab3936b548eefcb7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"ab1f9bcf67d84a738182f4d13ff656f2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"Y7ttBYeclcLI","colab_type":"text"},"source":["# CS242: Final Project\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"mQGm6JtgehIq","colab_type":"text"},"source":["> Harvard CS 242: Computing at Scale (Spring 2020)\n","> \n","> Instructor: Professor HT Kung\n",">\n","> Students: Kavya Kopparapu, Eric Lin, Jazz Zhao\n"]},{"cell_type":"markdown","metadata":{"id":"0UvFA89jTuON","colab_type":"text"},"source":["---\n","\n","### **1. General Setup Code**\n","\n","---\n","Define the dataset (CIFAR and MNIST) as well as the standard net we will be using for training."]},{"cell_type":"code","metadata":{"id":"s9WL6HA_Lpe8","colab_type":"code","outputId":"01765a1c-4d41-4d35-c422-85f2a193f2d0","executionInfo":{"status":"ok","timestamp":1589213614657,"user_tz":240,"elapsed":12493,"user":{"displayName":"Eric Lin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiJSVpS7WgbdsLtjFx4e5l_qZVXmPqY9C-tx5Vk=s64","userId":"12236039700078085648"}},"colab":{"base_uri":"https://localhost:8080/","height":117,"referenced_widgets":["1fb5479b83494b0e94e0f826ebe97fa0","1a66812d274d45aa9efa4e879be7a183","2a46b32e087a447e9d10e1dfc9e4e879","9a39e283c1254efc9c9a71659c54e7e9","bf6930b97ae34335881898b7181503b8","501cfc369c4142b689b07ee9f62c4c29","29f12b49edd84b4aab3936b548eefcb7","ab1f9bcf67d84a738182f4d13ff656f2"]}},"source":["## Code Cell 1.1\n","\n","import time\n","import copy\n","import sys\n","from collections import OrderedDict\n","\n","import torch\n","import torch.nn as nn\n","from torch import optim\n","import torch.nn.functional as F\n","import numpy as np\n","import torchvision\n","import torchvision.transforms as transforms\n","from torch.utils.data.sampler import SubsetRandomSampler\n","import matplotlib.pyplot as plt\n","\n","#Using MNIST\n","#dataset = datasets.MNIST(root='./data')\n","#idx = dataset.train_labels==1\n","#dataset.train_labels = dataset.train_labels[idx]\n","#dataset.train_data = dataset.train_data[idx]\n","\n","# Using CIFAR-10\n","# Load training data\n","transform_train = transforms.Compose([                                   \n","    transforms.RandomCrop(32, padding=4),                                       \n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","])\n","trainset = torchvision.datasets.CIFAR10(root='./data', train=True, \n","                                        download=True,\n","                                        transform=transform_train)\n","#Edit and Load validation data\n","validset = torchvision.datasets.CIFAR10(root='./data', train=True, \n","                                        download=True,\n","                                        transform=transform_train)\n","valid_size = 0.2\n","indices = list(range(len(trainset)))\n","split = int(np.floor(valid_size * len(trainset)))\n","    \n","if True:\n","    np.random.shuffle(indices)\n","\n","train_idx, valid_idx = indices[split:], indices[:split]\n","train_sampler = SubsetRandomSampler(train_idx)\n","valid_sampler = SubsetRandomSampler(valid_idx)\n","\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, \n","                                            sampler=valid_sampler, shuffle=False,\n","                                            num_workers=2)\n","validloader = torch.utils.data.DataLoader(validset, batch_size=128, \n","                                            sampler=valid_sampler, shuffle=False,\n","                                            num_workers=2)\n","# Load testing data\n","transform_test = transforms.Compose([                                           \n","    transforms.ToTensor(),\n","    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","])\n","testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n","                                       download=True,\n","                                       transform=transform_test)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False,\n","                                         num_workers=2)\n","\n","\n","# Using same ConvNet as in Assignment 1\n","def conv_block(in_channels, out_channels, kernel_size=3, stride=1,\n","               padding=1):\n","    return nn.Sequential(\n","        nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding,\n","                  bias=False),\n","        nn.BatchNorm2d(out_channels),\n","        nn.ReLU(inplace=True)\n","        )\n","\n","class ConvNet(nn.Module):\n","    def __init__(self):\n","        super(ConvNet, self).__init__()\n","        self.model = nn.Sequential(\n","            conv_block(3, 32),\n","            conv_block(32, 32),\n","            conv_block(32, 64, stride=2),\n","            conv_block(64, 64),\n","            conv_block(64, 64),\n","            conv_block(64, 128, stride=2),\n","            conv_block(128, 128),\n","            conv_block(128, 256),\n","            conv_block(256, 256),\n","            nn.AdaptiveAvgPool2d(1)\n","            )\n","\n","        self.classifier = nn.Linear(256, 10)\n","\n","    def forward(self, x):\n","        h = self.model(x)\n","        B, C, _, _ = h.shape\n","        h = h.view(B, C)\n","        return self.classifier(h)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1fb5479b83494b0e94e0f826ebe97fa0","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Extracting ./data/cifar-10-python.tar.gz to ./data\n","Files already downloaded and verified\n","Files already downloaded and verified\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wjiB2IYp7B25","colab_type":"text"},"source":["**Device Class and Train/Test Methods**\n","\n","---"]},{"cell_type":"code","metadata":{"id":"zSx1GxV2j0iI","colab_type":"code","colab":{}},"source":["## Code Cell 1.2\n","import statistics \n","\n","class DatasetSplit(torch.utils.data.Dataset):\n","    def __init__(self, dataset, idxs):\n","        self.dataset = dataset\n","        self.idxs = [int(i) for i in idxs]\n","\n","    def __len__(self):\n","        return len(self.idxs)\n","\n","    def __getitem__(self, item):\n","        image, label = self.dataset[self.idxs[item]]\n","        return image, torch.tensor(label)\n","\n","class Device():\n","    def __init__(self, net, device_id, trainset, validset, testset, train_idxs, valid_idxs, test_idxs, bias, archetype, lr=0.1,\n","                      milestones=None, batch_size=128):\n","        if milestones == None:\n","            milestones = [25, 50, 75]\n","\n","        device_net = copy.deepcopy(net)\n","        optimizer = torch.optim.SGD(device_net.parameters(), lr=lr, momentum=0.9,\n","                                    weight_decay=5e-4)\n","        scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,\n","                                                        milestones=milestones,\n","                                                        gamma=0.1)\n","        self.device_trainset = DatasetSplit(trainset, train_idxs)\n","        self.trainloader = torch.utils.data.DataLoader(self.device_trainset,\n","                                                        batch_size=batch_size,\n","                                                        shuffle=True,\n","                                                        num_workers=2)\n","        self.device_validset = DatasetSplit(validset, valid_idxs)\n","        self.validloader = torch.utils.data.DataLoader(self.device_validset,\n","                                                        batch_size=batch_size,\n","                                                        shuffle=True,\n","                                                        num_workers=2)\n","        self.device_testset = DatasetSplit(testset, test_idxs)\n","        self.testloader = torch.utils.data.DataLoader(self.device_testset,\n","                                                        batch_size=batch_size,\n","                                                        shuffle=True,\n","                                                        num_workers=2)\n","        self.nets = []\n","        self.idx = device_id\n","        self.ranking = [1.]\n","        self.active = [1] #either 1 or 0, depending on whether we got rid of it or not\n","        self.nets.append({\n","            'net': device_net,\n","            # 'id': device_id,\n","            # 'dataloader': device_trainloader, \n","            'optimizer': optimizer,\n","            'scheduler': scheduler,\n","            'train_loss_tracker': [],\n","            'train_acc_tracker': [],\n","            'valid_loss_tracker': [],\n","            'valid_acc_tracker': [],\n","            'test_loss_tracker': [],\n","            'test_acc_tracker': [],\n","        })\n","\n","        # Bias and archetype parameters\n","        self.bias = bias              # Number between 0 and 1 to represent linear comb. of archetypes\n","        self.archetype = archetype    # An array of possible archetypes\n","    \n","    def update_ranking(self, removed=False, duplicate_model_id=-1, offset_rank=-1):\n","        #print(\"updating for device \", self.idx)\n","        zero_threshold = 1 #number of standard deviations away for model deletion cutoff\n","\n","        if len(self.nets) > 1:\n","            #print(\"ranking 1 in the update_ranking method: \", self.ranking)\n","            metrics = []\n","            for i in range(len(self.nets)):\n","                if(len(self.nets[i]['valid_acc_tracker']) > 0):\n","                    rank = self.nets[i]['valid_acc_tracker'][-1]\n","                    if(len(self.nets[i]['valid_acc_tracker']) >= 3):\n","                        rank = (self.nets[i]['valid_acc_tracker'][-1]+self.nets[i]['valid_acc_tracker'][-2]+self.nets[i]['valid_acc_tracker'][-3])/3\n","                    if duplicate_model_id == i and offset_rank != -1:\n","                        rank = offset_rank      # Heavily rank the devices that are underperforming for new models and vice versa\n","                    if rank == 0:\n","                        rank += 0.001\n","                    metrics.append(rank)\n","                else:\n","                    metrics.append(50)\n","            \n","            #if we added more models, add active trackers for them\n","            while(len(self.nets) != len(self.active)):\n","                self.active.append(1)\n","            if removed:       # Auto-set a model as inactive if it was already removed\n","                self.active[duplicate_model_id] = 0\n","            \n","\n","            # normalization first time (with self.active)\n","            self.ranking = [metrics[i]*self.active[i]/sum(metrics) for i in range(len(metrics))]\n","\n","            \n","            #print(\"ranking 2 in the update_ranking method: \", self.ranking)\n","\n","            nonzero_elts = np.array(self.active).nonzero()[0]\n","            nonzero_arr = []\n","            for i in nonzero_elts:\n","                nonzero_arr.append(self.ranking[i])\n","\n","            # Remove models that are underperforming\n","            if offset_rank == -1:   # only remove if not duplicating round\n","                max_rank = max(self.ranking)\n","                if len(nonzero_elts) > 1:\n","                    std = statistics.stdev(nonzero_arr)\n","                    mean = sum(nonzero_arr)/len(nonzero_arr)\n","                    # remove models that are underperforming\n","                    for j in range(len(self.ranking)):\n","                        if self.active[j] != 0:\n","                            if(mean - self.ranking[j] > zero_threshold*std):\n","                                self.active[j] = 0\n","                                self.ranking[j] = 0\n","                            elif( len(self.ranking) > 3 and (self.ranking[j] * 10 < max_rank)):\n","                                self.active[j] = 0\n","                                self.ranking[j] = 0\n","\n","            bool_ranking_below_zero = False\n","            # Add noise\n","            # noise = random.gauss(0, statistics.stdev(self.ranking))\n","            noise = random.gauss(0, 0.01)\n","            if len(nonzero_elts) == 1:\n","                i = nonzero_elts[0]\n","            else:\n","                i = nonzero_elts[random.randint(0, len(nonzero_elts)-1)]\n","                for j in range(len(self.ranking)):\n","                    if(j != i):\n","                        self.ranking[j] -= noise/(len(nonzero_elts)-1)\n","                        if self.ranking[j] < 0:\n","                          bool_ranking_below_zero = True\n","            \n","            self.ranking[i] += noise\n","            if self.ranking[i] < 0:\n","                bool_ranking_below_zero = True\n","\n","            # for i in range(len(self.ranking)):\n","                # noise = random.gauss(0, 0.2)\n","                # self.ranking[i] += noise\n","                # self.ranking[i] = max(0, self.ranking[i])\n","\n","            # Normalize again\n","            if(bool_ranking_below_zero):\n","                self.ranking = [self.ranking[i]-min(self.ranking) for i in range(len(self.ranking))]\n","            self.ranking = [self.ranking[i]*self.active[i]/sum(self.ranking) for i in range(len(self.ranking))]\n","            #print(\"ranking 3 in the update_ranking method: \", self.ranking)\n","            \n","\n","def create_devices(net, trainset, validset, testset, train_idxs, valid_idxs, test_idxs, bias, archetype, lr=0.1,\n","                  milestones=None, batch_size=128, num_devices=2):\n","    devices_lst = [Device(net, i, trainset, validset, testset, train_idxs[i], valid_idxs[i], test_idxs[i], bias[i], archetype[i], lr,\n","                  milestones, batch_size) for i in range(num_devices)]\n","    return devices_lst\n","      \n","  \n","def train(epoch, device, model_id):\n","    device.nets[model_id]['net'].train()\n","    train_loss, correct, total = 0, 0, 0\n","\n","    dataset = device.device_trainset\n","    dataloader = device.trainloader\n","\n","    for batch_idx, (inputs, targets) in enumerate(dataloader):\n","        inputs, targets = inputs.cuda(), targets.cuda()\n","        device.nets[model_id]['optimizer'].zero_grad()\n","        outputs = device.nets[model_id]['net'](inputs)\n","        loss = criterion(outputs, targets)\n","        loss.backward()\n","        device.nets[model_id]['optimizer'].step()\n","        train_loss += loss.item()\n","        device.nets[model_id]['train_loss_tracker'].append(loss.item())\n","        loss = train_loss / (batch_idx + 1)\n","        _, predicted = outputs.max(1)\n","        total += targets.size(0)\n","        correct += predicted.eq(targets).sum().item()\n","        acc = 100. * correct / total\n","        dev_id = device.idx\n","        #if epoch == local_epochs - 1:\n","            #sys.stdout.write(f'\\r(Device {dev_id}/Epoch {epoch}) ' + \n","            #                f'Train Loss: {loss:.3f} | Train Acc: {acc:.3f}')\n","            #sys.stdout.flush()\n","    test_loss = 0\n","    outputs = [0]\n","    device.nets[model_id]['train_acc_tracker'].append(acc)\n","    #sys.stdout.flush()\n","\n","def validate(epoch, device, model_id):\n","    device.nets[model_id]['net'].eval()\n","    test_loss, correct, total = 0, 0, 0\n","\n","    dataset = device.device_validset\n","    dataloader = device.validloader\n","    with torch.no_grad():\n","        for batch_idx, (inputs, targets) in enumerate(dataloader):\n","            inputs, targets = inputs.cuda(), targets.cuda()\n","            outputs = device.nets[model_id]['net'](inputs)\n","            loss = criterion(outputs, targets)\n","            test_loss += loss.item()\n","            device.nets[model_id]['valid_loss_tracker'].append(loss.item())\n","            _, predicted = outputs.max(1)\n","            total += targets.size(0)\n","            correct += predicted.eq(targets).sum().item()\n","            loss = test_loss / (batch_idx + 1)\n","            acc = 100.* correct / total\n","        test_loss = 0\n","        outputs = [0]\n","    # if epoch == local_epochs - 1:\n","    #     print(\"Device: \" + str(device.idx) + \" VALIDATION: model_id # \" + str(model_id))\n","    #     sys.stdout.write(f' | Valid Loss: {loss:.3f} | Valid Acc: {acc:.3f}\\n')\n","    #     sys.stdout.flush()  \n","    acc = 100.*correct/total\n","    device.nets[model_id]['valid_acc_tracker'].append(acc)\n","    device.nets[model_id]['net'].train()\n","\n","def test(epoch, device, model_id, dataset, dataloader):\n","    criterion = nn.CrossEntropyLoss()\n","\n","    device.nets[model_id]['net'].eval()\n","    test_loss, correct, total = 0, 0, 0\n","\n","    with torch.no_grad():\n","        for batch_idx, (inputs, targets) in enumerate(dataloader):\n","            inputs, targets = inputs.cuda(), targets.cuda()\n","            outputs = device.nets[model_id]['net'](inputs)\n","            loss = criterion(outputs, targets)\n","            test_loss += loss.item()\n","\n","            # print(\"test\", loss.item(), targets, batch_idx)\n","            \n","            device.nets[model_id]['test_loss_tracker'].append(loss.item())\n","            _, predicted = outputs.max(1)\n","            total += targets.size(0)\n","            correct += predicted.eq(targets).sum().item()\n","            loss = test_loss / (batch_idx + 1)\n","            #print('loss:', loss)\n","            acc = 100.* correct / total\n","        test_loss = 0\n","        outputs = [0]\n","    # sys.stdout.write(f' | Test Loss: {loss:.3f} | Test Acc: {acc:.3f}\\n')\n","    # sys.stdout.flush()  \n","    acc = 100.*correct/total\n","    device.nets[model_id]['test_acc_tracker'].append(acc)\n","\n","    device.nets[model_id]['net'].train()\n","    return ('%.3f' % loss, '%.3f' % acc) "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6DwoSIi0ftBW","colab_type":"code","colab":{}},"source":["## Code Cell 1.3\n","import random #to use the random.sample method\n","\n","def iid_sampler(dataset, num_devices, data_pct):\n","    '''\n","    dataset: PyTorch Dataset (e.g., CIFAR-10 training set)\n","    num_devices: integer number of devices to create subsets for\n","    data_pct: percentage of training samples to give each device\n","              e.g., 0.1 represents 10%\n","\n","    return: a dictionary of the following format:\n","      {\n","        0: [3, 65, 2233, ..., 22] // device 0 sample indexes\n","        1: [0, 2, 4, ..., 583] // device 1 sample indexes\n","        ...\n","      }\n","\n","    iid (independent and identically distributed) means that the indexes\n","    should be drawn independently in a uniformly random fashion.\n","    '''\n","\n","    # total number of samples in the dataset\n","    total_samples = len(dataset)\n","\n","    # Part 1.1: Implement!\n","    arr = [i for i in range(total_samples)] #create an arrray of length total_samples\n","    d = {} #initialize the dictonary\n","    for i in range(num_devices): #for every device\n","        d[i] = random.sample(list(arr), k=round(data_pct*total_samples)) #select data_pct*total_samples from the array, without replacement\n","    return d #return the dictionary"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4waD2ZlQI4X2","colab_type":"text"},"source":["**Implementing Components for Federated Learning**\n","\n","---"]},{"cell_type":"code","metadata":{"id":"6X0fTWKg6hBY","colab_type":"code","colab":{}},"source":["## Code Cell 1.5\n","\n","def model_average_weight(devices, model_id):\n","    '''\n","    devices: a list of devices generated by create_devices\n","    Returns an the average of the weights.\n","    '''\n","    d_id = 0\n","    while(d_id < len(devices) and devices[d_id].active[model_id] == 0):\n","        d_id += 1\n","\n","    if(d_id >= len(devices)):\n","        return None\n","        \n","    global_tensors = copy.deepcopy(devices[d_id].nets[model_id]['net'].state_dict()) #initialize a global tensor with the weights of the first device\n","    ranking_sum = devices[d_id].ranking[model_id]\n","\n","    for i in range(0, len(devices)):#iterate over the remaining devices\n","        if(i == d_id):\n","            for j in global_tensors.keys(): #add the tensors together by the key they are indexed by\n","                global_tensors[j] = global_tensors[j]*devices[d_id].ranking[model_id]\n","        if(devices[i].active[model_id] == 1):\n","            #for easy/ less complicated referencing, store the device and the state_dict\n","            d = devices[i]\n","            d_tensors = d.nets[model_id]['net'].state_dict()\n","            for j in global_tensors.keys(): #add the tensors together by the key they are indexed by\n","                global_tensors[j] += d_tensors[j]*d.ranking[model_id]\n","            ranking_sum += devices[i].ranking[model_id]\n","\n","    for j in global_tensors.keys(): #average each tensor by the number of devices\n","        global_tensors[j] = global_tensors[j]/ranking_sum\n","    return global_tensors #return the averaged weights\n","\n","\n","\n","def get_devices_for_round(devices, device_pct):\n","    '''\n","    This function will select a percentage of devices to participate in each training round.\n","    '''\n","    # Part 1.2: Implement!\n","    #randomly choose device_pct*len(devices) devices from the devices array without replacement\n","    arr = random.sample(devices, k=round(device_pct*len(devices)))\n","    return arr"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0KNmDbPh5b0h","colab_type":"text"},"source":["---\n","\n","### **2. Non-IID Testing and Archetype Definition Code**\n","\n","---\n"]},{"cell_type":"markdown","metadata":{"id":"-yUOGnkZCbp-","colab_type":"text"},"source":["**Non-iid Sampling**\n","\n","---"]},{"cell_type":"code","metadata":{"id":"VajsGz0wiL05","colab_type":"code","colab":{}},"source":["## Code Cell 2.1\n","\n","# creates noniid TRAINING and VALIDATION datasets for each group\n","def noniid_group_sampler(dataset, num_items_per_device, archetype, bias):\n","    '''\n","    dataset: PyTorch Dataset (e.g., CIFAR-10 training set)\n","    num_items_per_device: how many samples to assign to each device\n","    archetype: a dictionary of arrays representing the labels that is predominantly represented by this edge device\n","        device index -> array of archetypes\n","    bias: a dictionary of the percent of samples that are represented by the archetype\n","        device index -> value from 0 to 1\n","\n","    return: a dictionary of the following format:\n","      {\n","        0: [3, 65, 2233, ..., 22] // device 0 sample indexes\n","        1: [0, 2, 4, ..., 583] // device 1 sample indexes\n","        ...\n","      }\n","\n","    '''\n","    #label dict stores the indexes of the dataset examples that fall into the ith group of CIFAR\n","    label_dict = {}\n","    for i in range(0, 10): #assuming CIFAR, which has labels 0-9\n","        label_dict[i] = []\n","    for i in range(len(dataset)):\n","        label = dataset[i][1]\n","        label_dict[label].append(i)\n","    \n","    num_devices = len(archetype)\n","\n","    final_dict = {} #final dict is to be returned\n","    for i in range(num_devices):\n","        bias_group = []\n","        not_bias_group = []\n","        # archs = [0,1,2,3,4,5] #6 archetypes   CHANGEHERE\n","        archs = [0,1,2]\n","\n","        archs.pop(archetype[i][0])\n","        for j in label_dict.keys():\n","            if(j in archetype[i]):\n","                bias_group += label_dict[j]\n","            #else:\n","            if(j in archs):\n","                not_bias_group += label_dict[j]\n","\n","\n","\n","        # for j in label_dict.keys(): CHANGEHERE\n","        #     if(j in archetype[i]):\n","        #         bias_group += label_dict[j]\n","            #else:\n","            # if(archetype[i][0] in [0,1,2]): #two meta-archetypes   CHANGEHERE\n","            #     if(j in [0,1,2] and j != archetype[i][0]):\n","            #         not_bias_group += label_dict[j]\n","            # elif(archetype[i][0] in [3,4,5]): #two meta-archetypes\n","            #     if(j in [3,4,5] and j != archetype[i][0]):\n","            #         not_bias_group += label_dict[j]\n","\n","        exs = random.sample(bias_group, int(num_items_per_device*bias[i]))\n","        exs.extend(random.sample(not_bias_group, num_items_per_device-int(num_items_per_device*bias[i])))\n","        random.shuffle(exs)\n","        final_dict[i] = exs\n","    return final_dict"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JBHBZVehNZNI","colab_type":"text"},"source":["---\n","**Group-based Testing**\n","\n"]},{"cell_type":"code","metadata":{"id":"99S3opJONpeW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"f58bbd72-c2fa-403f-e660-1c882ac3771c","executionInfo":{"status":"ok","timestamp":1589213614894,"user_tz":240,"elapsed":12704,"user":{"displayName":"Eric Lin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiJSVpS7WgbdsLtjFx4e5l_qZVXmPqY9C-tx5Vk=s64","userId":"12236039700078085648"}}},"source":["## Code Cell 2.3\n","\n","# creates noniid TEST datasets for each group\n","def cifar_noniid_group_test(dataset):\n","\n","    #label dict stores the indexes of the dataset examples that fall into the ith group of CIFAR\n","    label_dict = {}\n","    for i in range(0, 10): #assuming CIFAR, which has labels 0-9, change later\n","        label_dict[i] = []\n","    for i in range(len(dataset)):\n","        label = dataset[i][1]\n","        label_dict[label].append(i)\n","    return label_dict\n","\n","# gets per-group accuracy of global model\n","def test_group(epoch, device, model_id, label_dict, dataset = testset):\n","    \n","    net = device.nets[model_id]['net']\n","    net.eval() #turn the net into evaluaton mode\n","    # sys.stdout.write(' | accuracy: ')\n","    with torch.no_grad():\n","        #for group in label_dict.keys(): \n","        # for group in [0,1,2,3,4,5]: #6 archetypes\n","        for group in [0,1,2]:\n","            test_loss, correct, total = 0, 0, 0\n","            new_dataset = DatasetSplit(dataset, label_dict[group])\n","            dataloader = torch.utils.data.DataLoader(new_dataset, batch_size=128, shuffle=False,\n","                                            num_workers=2)\n","            for batch_idx, (inputs, targets) in enumerate(dataloader):\n","                inputs, targets = inputs.cuda(), targets.cuda()\n","                outputs = net(inputs)\n","                loss = criterion(outputs, targets)\n","                test_loss += loss.item()\n","                #print(\"test_group\", loss.item())\n","                _, predicted = outputs.max(1)\n","                total += targets.size(0)\n","                correct += predicted.eq(targets).sum().item()\n","            # Compute and print loss and accuracy at the end of the group\n","            loss = test_loss / (batch_idx + 1)\n","            acc = 100.* correct / total\n","            # sys.stdout.write(f'{acc:.3f} | ')\n","\n","            outputs = [0]\n","            test_loss = 0\n","    net.train()\n","    # sys.stdout.write('\\n')\n","    sys.stdout.flush()  \n"],"execution_count":7,"outputs":[{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"d9KiAYyHS8VA","colab_type":"code","outputId":"f665e884-6bac-4153-c2d8-d8dc8a109e17","executionInfo":{"status":"ok","timestamp":1589213614894,"user_tz":240,"elapsed":12698,"user":{"displayName":"Eric Lin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiJSVpS7WgbdsLtjFx4e5l_qZVXmPqY9C-tx5Vk=s64","userId":"12236039700078085648"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["## Code Cell 3.1\n","\n","def quantizer(input, nbit):\n","    '''\n","    input: full precision tensor in the range [0, 1]\n","    return: quantized tensor\n","    '''\n","    scale_factor = 1 / (2**nbit -  1)\n","\n","    # scale input by inverse of scale_factor and round to nearest integer\n","    output = input / scale_factor\n","    output = torch.round(output)\n","\n","    # scale rounded output back and return\n","    output *= scale_factor\n","    return output\n","\n","# Test Code\n","test_data = torch.tensor([i/11 for i in range(11)])\n","\n","# ground truth results of 4-bit quantization\n","ground_truth = torch.tensor([0.0000, 0.0667, 0.2000, 0.2667, 0.3333, 0.4667,\n","                             0.5333, 0.6667, 0.7333, 0.8000, 0.9333])\n","\n","# output of your quantization function\n","quantizer_output = quantizer(test_data, 4)\n","\n","if torch.allclose(quantizer_output, ground_truth, atol=1e-04):\n","    print('Output of Quantization Matches!')\n","else:\n","    print('Output of Quantization DOES NOT Match!')\n","\n","\n","## Code Cell 3.2\n","\n","def quantize_model(model, nbit):\n","    '''\n","    Used in Code Cell 3.3 to quantize the ConvNet model\n","    '''\n","    for m in model.modules():\n","        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n","            m.weight.data, m.adaptive_scale = dorefa_g(m.weight, nbit)\n","            if m.bias is not None:\n","                m.bias.data,_ = dorefa_g(m.bias, nbit, m.adaptive_scale)\n","\n","def dorefa_g(w, nbit, adaptive_scale=None):\n","    '''\n","    w: a floating-point weight tensor to quantize\n","    nbit: the number of bits in the quantized representation\n","    adaptive_scale: the maximum scale value. if None, it is set to be the\n","                    absolute maximum value in w.\n","    '''\n","    if adaptive_scale is None:\n","        adaptive_scale = torch.max(torch.abs(w))\n","    \n","    # follows equations above\n","    sigma = torch.rand(w.shape) - 0.5\n","    noise = sigma / (2**nbit - 1)\n","    # avoid type errors\n","    noise = noise.type(w.type())\n","    inp = w / (2*adaptive_scale) + 0.5 + noise\n","    w_q = 2*adaptive_scale * (quantizer(inp, nbit) - 0.5)\n","\n","    return w_q, adaptive_scale\n","\n","\n","# Test Code\n","test_data = torch.tensor([i/11 for i in range(11)])\n","\n","# ground truth results of 4-bit quantization\n","ground_truth = torch.tensor([-0.0606, 0.0606, 0.1818, 0.3030, 0.3030, 0.4242,\n","                             0.5455, 0.5455, 0.7879, 0.7879, 0.9091])\n","\n","# output of your quantization function\n","torch.manual_seed(43)\n","quantizer_output, adaptive_scale = dorefa_g(test_data, 4)\n","\n","if torch.allclose(quantizer_output, ground_truth, atol=1e-04):\n","    print('Output of Quantization Matches!')\n","else:\n","    print('Output of Quantization DOES NOT Match!')"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Output of Quantization Matches!\n","Output of Quantization Matches!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CtkYyNF5Oo9J","colab_type":"text"},"source":["---\n","**Federated Learning Results in Non-IID Setting**"]},{"cell_type":"code","metadata":{"id":"FKogIgMUgE0f","colab_type":"code","colab":{}},"source":["# Train model on each device\n","# Get rankings on each device\n","# Update weights\n","\n","# use these parameters\n","rounds = 45\n","local_epochs = 3\n","num_devices = 9\n","num_labels = 3\n","num_items_per_device = 5000\n","device_pct = 0.5\n","data_pct = 0.1\n","net = ConvNet().cuda()\n","criterion = nn.CrossEntropyLoss()\n","# duplicate_milestones = [5, 15, 25]\n","duplicate_milestones = []\n","\n","devices_archetype = [[i//num_labels] for i in range(num_devices)]\n","devices_bias = [random.uniform(0.75, 0.85) for i in range(num_devices)]\n","\n","#data_idxs = iid_sampler(trainset, num_devices, data_pct) #this is in the uniform case, without archetypes\n","train_idxs = noniid_group_sampler(trainset, num_items_per_device, devices_archetype, devices_bias)\n","valid_idxs = noniid_group_sampler(validset, num_items_per_device // 3, devices_archetype, devices_bias)\n","test_idxs_device = noniid_group_sampler(testset, 500, devices_archetype, devices_bias)\n","\n","label_dict_test = cifar_noniid_group_test(testset)\n","# test_idxs = label_dict_test[0] + label_dict_test[1]\n","test_idxs = []\n","for i in range(0, num_labels):\n","    test_idxs += label_dict_test[i] \n","random.shuffle(test_idxs)\n","\n","arch_testset = DatasetSplit(testset, test_idxs)\n","test_dataloader = torch.utils.data.DataLoader(arch_testset, batch_size=128,\n","                                                shuffle=True, num_workers=2)\n","\n","label_dict_valid = cifar_noniid_group_test(validset)\n","\n","#print(devices_archetype)\n","#print(data_idxs)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Nnf7JCFHUQPI","colab_type":"code","outputId":"7ecd5a6b-a326-4b1a-ecca-43206f9f9d90","executionInfo":{"status":"ok","timestamp":1589216653194,"user_tz":240,"elapsed":858589,"user":{"displayName":"Eric Lin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiJSVpS7WgbdsLtjFx4e5l_qZVXmPqY9C-tx5Vk=s64","userId":"12236039700078085648"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["nbit = 16\n","model_id_lst = [0]\n","\n","## Device creation\n","devices = create_devices(net, trainset, validset, testset, train_idxs, valid_idxs, test_idxs_device, devices_bias, devices_archetype, num_devices=num_devices)\n","print('Devices', len(devices))\n","\n","## NON- IID Federated Learning\n","##\n","##\n","##\n","start_time = time.time()\n","for round_num in range(rounds):\n","  \n","    # Part 1.3: Implement getting devices for each round here\n","    round_devices = get_devices_for_round(devices, device_pct)\n","    # print('Round Devices', round_devices)\n","\n","    # print('--------------Round: ' + str(round_num) + \"-------------\")\n","    \n","    for device in round_devices:\n","        for model_id in model_id_lst:\n","            if(device.active[model_id] != 0):\n","                # Training\n","                for local_epoch in range(local_epochs):\n","                    train(local_epoch, device, model_id) \n","                    # print(\"Device: \" + str(device.idx) + \" VALIDATION: model_id # \" + str(model_id))\n","                    # validate(local_epoch, device, model_id) \n","                    # print()\n","                    # test_group(round_num, device, model_id, label_dict_valid, validset)   \n","                # after training, quantize the learned model\n","                quantize_model(device.nets[model_id]['net'], nbit)\n","        \n","\n","    # Part 1.3: Implement weight averaging here\n","    for model_id in model_id_lst:\n","        w_avg = model_average_weight(round_devices, model_id)\n","\n","        if(w_avg != None):\n","            for device in devices:\n","                if(device.active[model_id]!= 0):\n","                    device.nets[model_id]['net'].load_state_dict(w_avg)\n","                    device.nets[model_id]['optimizer'].zero_grad()\n","                    device.nets[model_id]['optimizer'].step()\n","                    device.nets[model_id]['scheduler'].step()\n","            \n","        # test accuracy with highest ranking model\n","        if((devices[0].active[model_id] == 1) and (devices[0].ranking[model_id] == max(devices[0].ranking))):\n","            print()\n","            # print(\"ALL-TEST ACCURACY\", test(round_num, devices[0], model_id, arch_testset, test_dataloader)[1])\n","            # print(\"ALL-TEST TEST GROUPS ACCURACY\")\n","            # test_group(round_num, devices[0], model_id, label_dict_test)\n","    \n","    # Validation\n","    if round_num not in duplicate_milestones and len(duplicate_milestones) > 0:\n","        for device in round_devices:\n","            for model_id in model_id_lst:\n","                if(device.active[model_id] != 0):\n","                    validate(local_epochs - 1, device, model_id)    # <- there are print statements here\n","            # Figure out rankings here\n","            device.update_ranking()\n","\n","    # Printing IID Test Results\n","    test_iid_results = []\n","    for index in range(len(devices)):\n","        device = devices[index]\n","        max_model = device.ranking.index(max(device.ranking))\n","        test_iid_results.append(float(test(round_num, device, max_model, device.device_testset, device.testloader)[1]))\n","    # print(\"TESTING WITH IID FROM DEVICE:\", test_iid_results)\n","    print([round_num] + [test(round_num, devices[0], model_id, arch_testset, test_dataloader)[1]] + test_iid_results)\n","\n","    # Printing number of active devices per round\n","    # active_arr_tracker = [sum(devices[i].active) for i in range(len(devices))]\n","    # print([round_num] + active_arr_tracker)\n","    \n","    #duplicate all models if reached a milestone\n","    if(round_num in duplicate_milestones):\n","        # Run validation and update rankings for everyone\n","        for device in devices:\n","            for model_id in model_id_lst:\n","                if(device.active[model_id] != 0):\n","                    validate(local_epochs - 1, device, model_id)    # <- there are print statements here\n","            # Figure out rankings here\n","            device.update_ranking()\n","\n","        # Number of nets to duplicate\n","        nets_to_create = len(model_id_lst)\n","        for model_id in range(0, nets_to_create):\n","            for device in devices:\n","                if device.active[model_id] != 0:    # If model wasn't already removed\n","                    device_net = ConvNet().cuda()\n","                    device_net.load_state_dict(device.nets[model_id]['net'].state_dict())\n","                    # device_net = copy.deepcopy(device.nets[model_id]['net'])\n","                    # optimizer = copy.deepcopy(device.nets[model_id]['optimizer'])\n","                    # scheduler = copy.deepcopy(device.nets[model_id]['scheduler'])\n","                    # train_loss_tracker = [copy.deepcopy(device.nets[model_id]['train_loss_tracker'][-1])]\n","                    # print('model 0:', device.nets[0]['train_acc_tracker'])\n","                    # train_acc_tracker = [copy.deepcopy(device.nets[model_id]['train_acc_tracker'][-1])]\n","                    valid_loss_tracker = [copy.deepcopy(device.nets[model_id]['valid_loss_tracker'][-1])]\n","                    valid_acc_tracker = [100 - copy.deepcopy(device.nets[model_id]['valid_acc_tracker'][-1])]\n","                    optimizer = torch.optim.SGD(device_net.parameters(), lr=0.1, momentum=0.9,\n","                                                weight_decay=5e-4)\n","                    rounds_passed = len(valid_acc_tracker)\n","                    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,\n","                                                                    milestones=[25-rounds_passed, 50-rounds_passed, 75-rounds_passed],\n","                                                                    gamma=0.1)\n","                    device.nets.append({\n","                        'net': device_net,\n","                        'optimizer': optimizer,\n","                        'scheduler': scheduler,\n","                        'train_loss_tracker': [],\n","                        'train_acc_tracker': [],\n","                        'valid_loss_tracker': valid_loss_tracker,\n","                        'valid_acc_tracker': valid_acc_tracker,\n","                        'test_loss_tracker': [],\n","                        'test_acc_tracker': [],\n","                    })\n","                    device.active.append(1)\n","                    # Heavily rank the devices that are underperforming for new models and vice versa\n","                    if len(valid_acc_tracker) > 0:\n","                        device.update_ranking(removed = False, duplicate_model_id = model_id + nets_to_create, offset_rank = valid_acc_tracker[-1])\n","                    else:\n","                        device.update_ranking()\n","\n","                else:                           # If model was already removed\n","                    device.nets.append({\n","                        'valid_acc_tracker': [0.],\n","                    })\n","                    device.active.append(0)\n","                    device.update_ranking(removed = True, duplicate_model_id = model_id + nets_to_create)\n","\n","            model_id_lst.append(model_id + nets_to_create)\n","    # print('model id list:', model_id_lst)\n","    # print('best model:', [device.ranking.index(max(device.ranking)) for device in devices])\n","    # for device in devices:\n","        # print(\"device's active models:\", device.active)\n","        # print(\"device \" + str(device.idx) + \" ranking: \" + str(device.ranking))\n","        # print(\"archetype:\", device.archetype)\n","\n","\n","total_time = time.time() - start_time\n","print('Total training time: {} seconds'.format(total_time))"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Devices 9\n","\n","[0, '33.333', 12.0, 20.4, 12.6, 9.6, 7.4, 11.4, 84.2, 82.6, 84.2]\n","\n","[1, '59.100', 18.8, 20.8, 20.2, 79.2, 85.8, 74.4, 73.2, 81.2, 85.6]\n","\n","[2, '76.867', 79.8, 75.8, 74.8, 67.0, 77.8, 54.4, 80.2, 86.4, 78.8]\n","\n","[3, '73.900', 84.8, 79.8, 76.6, 45.2, 53.6, 44.2, 79.0, 83.6, 79.0]\n","\n","[4, '79.733', 67.8, 69.4, 63.8, 88.6, 91.4, 89.4, 81.4, 80.6, 81.8]\n","\n","[5, '61.600', 52.6, 51.0, 51.0, 50.8, 55.8, 44.0, 90.2, 90.4, 89.2]\n","\n","[6, '82.867', 81.2, 79.2, 75.8, 90.6, 90.8, 91.6, 75.4, 72.4, 74.8]\n","\n","[7, '66.667', 46.6, 45.6, 45.8, 67.4, 60.4, 74.0, 90.4, 91.0, 91.2]\n","\n","[8, '84.667', 87.4, 85.6, 86.0, 86.6, 82.2, 88.6, 87.4, 85.2, 85.6]\n","\n","[9, '79.967', 62.8, 59.2, 61.2, 87.6, 87.2, 91.0, 93.2, 93.2, 93.6]\n","\n","[10, '83.600', 85.8, 84.8, 83.2, 80.2, 79.4, 89.2, 88.2, 87.8, 88.6]\n","\n","[11, '84.133', 92.2, 88.6, 90.8, 79.2, 80.0, 75.4, 78.4, 79.0, 79.0]\n","\n","[12, '81.267', 63.0, 67.2, 63.2, 89.4, 88.2, 87.6, 93.6, 93.0, 94.4]\n","\n","[13, '87.733', 88.8, 85.2, 85.2, 88.6, 92.4, 90.8, 87.4, 82.8, 84.6]\n","\n","[14, '86.467', 84.2, 80.0, 78.8, 77.8, 85.0, 82.4, 91.8, 89.8, 93.2]\n","\n","[15, '83.833', 84.8, 85.2, 85.8, 75.0, 82.2, 80.6, 88.0, 87.6, 90.2]\n","\n","[16, '88.900', 92.8, 88.0, 91.2, 90.4, 93.8, 93.8, 83.4, 79.4, 85.8]\n","\n","[17, '81.833', 74.6, 73.6, 70.8, 78.2, 78.8, 80.4, 92.8, 92.8, 94.0]\n","\n","[18, '87.933', 78.4, 78.0, 77.0, 91.4, 92.4, 93.4, 94.8, 95.4, 93.2]\n","\n","[19, '89.600', 94.0, 92.4, 92.2, 88.2, 91.4, 93.2, 84.2, 83.4, 81.8]\n","\n","[20, '87.300', 87.2, 86.0, 87.8, 84.4, 84.0, 89.6, 87.8, 91.0, 90.2]\n","\n","[21, '91.467', 89.8, 88.4, 84.6, 93.6, 93.6, 94.4, 93.0, 94.2, 92.4]\n","\n","[22, '89.133', 90.2, 88.8, 90.6, 82.4, 85.8, 86.8, 92.6, 92.8, 91.8]\n","\n","[23, '89.867', 83.4, 80.4, 87.0, 89.8, 92.0, 91.8, 95.6, 95.6, 95.0]\n","\n","[24, '91.767', 90.4, 88.2, 88.4, 93.2, 95.2, 93.2, 92.2, 92.6, 93.2]\n","\n","[25, '91.733', 95.6, 93.4, 95.0, 93.4, 92.8, 94.2, 85.4, 84.2, 85.0]\n","\n","[26, '93.200', 91.8, 90.2, 91.6, 93.8, 94.8, 95.8, 94.4, 94.4, 92.6]\n","\n","[27, '92.933', 95.0, 93.0, 94.4, 94.0, 94.8, 95.4, 89.0, 90.0, 88.8]\n","\n","[28, '90.133', 81.6, 79.6, 79.6, 93.4, 94.2, 94.8, 96.6, 96.0, 96.0]\n","\n","[29, '93.767', 93.6, 91.6, 91.6, 95.2, 96.6, 96.4, 93.8, 93.8, 92.8]\n","\n","[30, '93.767', 94.6, 93.4, 94.6, 94.4, 95.2, 95.8, 91.0, 91.4, 90.2]\n","\n","[31, '93.500', 94.6, 93.0, 94.4, 94.8, 95.6, 96.6, 89.6, 89.4, 88.6]\n","\n","[32, '92.000', 86.0, 85.6, 85.6, 93.8, 94.8, 95.0, 96.8, 96.4, 95.8]\n","\n","[33, '94.233', 92.8, 92.0, 92.4, 95.2, 96.8, 96.8, 93.8, 93.6, 93.2]\n","\n","[34, '93.400', 95.0, 93.2, 93.6, 94.6, 95.8, 95.2, 92.6, 93.4, 92.4]\n","\n","[35, '93.933', 92.0, 90.8, 90.8, 95.8, 95.8, 97.0, 96.0, 95.8, 94.8]\n","\n","[36, '93.967', 90.8, 89.4, 89.2, 95.8, 96.2, 96.6, 96.2, 96.4, 95.4]\n","\n","[37, '94.300', 94.6, 92.8, 94.4, 95.2, 96.4, 96.0, 92.0, 93.2, 91.8]\n","\n","[38, '94.467', 94.0, 92.6, 93.6, 95.0, 94.8, 95.4, 94.2, 94.8, 94.0]\n","\n","[39, '94.167', 95.0, 93.2, 95.2, 93.8, 94.6, 94.6, 92.8, 92.6, 91.2]\n","\n","[40, '93.600', 89.8, 89.4, 88.4, 95.0, 95.0, 95.4, 96.6, 96.4, 95.8]\n","\n","[41, '94.433', 94.2, 92.8, 94.4, 95.6, 95.8, 96.2, 95.2, 92.8, 92.0]\n","\n","[42, '94.400', 92.2, 91.2, 92.8, 95.8, 95.6, 96.4, 96.0, 96.6, 94.8]\n","\n","[43, '93.333', 91.2, 89.8, 89.4, 95.2, 96.0, 96.2, 96.2, 96.8, 95.4]\n","\n","[44, '93.800', 88.8, 87.6, 87.0, 97.2, 97.4, 97.2, 96.0, 95.4, 96.2]\n","Total training time: 857.936368227005 seconds\n"],"name":"stdout"}]}]}