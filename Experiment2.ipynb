{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Eric-Kavya-Jazz-CS242-Final Project_Experiment2.ipynb","provenance":[{"file_id":"https://github.com/kavyakvk/CS242-FederatedLearning/blob/master/Eric_Kavya_Jazz_CS242_Assignment_2_copy.ipynb","timestamp":1587496718015}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"66d49d7f98464179935678fe27569d03":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_c5720bde09484e9c95b49d5f5d9b21ca","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_65ea3b10331440a2bdaa9b9a081f0146","IPY_MODEL_ffa6eb6815854e7e813d794350277aa6"]}},"c5720bde09484e9c95b49d5f5d9b21ca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"65ea3b10331440a2bdaa9b9a081f0146":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_a21c0055abbd4fd48363fcaa68f6aaf5","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"info","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_574884183b6843ff8c4b19730f612eec"}},"ffa6eb6815854e7e813d794350277aa6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_fdfb7410b06644a483b9b3ac0af26af6","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"â€‹","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 170500096/? [00:20&lt;00:00, 33101320.60it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0e0fe9eaa0654bc79d31ae782fc37ae4"}},"a21c0055abbd4fd48363fcaa68f6aaf5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"574884183b6843ff8c4b19730f612eec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"fdfb7410b06644a483b9b3ac0af26af6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"0e0fe9eaa0654bc79d31ae782fc37ae4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"Y7ttBYeclcLI","colab_type":"text"},"source":["# CS242: Final Project\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"mQGm6JtgehIq","colab_type":"text"},"source":["> Harvard CS 242: Computing at Scale (Spring 2020)\n","> \n","> Instructor: Professor HT Kung\n",">\n","> Students: Kavya Kopparapu, Eric Lin, Jazz Zhao\n"]},{"cell_type":"markdown","metadata":{"id":"0UvFA89jTuON","colab_type":"text"},"source":["---\n","\n","### **1. General Setup Code**\n","\n","---\n","Define the dataset (CIFAR and MNIST) as well as the standard net we will be using for training."]},{"cell_type":"code","metadata":{"id":"s9WL6HA_Lpe8","colab_type":"code","outputId":"37422a1e-b0fa-41c5-be14-c29c7d20b4b9","executionInfo":{"status":"ok","timestamp":1589210273510,"user_tz":240,"elapsed":14489,"user":{"displayName":"Kavya Kopparapu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiSZkZc_WsTKxb-RMJ2DJNxff5HEZ7P3COKzu86vA=s64","userId":"15391065872301160663"}},"colab":{"base_uri":"https://localhost:8080/","height":117,"referenced_widgets":["66d49d7f98464179935678fe27569d03","c5720bde09484e9c95b49d5f5d9b21ca","65ea3b10331440a2bdaa9b9a081f0146","ffa6eb6815854e7e813d794350277aa6","a21c0055abbd4fd48363fcaa68f6aaf5","574884183b6843ff8c4b19730f612eec","fdfb7410b06644a483b9b3ac0af26af6","0e0fe9eaa0654bc79d31ae782fc37ae4"]}},"source":["## Code Cell 1.1\n","\n","import time\n","import copy\n","import sys\n","from collections import OrderedDict\n","\n","import torch\n","import torch.nn as nn\n","from torch import optim\n","import torch.nn.functional as F\n","import numpy as np\n","import torchvision\n","import torchvision.transforms as transforms\n","from torch.utils.data.sampler import SubsetRandomSampler\n","import matplotlib.pyplot as plt\n","\n","#Using MNIST\n","#dataset = datasets.MNIST(root='./data')\n","#idx = dataset.train_labels==1\n","#dataset.train_labels = dataset.train_labels[idx]\n","#dataset.train_data = dataset.train_data[idx]\n","\n","# Using CIFAR-10\n","# Load training data\n","transform_train = transforms.Compose([                                   \n","    transforms.RandomCrop(32, padding=4),                                       \n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","])\n","trainset = torchvision.datasets.CIFAR10(root='./data', train=True, \n","                                        download=True,\n","                                        transform=transform_train)\n","#Edit and Load validation data\n","validset = torchvision.datasets.CIFAR10(root='./data', train=True, \n","                                        download=True,\n","                                        transform=transform_train)\n","valid_size = 0.2\n","indices = list(range(len(trainset)))\n","split = int(np.floor(valid_size * len(trainset)))\n","    \n","if True:\n","    np.random.shuffle(indices)\n","\n","train_idx, valid_idx = indices[split:], indices[:split]\n","train_sampler = SubsetRandomSampler(train_idx)\n","valid_sampler = SubsetRandomSampler(valid_idx)\n","\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, \n","                                            sampler=valid_sampler, shuffle=False,\n","                                            num_workers=2)\n","validloader = torch.utils.data.DataLoader(validset, batch_size=128, \n","                                            sampler=valid_sampler, shuffle=False,\n","                                            num_workers=2)\n","# Load testing data\n","transform_test = transforms.Compose([                                           \n","    transforms.ToTensor(),\n","    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","])\n","testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n","                                       download=True,\n","                                       transform=transform_test)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False,\n","                                         num_workers=2)\n","\n","\n","# Using same ConvNet as in Assignment 1\n","def conv_block(in_channels, out_channels, kernel_size=3, stride=1,\n","               padding=1):\n","    return nn.Sequential(\n","        nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding,\n","                  bias=False),\n","        nn.BatchNorm2d(out_channels),\n","        nn.ReLU(inplace=True)\n","        )\n","\n","class ConvNet(nn.Module):\n","    def __init__(self):\n","        super(ConvNet, self).__init__()\n","        self.model = nn.Sequential(\n","            conv_block(3, 32),\n","            conv_block(32, 32),\n","            conv_block(32, 64, stride=2),\n","            conv_block(64, 64),\n","            conv_block(64, 64),\n","            conv_block(64, 128, stride=2),\n","            conv_block(128, 128),\n","            conv_block(128, 256),\n","            conv_block(256, 256),\n","            nn.AdaptiveAvgPool2d(1)\n","            )\n","\n","        self.classifier = nn.Linear(256, 10)\n","\n","    def forward(self, x):\n","        h = self.model(x)\n","        B, C, _, _ = h.shape\n","        h = h.view(B, C)\n","        return self.classifier(h)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"66d49d7f98464179935678fe27569d03","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Extracting ./data/cifar-10-python.tar.gz to ./data\n","Files already downloaded and verified\n","Files already downloaded and verified\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wjiB2IYp7B25","colab_type":"text"},"source":["**Device Class and Train/Test Methods**\n","\n","---"]},{"cell_type":"code","metadata":{"id":"zSx1GxV2j0iI","colab_type":"code","colab":{}},"source":["## Code Cell 1.2\n","import statistics \n","\n","class DatasetSplit(torch.utils.data.Dataset):\n","    def __init__(self, dataset, idxs):\n","        self.dataset = dataset\n","        self.idxs = [int(i) for i in idxs]\n","\n","    def __len__(self):\n","        return len(self.idxs)\n","\n","    def __getitem__(self, item):\n","        image, label = self.dataset[self.idxs[item]]\n","        return image, torch.tensor(label)\n","\n","class Device():\n","    def __init__(self, net, device_id, trainset, validset, testset, train_idxs, valid_idxs, test_idxs, bias, archetype, lr=0.1,\n","                      milestones=None, batch_size=128):\n","        if milestones == None:\n","            milestones = [25, 50, 75]\n","\n","        device_net = copy.deepcopy(net)\n","        optimizer = torch.optim.SGD(device_net.parameters(), lr=lr, momentum=0.9,\n","                                    weight_decay=5e-4)\n","        scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,\n","                                                        milestones=milestones,\n","                                                        gamma=0.1)\n","        self.device_trainset = DatasetSplit(trainset, train_idxs)\n","        self.trainloader = torch.utils.data.DataLoader(self.device_trainset,\n","                                                        batch_size=batch_size,\n","                                                        shuffle=True,\n","                                                        num_workers=2)\n","        self.device_validset = DatasetSplit(validset, valid_idxs)\n","        self.validloader = torch.utils.data.DataLoader(self.device_validset,\n","                                                        batch_size=batch_size,\n","                                                        shuffle=True,\n","                                                        num_workers=2)\n","        self.device_testset = DatasetSplit(testset, test_idxs)\n","        self.testloader = torch.utils.data.DataLoader(self.device_testset,\n","                                                        batch_size=batch_size,\n","                                                        shuffle=True,\n","                                                        num_workers=2)\n","        self.nets = []\n","        self.idx = device_id\n","        self.ranking = [1.]\n","        self.active = [1] #either 1 or 0, depending on whether we got rid of it or not\n","        self.nets.append({\n","            'net': device_net,\n","            # 'id': device_id,\n","            # 'dataloader': device_trainloader, \n","            'optimizer': optimizer,\n","            'scheduler': scheduler,\n","            'train_loss_tracker': [],\n","            'train_acc_tracker': [],\n","            'valid_loss_tracker': [],\n","            'valid_acc_tracker': [],\n","            'test_loss_tracker': [],\n","            'test_acc_tracker': [],\n","        })\n","\n","        # Bias and archetype parameters\n","        self.bias = bias              # Number between 0 and 1 to represent linear comb. of archetypes\n","        self.archetype = archetype    # An array of possible archetypes\n","    \n","    def update_ranking(self, removed=False, duplicate_model_id=-1, offset_rank=-1):\n","        #print(\"updating for device \", self.idx)\n","        zero_threshold = 1 #number of standard deviations away for model deletion cutoff\n","\n","        if len(self.nets) > 1:\n","            #print(\"ranking 1 in the update_ranking method: \", self.ranking)\n","            metrics = []\n","            for i in range(len(self.nets)):\n","                if(len(self.nets[i]['valid_acc_tracker']) > 0):\n","                    rank = self.nets[i]['valid_acc_tracker'][-1]\n","                    if(len(self.nets[i]['valid_acc_tracker']) >= 3):\n","                        rank = (self.nets[i]['valid_acc_tracker'][-1]+self.nets[i]['valid_acc_tracker'][-2]+self.nets[i]['valid_acc_tracker'][-3])/3\n","                    if duplicate_model_id == i and offset_rank != -1:\n","                        rank = offset_rank      # Heavily rank the devices that are underperforming for new models and vice versa\n","                    if rank == 0:\n","                        rank += 0.001\n","                    metrics.append(rank)\n","                else:\n","                    metrics.append(50)\n","            \n","            #if we added more models, add active trackers for them\n","            while(len(self.nets) != len(self.active)):\n","                self.active.append(1)\n","            if removed:       # Auto-set a model as inactive if it was already removed\n","                self.active[duplicate_model_id] = 0\n","            \n","\n","            # normalization first time (with self.active)\n","            self.ranking = [metrics[i]*self.active[i]/sum(metrics) for i in range(len(metrics))]\n","\n","            \n","            #print(\"ranking 2 in the update_ranking method: \", self.ranking)\n","\n","            nonzero_elts = np.array(self.active).nonzero()[0]\n","            nonzero_arr = []\n","            for i in nonzero_elts:\n","                nonzero_arr.append(self.ranking[i])\n","            \n","            \n","            # Remove models that are underperforming\n","            if offset_rank == -1:   # only remove if not duplicating round\n","                max_rank = max(self.ranking)\n","                if len(nonzero_elts) > 1:\n","                    std = statistics.stdev(nonzero_arr)\n","                    mean = sum(nonzero_arr)/len(nonzero_arr)\n","\n","                    # remove models that are underperforming\n","                    for j in range(len(self.ranking)):\n","                        if self.active[j] != 0:\n","                            if(mean - self.ranking[j] > zero_threshold*std):\n","                                self.ranking[j] = 0\n","                                self.active[j] = 0\n","                            elif( len(self.ranking) > 3 and (self.ranking[j] * 10 < max_rank)):\n","                                self.ranking[j] = 0\n","                                self.active[j] = 0\n","\n","            bool_ranking_below_zero = False\n","            # Add noise\n","            # noise = random.gauss(0, statistics.stdev(self.ranking))\n","            noise = random.gauss(0, 0.01)\n","            if len(nonzero_elts) == 1:\n","                i = nonzero_elts[0]\n","            else:\n","                i = nonzero_elts[random.randint(0, len(nonzero_elts)-1)]\n","                for j in range(len(self.ranking)):\n","                    if(j != i):\n","                        self.ranking[j] -= noise/(len(nonzero_elts)-1)\n","                        if self.ranking[j] < 0:\n","                          bool_ranking_below_zero = True\n","            \n","            self.ranking[i] += noise\n","            if self.ranking[i] < 0:\n","                bool_ranking_below_zero = True\n","\n","            # for i in range(len(self.ranking)):\n","                # noise = random.gauss(0, 0.2)\n","                # self.ranking[i] += noise\n","                # self.ranking[i] = max(0, self.ranking[i])\n","\n","            # Normalize again\n","            if(bool_ranking_below_zero):\n","                self.ranking = [self.ranking[i]-min(self.ranking) for i in range(len(self.ranking))]\n","            self.ranking = [self.ranking[i]*self.active[i]/sum(self.ranking) for i in range(len(self.ranking))]\n","            #print(\"ranking 3 in the update_ranking method: \", self.ranking)\n","            \n","\n","def create_devices(net, trainset, validset, testset, train_idxs, valid_idxs, test_idxs, bias, archetype, lr=0.1,\n","                  milestones=None, batch_size=128, num_devices=2):\n","    devices_lst = [Device(net, i, trainset, validset, testset, train_idxs[i], valid_idxs[i], test_idxs[i], bias[i], archetype[i], lr,\n","                  milestones, batch_size) for i in range(num_devices)]\n","    return devices_lst\n","      \n","  \n","def train(epoch, device, model_id):\n","    device.nets[model_id]['net'].train()\n","    train_loss, correct, total = 0, 0, 0\n","\n","    dataset = device.device_trainset\n","    dataloader = device.trainloader\n","\n","    for batch_idx, (inputs, targets) in enumerate(dataloader):\n","        inputs, targets = inputs.cuda(), targets.cuda()\n","        device.nets[model_id]['optimizer'].zero_grad()\n","        outputs = device.nets[model_id]['net'](inputs)\n","        loss = criterion(outputs, targets)\n","        loss.backward()\n","        device.nets[model_id]['optimizer'].step()\n","        train_loss += loss.item()\n","        device.nets[model_id]['train_loss_tracker'].append(loss.item())\n","        loss = train_loss / (batch_idx + 1)\n","        _, predicted = outputs.max(1)\n","        total += targets.size(0)\n","        correct += predicted.eq(targets).sum().item()\n","        acc = 100. * correct / total\n","        dev_id = device.idx\n","        #if epoch == local_epochs - 1:\n","            #sys.stdout.write(f'\\r(Device {dev_id}/Epoch {epoch}) ' + \n","            #                f'Train Loss: {loss:.3f} | Train Acc: {acc:.3f}')\n","            #sys.stdout.flush()\n","    test_loss = 0\n","    outputs = [0]\n","    device.nets[model_id]['train_acc_tracker'].append(acc)\n","    #sys.stdout.flush()\n","\n","def validate(epoch, device, model_id):\n","    device.nets[model_id]['net'].eval()\n","    test_loss, correct, total = 0, 0, 0\n","\n","    dataset = device.device_validset\n","    dataloader = device.validloader\n","    with torch.no_grad():\n","        for batch_idx, (inputs, targets) in enumerate(dataloader):\n","            inputs, targets = inputs.cuda(), targets.cuda()\n","            outputs = device.nets[model_id]['net'](inputs)\n","            loss = criterion(outputs, targets)\n","            test_loss += loss.item()\n","            device.nets[model_id]['valid_loss_tracker'].append(loss.item())\n","            _, predicted = outputs.max(1)\n","            total += targets.size(0)\n","            correct += predicted.eq(targets).sum().item()\n","            loss = test_loss / (batch_idx + 1)\n","            acc = 100.* correct / total\n","        test_loss = 0\n","        outputs = [0]\n","    # if epoch == local_epochs - 1:\n","        # print(\"Device: \" + str(device.idx) + \" VALIDATION: model_id # \" + str(model_id))\n","        # sys.stdout.write(f' | Valid Loss: {loss:.3f} | Valid Acc: {acc:.3f}\\n')\n","        # sys.stdout.flush()  \n","    acc = 100.*correct/total\n","    device.nets[model_id]['valid_acc_tracker'].append(acc)\n","    device.nets[model_id]['net'].train()\n","\n","def test(epoch, device, model_id, dataset, dataloader):\n","    criterion = nn.CrossEntropyLoss()\n","\n","    device.nets[model_id]['net'].eval()\n","    test_loss, correct, total = 0, 0, 0\n","\n","    with torch.no_grad():\n","        for batch_idx, (inputs, targets) in enumerate(dataloader):\n","            inputs, targets = inputs.cuda(), targets.cuda()\n","            outputs = device.nets[model_id]['net'](inputs)\n","            loss = criterion(outputs, targets)\n","            test_loss += loss.item()\n","\n","            # print(\"test\", loss.item(), targets, batch_idx)\n","            \n","            device.nets[model_id]['test_loss_tracker'].append(loss.item())\n","            _, predicted = outputs.max(1)\n","            total += targets.size(0)\n","            correct += predicted.eq(targets).sum().item()\n","            loss = test_loss / (batch_idx + 1)\n","            #print('loss:', loss)\n","            acc = 100.* correct / total\n","        test_loss = 0\n","        outputs = [0]\n","    # sys.stdout.write(f' | Test Loss: {loss:.3f} | Test Acc: {acc:.3f}\\n')\n","    # sys.stdout.flush()  \n","    acc = 100.*correct/total\n","    device.nets[model_id]['test_acc_tracker'].append(acc)\n","\n","    device.nets[model_id]['net'].train()\n","    return ('%.3f' % loss, '%.3f' % acc)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6DwoSIi0ftBW","colab_type":"code","colab":{}},"source":["## Code Cell 1.3\n","import random #to use the random.sample method\n","\n","def iid_sampler(dataset, num_devices, data_pct):\n","    '''\n","    dataset: PyTorch Dataset (e.g., CIFAR-10 training set)\n","    num_devices: integer number of devices to create subsets for\n","    data_pct: percentage of training samples to give each device\n","              e.g., 0.1 represents 10%\n","\n","    return: a dictionary of the following format:\n","      {\n","        0: [3, 65, 2233, ..., 22] // device 0 sample indexes\n","        1: [0, 2, 4, ..., 583] // device 1 sample indexes\n","        ...\n","      }\n","\n","    iid (independent and identically distributed) means that the indexes\n","    should be drawn independently in a uniformly random fashion.\n","    '''\n","\n","    # total number of samples in the dataset\n","    total_samples = len(dataset)\n","\n","    # Part 1.1: Implement!\n","    arr = [i for i in range(total_samples)] #create an arrray of length total_samples\n","    d = {} #initialize the dictonary\n","    for i in range(num_devices): #for every device\n","        d[i] = random.sample(list(arr), k=round(data_pct*total_samples)) #select data_pct*total_samples from the array, without replacement\n","    return d #return the dictionary"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4waD2ZlQI4X2","colab_type":"text"},"source":["**Implementing Components for Federated Learning**\n","\n","---"]},{"cell_type":"code","metadata":{"id":"6X0fTWKg6hBY","colab_type":"code","colab":{}},"source":["## Code Cell 1.5\n","\n","def model_average_weight(devices, model_id):\n","    '''\n","    devices: a list of devices generated by create_devices\n","    Returns an the average of the weights.\n","    '''\n","    d_id = 0\n","    while(d_id < len(devices) and devices[d_id].active[model_id] == 0):\n","        d_id += 1\n","\n","    if(d_id >= len(devices)):\n","        return None\n","        \n","    global_tensors = copy.deepcopy(devices[d_id].nets[model_id]['net'].state_dict()) #initialize a global tensor with the weights of the first device\n","    ranking_sum = devices[d_id].ranking[model_id]\n","\n","    for i in range(0, len(devices)):#iterate over the remaining devices\n","        if(i == d_id):\n","            for j in global_tensors.keys(): #add the tensors together by the key they are indexed by\n","                global_tensors[j] = global_tensors[j]*devices[d_id].ranking[model_id]\n","        if(devices[i].active[model_id] == 1):\n","            #for easy/ less complicated referencing, store the device and the state_dict\n","            d = devices[i]\n","            d_tensors = d.nets[model_id]['net'].state_dict()\n","            for j in global_tensors.keys(): #add the tensors together by the key they are indexed by\n","                global_tensors[j] += d_tensors[j]*d.ranking[model_id]\n","            ranking_sum += devices[i].ranking[model_id]\n","\n","    for j in global_tensors.keys(): #average each tensor by the number of devices\n","        global_tensors[j] = global_tensors[j]/ranking_sum\n","    return global_tensors #return the averaged weights\n","\n","\n","\n","def get_devices_for_round(devices, device_pct):\n","    '''\n","    This function will select a percentage of devices to participate in each training round.\n","    '''\n","    # Part 1.2: Implement!\n","    #randomly choose device_pct*len(devices) devices from the devices array without replacement\n","    arr = random.sample(devices, k=round(device_pct*len(devices)))\n","    return arr"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0KNmDbPh5b0h","colab_type":"text"},"source":["---\n","\n","### **2. Non-IID Testing and Archetype Definition Code**\n","\n","---\n"]},{"cell_type":"markdown","metadata":{"id":"-yUOGnkZCbp-","colab_type":"text"},"source":["**Non-iid Sampling**\n","\n","---"]},{"cell_type":"code","metadata":{"id":"VajsGz0wiL05","colab_type":"code","colab":{}},"source":["## Code Cell 2.1\n","\n","# creates noniid TRAINING and VALIDATION datasets for each group\n","def noniid_group_sampler(dataset, num_items_per_device, archetype, bias):\n","    '''\n","    dataset: PyTorch Dataset (e.g., CIFAR-10 training set)\n","    num_items_per_device: how many samples to assign to each device\n","    archetype: a dictionary of arrays representing the labels that is predominantly represented by this edge device\n","        device index -> array of archetypes\n","    bias: a dictionary of the percent of samples that are represented by the archetype\n","        device index -> value from 0 to 1\n","\n","    return: a dictionary of the following format:\n","      {\n","        0: [3, 65, 2233, ..., 22] // device 0 sample indexes\n","        1: [0, 2, 4, ..., 583] // device 1 sample indexes\n","        ...\n","      }\n","\n","    '''\n","    #label dict stores the indexes of the dataset examples that fall into the ith group of CIFAR\n","    label_dict = {}\n","    for i in range(0, 10): #assuming CIFAR, which has labels 0-9\n","        label_dict[i] = []\n","    for i in range(len(dataset)):\n","        label = dataset[i][1]\n","        label_dict[label].append(i)\n","    \n","    num_devices = len(archetype)\n","\n","    final_dict = {} #final dict is to be returned\n","    for i in range(num_devices):\n","        bias_group = []\n","        not_bias_group = []\n","        archs = [0,1,2,3,4,5] #6 archetypes\n","        for j in label_dict.keys():\n","            if(j in archetype[i]):\n","                bias_group += label_dict[j]\n","            #else:\n","            if(archetype[i][0] in [0,1,2]): #two meta-archetypes\n","                if(j in [0,1,2] and j != archetype[i][0]):\n","                    not_bias_group += label_dict[j]\n","            elif(archetype[i][0] in [3,4,5]): #two meta-archetypes\n","                if(j in [3,4,5] and j != archetype[i][0]):\n","                    not_bias_group += label_dict[j]\n","\n","        exs = random.sample(bias_group, int(num_items_per_device*bias[i]))\n","        exs.extend(random.sample(not_bias_group, num_items_per_device-int(num_items_per_device*bias[i])))\n","        random.shuffle(exs)\n","        final_dict[i] = exs\n","    return final_dict"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JBHBZVehNZNI","colab_type":"text"},"source":["---\n","**Group-based Testing**\n","\n"]},{"cell_type":"code","metadata":{"id":"99S3opJONpeW","colab_type":"code","colab":{}},"source":["## Code Cell 2.3\n","\n","# creates noniid TEST datasets for each group\n","def cifar_noniid_group_test(dataset):\n","\n","    #label dict stores the indexes of the dataset examples that fall into the ith group of CIFAR\n","    label_dict = {}\n","    for i in range(0, 10): #assuming CIFAR, which has labels 0-9, change later\n","        label_dict[i] = []\n","    for i in range(len(dataset)):\n","        label = dataset[i][1]\n","        label_dict[label].append(i)\n","    return label_dict\n","\n","# gets per-group accuracy of global model\n","def test_group(epoch, device, model_id, label_dict, dataset = testset):\n","    \n","    net = device.nets[model_id]['net']\n","    net.eval() #turn the net into evaluaton mode\n","    # sys.stdout.write(' | accuracy: ')\n","    with torch.no_grad():\n","        #for group in label_dict.keys(): \n","        for group in [0,1,2,3,4,5]: #6 archetypes\n","            test_loss, correct, total = 0, 0, 0\n","            new_dataset = DatasetSplit(dataset, label_dict[group])\n","            dataloader = torch.utils.data.DataLoader(new_dataset, batch_size=128, shuffle=False,\n","                                            num_workers=2)\n","            for batch_idx, (inputs, targets) in enumerate(dataloader):\n","                inputs, targets = inputs.cuda(), targets.cuda()\n","                outputs = net(inputs)\n","                loss = criterion(outputs, targets)\n","                test_loss += loss.item()\n","                #print(\"test_group\", loss.item())\n","                _, predicted = outputs.max(1)\n","                total += targets.size(0)\n","                correct += predicted.eq(targets).sum().item()\n","            # Compute and print loss and accuracy at the end of the group\n","            loss = test_loss / (batch_idx + 1)\n","            acc = 100.* correct / total\n","            # sys.stdout.write(f'{acc:.3f} | ')\n","\n","            outputs = [0]\n","            test_loss = 0\n","    net.train()\n","    # sys.stdout.write('\\n')\n","    sys.stdout.flush()  \n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"d9KiAYyHS8VA","colab_type":"code","outputId":"0dd9a57f-b9c9-4bc3-cabd-5ed7230cff7c","executionInfo":{"status":"ok","timestamp":1589210274682,"user_tz":240,"elapsed":15605,"user":{"displayName":"Kavya Kopparapu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiSZkZc_WsTKxb-RMJ2DJNxff5HEZ7P3COKzu86vA=s64","userId":"15391065872301160663"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["## Code Cell 3.1\n","\n","def quantizer(input, nbit):\n","    '''\n","    input: full precision tensor in the range [0, 1]\n","    return: quantized tensor\n","    '''\n","    scale_factor = 1 / (2**nbit -  1)\n","\n","    # scale input by inverse of scale_factor and round to nearest integer\n","    output = input / scale_factor\n","    output = torch.round(output)\n","\n","    # scale rounded output back and return\n","    output *= scale_factor\n","    return output\n","\n","# Test Code\n","test_data = torch.tensor([i/11 for i in range(11)])\n","\n","# ground truth results of 4-bit quantization\n","ground_truth = torch.tensor([0.0000, 0.0667, 0.2000, 0.2667, 0.3333, 0.4667,\n","                             0.5333, 0.6667, 0.7333, 0.8000, 0.9333])\n","\n","# output of your quantization function\n","quantizer_output = quantizer(test_data, 4)\n","\n","if torch.allclose(quantizer_output, ground_truth, atol=1e-04):\n","    print('Output of Quantization Matches!')\n","else:\n","    print('Output of Quantization DOES NOT Match!')\n","\n","\n","## Code Cell 3.2\n","\n","def quantize_model(model, nbit):\n","    '''\n","    Used in Code Cell 3.3 to quantize the ConvNet model\n","    '''\n","    for m in model.modules():\n","        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n","            m.weight.data, m.adaptive_scale = dorefa_g(m.weight, nbit)\n","            if m.bias is not None:\n","                m.bias.data,_ = dorefa_g(m.bias, nbit, m.adaptive_scale)\n","\n","def dorefa_g(w, nbit, adaptive_scale=None):\n","    '''\n","    w: a floating-point weight tensor to quantize\n","    nbit: the number of bits in the quantized representation\n","    adaptive_scale: the maximum scale value. if None, it is set to be the\n","                    absolute maximum value in w.\n","    '''\n","    if adaptive_scale is None:\n","        adaptive_scale = torch.max(torch.abs(w))\n","    \n","    # follows equations above\n","    sigma = torch.rand(w.shape) - 0.5\n","    noise = sigma / (2**nbit - 1)\n","    # avoid type errors\n","    noise = noise.type(w.type())\n","    inp = w / (2*adaptive_scale) + 0.5 + noise\n","    w_q = 2*adaptive_scale * (quantizer(inp, nbit) - 0.5)\n","\n","    return w_q, adaptive_scale\n","\n","\n","# Test Code\n","test_data = torch.tensor([i/11 for i in range(11)])\n","\n","# ground truth results of 4-bit quantization\n","ground_truth = torch.tensor([-0.0606, 0.0606, 0.1818, 0.3030, 0.3030, 0.4242,\n","                             0.5455, 0.5455, 0.7879, 0.7879, 0.9091])\n","\n","# output of your quantization function\n","torch.manual_seed(43)\n","quantizer_output, adaptive_scale = dorefa_g(test_data, 4)\n","\n","if torch.allclose(quantizer_output, ground_truth, atol=1e-04):\n","    print('Output of Quantization Matches!')\n","else:\n","    print('Output of Quantization DOES NOT Match!')"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Output of Quantization Matches!\n","Output of Quantization Matches!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CtkYyNF5Oo9J","colab_type":"text"},"source":["---\n","**Federated Learning Results in Non-IID Setting**"]},{"cell_type":"code","metadata":{"id":"FKogIgMUgE0f","colab_type":"code","colab":{}},"source":["# Train model on each device\n","# Get rankings on each device\n","# Update weights\n","\n","# use these parameters\n","rounds = 45\n","local_epochs = 3\n","num_devices = 18\n","num_labels = 6\n","num_items_per_device = 5000\n","device_pct = 0.5\n","data_pct = 0.1\n","net = ConvNet().cuda()\n","criterion = nn.CrossEntropyLoss()\n","#duplicate_milestones = [2, 5, 15]\n","duplicate_milestones = [5, 15, 25, 30]\n","\n","devices_archetype = [[i//3] for i in range(num_devices)]\n","devices_bias = [random.uniform(0.6, 0.7) for i in range(num_devices)]\n","\n","'''\n","device 0-2: 85% 0, 15% 1,2\n","device 3-5: 85% 1, 15% 0,2\n","device 6-8: 85% 2, 15% 0,1\n","device 9-11: 85% 3, 15% 4,5\n","device 12-14: 85% 4, 15% 3,5\n","device 15-17: 85% 5, 15% 3,4\n","'''\n","#data_idxs = iid_sampler(trainset, num_devices, data_pct) #this is in the uniform case, without archetypes\n","train_idxs = noniid_group_sampler(trainset, num_items_per_device, devices_archetype, devices_bias)\n","valid_idxs = noniid_group_sampler(validset, num_items_per_device // 3, devices_archetype, devices_bias)\n","test_idxs_device = noniid_group_sampler(testset, 500, devices_archetype, devices_bias)\n","\n","label_dict_test = cifar_noniid_group_test(testset)\n","# test_idxs = label_dict_test[0] + label_dict_test[1]\n","test_idxs = []\n","for i in range(0, num_labels):\n","    test_idxs += label_dict_test[i] \n","random.shuffle(test_idxs)\n","\n","arch_testset = DatasetSplit(testset, test_idxs)\n","test_dataloader = torch.utils.data.DataLoader(arch_testset, batch_size=128,\n","                                                shuffle=True, num_workers=2)\n","\n","label_dict_valid = cifar_noniid_group_test(validset)\n","\n","#print(devices_archetype)\n","#print(data_idxs)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Nnf7JCFHUQPI","colab_type":"code","outputId":"61823717-5513-4156-b1e6-d986d8da26b1","executionInfo":{"status":"ok","timestamp":1589215183348,"user_tz":240,"elapsed":4568289,"user":{"displayName":"Kavya Kopparapu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiSZkZc_WsTKxb-RMJ2DJNxff5HEZ7P3COKzu86vA=s64","userId":"15391065872301160663"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["#nbit = 8\n","model_id_lst = [0]\n","\n","## Device creation\n","devices = create_devices(net, trainset, validset, testset, train_idxs, valid_idxs, test_idxs_device, devices_bias, devices_archetype, num_devices=num_devices)\n","print('Devices', len(devices))\n","\n","## NON- IID Federated Learning\n","##\n","##\n","##\n","start_time = time.time()\n","for round_num in range(rounds):\n","  \n","    # Part 1.3: Implement getting devices for each round here\n","    round_devices = get_devices_for_round(devices, device_pct)\n","    # print('Round Devices', round_devices)\n","\n","    #print('--------------Round: ' + str(round_num) + \"-------------\")\n","    \n","    for device in round_devices:\n","        for model_id in model_id_lst:\n","            if(device.active[model_id] != 0):\n","                # Training\n","                for local_epoch in range(local_epochs):\n","                    train(local_epoch, device, model_id) \n","                    # print(\"Device: \" + str(device.idx) + \" VALIDATION: model_id # \" + str(model_id))\n","                    # validate(local_epoch, device, model_id) \n","                    # print()\n","                    # test_group(round_num, device, model_id, label_dict_valid, validset)   \n","                # after training, quantize the learned model\n","                #quantize_model(device.nets[model_id]['net'], nbit)\n","        \n","\n","    # Part 1.3: Implement weight averaging here\n","    for model_id in model_id_lst:\n","        w_avg = model_average_weight(round_devices, model_id)\n","\n","        if(w_avg != None):\n","            for device in devices:\n","                if(device.active[model_id]!= 0):\n","                    device.nets[model_id]['net'].load_state_dict(w_avg)\n","                    device.nets[model_id]['optimizer'].zero_grad()\n","                    device.nets[model_id]['optimizer'].step()\n","                    device.nets[model_id]['scheduler'].step()\n","            \n","        # test accuracy with highest ranking model\n","        if((devices[0].active[model_id] == 1) and (devices[0].ranking[model_id] == max(devices[0].ranking))):\n","            # print()\n","            # print(\"ALL-TEST ACCURACY\")\n","            test(round_num, devices[0], model_id, arch_testset, test_dataloader)\n","            # print(\"ALL-TEST TEST GROUPS ACCURACY\")\n","            # test_group(round_num, devices[0], model_id, label_dict_test)\n","    \n","    # Validation\n","    if round_num not in duplicate_milestones:\n","        for device in round_devices:\n","            for model_id in model_id_lst:\n","                if(device.active[model_id] != 0):\n","                    validate(local_epochs - 1, device, model_id)    # <- there are print statements here\n","            # Figure out rankings here\n","            device.update_ranking()\n","\n","    # Testing with IID from device\n","    test_iid_results = []\n","    for index in range(len(devices)):\n","        device = devices[index]\n","        max_model = device.ranking.index(max(device.ranking))\n","        test_iid_results.append(float(test(round_num, device, max_model, device.device_testset, device.testloader)[1]))\n","\n","    #print(round_num, test_iid_results)\n","    \"\"\"\n","    active_arr_tracker = [sum(devices[i].active) for i in range(len(devices))]\n","    print(round_num, active_arr_tracker)\n","    for i in range(len(devices)):\n","        print(i, devices[i].ranking)\n","    \"\"\"\n","\n","    #duplicate all models\n","    if(round_num in duplicate_milestones):\n","        # Run validation and update rankings for everyone\n","        for device in devices:\n","            for model_id in model_id_lst:\n","                if(device.active[model_id] != 0):\n","                    validate(local_epochs - 1, device, model_id)    # <- there are print statements here\n","            # Figure out rankings here\n","            device.update_ranking()\n","\n","        # Number of nets to duplicate\n","        nets_to_create = len(model_id_lst)\n","        for model_id in range(0, nets_to_create):\n","            for device in devices:\n","                if device.active[model_id] != 0:    # If model wasn't already removed\n","                    device_net = ConvNet().cuda()\n","                    device_net.load_state_dict(device.nets[model_id]['net'].state_dict())\n","                    # device_net = copy.deepcopy(device.nets[model_id]['net'])\n","                    # optimizer = copy.deepcopy(device.nets[model_id]['optimizer'])\n","                    # scheduler = copy.deepcopy(device.nets[model_id]['scheduler'])\n","                    # train_loss_tracker = [copy.deepcopy(device.nets[model_id]['train_loss_tracker'][-1])]\n","                    # print('model 0:', device.nets[0]['train_acc_tracker'])\n","                    # train_acc_tracker = [copy.deepcopy(device.nets[model_id]['train_acc_tracker'][-1])]\n","                    valid_loss_tracker = [copy.deepcopy(device.nets[model_id]['valid_loss_tracker'][-1])]\n","                    valid_acc_tracker = [100 - copy.deepcopy(device.nets[model_id]['valid_acc_tracker'][-1])]\n","                    optimizer = torch.optim.SGD(device_net.parameters(), lr=0.1, momentum=0.9,\n","                                                weight_decay=5e-4)\n","                    rounds_passed = len(device.nets[model_id]['train_acc_tracker'])\n","                    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,\n","                                                                    milestones=[25-rounds_passed, 50-rounds_passed, 75-rounds_passed],\n","                                                                    gamma=0.1)\n","                    device.nets.append({\n","                        'net': device_net,\n","                        'optimizer': optimizer,\n","                        'scheduler': scheduler,\n","                        'train_loss_tracker': [],\n","                        'train_acc_tracker': [],\n","                        'valid_loss_tracker': valid_loss_tracker,\n","                        'valid_acc_tracker': valid_acc_tracker,\n","                        'test_loss_tracker': [],\n","                        'test_acc_tracker': [],\n","                    })\n","                    device.active.append(1)\n","                    # Heavily rank the devices that are underperforming for new models and vice versa\n","                    if len(valid_acc_tracker) > 0:\n","                        device.update_ranking(removed = False, duplicate_model_id = model_id + nets_to_create, offset_rank = valid_acc_tracker[-1])\n","                    else:\n","                        device.update_ranking()\n","\n","                else:                           # If model was already removed\n","                    device.nets.append({\n","                        'valid_acc_tracker': [0.],\n","                    })\n","                    device.active.append(0)\n","                    device.update_ranking(removed = True, duplicate_model_id = model_id + nets_to_create)\n","\n","            model_id_lst.append(model_id + nets_to_create)\n","    # print('model id list:', model_id_lst)\n","    # print('best model:', [device.ranking.index(max(device.ranking)) for device in devices])\n","    # for device in devices:\n","        # print(\"device's active models:\", device.active)\n","        # print(\"device \" + str(device.idx) + \" ranking: \" + str(device.ranking))\n","        # print(\"archetype:\", device.archetype)\n","\n","\n","total_time = time.time() - start_time\n","print('Total training time: {} seconds'.format(total_time))"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Devices 18\n","0 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","0  hi  [1.0]\n","1  hi  [1.0]\n","2  hi  [1.0]\n","3  hi  [1.0]\n","4  hi  [1.0]\n","5  hi  [1.0]\n","6  hi  [1.0]\n","7  hi  [1.0]\n","8  hi  [1.0]\n","9  hi  [1.0]\n","10  hi  [1.0]\n","11  hi  [1.0]\n","12  hi  [1.0]\n","13  hi  [1.0]\n","14  hi  [1.0]\n","15  hi  [1.0]\n","16  hi  [1.0]\n","17  hi  [1.0]\n","1 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","0  hi  [1.0]\n","1  hi  [1.0]\n","2  hi  [1.0]\n","3  hi  [1.0]\n","4  hi  [1.0]\n","5  hi  [1.0]\n","6  hi  [1.0]\n","7  hi  [1.0]\n","8  hi  [1.0]\n","9  hi  [1.0]\n","10  hi  [1.0]\n","11  hi  [1.0]\n","12  hi  [1.0]\n","13  hi  [1.0]\n","14  hi  [1.0]\n","15  hi  [1.0]\n","16  hi  [1.0]\n","17  hi  [1.0]\n","2 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","0  hi  [1.0]\n","1  hi  [1.0]\n","2  hi  [1.0]\n","3  hi  [1.0]\n","4  hi  [1.0]\n","5  hi  [1.0]\n","6  hi  [1.0]\n","7  hi  [1.0]\n","8  hi  [1.0]\n","9  hi  [1.0]\n","10  hi  [1.0]\n","11  hi  [1.0]\n","12  hi  [1.0]\n","13  hi  [1.0]\n","14  hi  [1.0]\n","15  hi  [1.0]\n","16  hi  [1.0]\n","17  hi  [1.0]\n","3 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","0  hi  [1.0]\n","1  hi  [1.0]\n","2  hi  [1.0]\n","3  hi  [1.0]\n","4  hi  [1.0]\n","5  hi  [1.0]\n","6  hi  [1.0]\n","7  hi  [1.0]\n","8  hi  [1.0]\n","9  hi  [1.0]\n","10  hi  [1.0]\n","11  hi  [1.0]\n","12  hi  [1.0]\n","13  hi  [1.0]\n","14  hi  [1.0]\n","15  hi  [1.0]\n","16  hi  [1.0]\n","17  hi  [1.0]\n","4 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","0  hi  [1.0]\n","1  hi  [1.0]\n","2  hi  [1.0]\n","3  hi  [1.0]\n","4  hi  [1.0]\n","5  hi  [1.0]\n","6  hi  [1.0]\n","7  hi  [1.0]\n","8  hi  [1.0]\n","9  hi  [1.0]\n","10  hi  [1.0]\n","11  hi  [1.0]\n","12  hi  [1.0]\n","13  hi  [1.0]\n","14  hi  [1.0]\n","15  hi  [1.0]\n","16  hi  [1.0]\n","17  hi  [1.0]\n","5 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","0  hi  [1.0]\n","1  hi  [1.0]\n","2  hi  [1.0]\n","3  hi  [1.0]\n","4  hi  [1.0]\n","5  hi  [1.0]\n","6  hi  [1.0]\n","7  hi  [1.0]\n","8  hi  [1.0]\n","9  hi  [1.0]\n","10  hi  [1.0]\n","11  hi  [1.0]\n","12  hi  [1.0]\n","13  hi  [1.0]\n","14  hi  [1.0]\n","15  hi  [1.0]\n","16  hi  [1.0]\n","17  hi  [1.0]\n","6 [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n","0  hi  [0.03477131650359538, 0.9652286834964047]\n","1  hi  [0.32004932163674515, 0.6799506783632548]\n","2  hi  [0.012962430305919165, 0.9870375696940809]\n","3  hi  [0.20956025359869013, 0.7904397464013099]\n","4  hi  [0.025089243460525934, 0.974910756539474]\n","5  hi  [0.02681552661294871, 0.9731844733870514]\n","6  hi  [0.2484231716542525, 0.7515768283457475]\n","7  hi  [0.2554431588109261, 0.744556841189074]\n","8  hi  [0.0557129847232313, 0.9442870152767687]\n","9  hi  [1.0, 0.0]\n","10  hi  [0.4301360791965296, 0.5698639208034704]\n","11  hi  [1.0, 0.0]\n","12  hi  [1.0, 0.0]\n","13  hi  [0.42679111593229824, 0.5732088840677018]\n","14  hi  [0.25567452405823266, 0.7443254759417673]\n","15  hi  [0.9995495722994073, 0.0004504277005927193]\n","16  hi  [0.22830045119430548, 0.7716995488056946]\n","17  hi  [0.09619758269605343, 0.9038024173039466]\n","7 [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n","0  hi  [0.037123421600862606, 0.9628765783991374]\n","1  hi  [0.24159249440204628, 0.7584075055979538]\n","2  hi  [0.012206359689795136, 0.9877936403102048]\n","3  hi  [0.28440025896853727, 0.7155997410314627]\n","4  hi  [0.025089243460525934, 0.974910756539474]\n","5  hi  [0.02681552661294871, 0.9731844733870514]\n","6  hi  [0.04743761806402175, 0.9525623819359782]\n","7  hi  [0.2554431588109261, 0.744556841189074]\n","8  hi  [0.0557129847232313, 0.9442870152767687]\n","9  hi  [0.8511210303400291, 0.14887896965997097]\n","10  hi  [1.0, 0.0]\n","11  hi  [1.0, 0.0]\n","12  hi  [0.7955304691100489, 0.20446953088995112]\n","13  hi  [0.42679111593229824, 0.5732088840677018]\n","14  hi  [0.25567452405823266, 0.7443254759417673]\n","15  hi  [0.9995495722994073, 0.0004504277005927193]\n","16  hi  [1.0, 0.0]\n","17  hi  [0.09619758269605343, 0.9038024173039466]\n","8 [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n","0  hi  [0.037123421600862606, 0.9628765783991374]\n","1  hi  [0.24159249440204628, 0.7584075055979538]\n","2  hi  [0.012206359689795136, 0.9877936403102048]\n","3  hi  [0.175028266710803, 0.824971733289197]\n","4  hi  [0.1847494226154357, 0.8152505773845642]\n","5  hi  [0.17205690997876458, 0.8279430900212355]\n","6  hi  [0.23980246107195977, 0.7601975389280403]\n","7  hi  [0.22766450787540776, 0.7723354921245923]\n","8  hi  [0.23229663908529266, 0.7677033609147074]\n","9  hi  [0.8511210303400291, 0.14887896965997097]\n","10  hi  [0.8176097960308699, 0.18239020396913014]\n","11  hi  [1.0, 0.0]\n","12  hi  [0.7955304691100489, 0.20446953088995112]\n","13  hi  [1.0, 0.0]\n","14  hi  [0.25567452405823266, 0.7443254759417673]\n","15  hi  [0.9995495722994073, 0.0004504277005927193]\n","16  hi  [1.0, 0.0]\n","17  hi  [0.9827360530934452, 0.017263946906554712]\n","9 [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n","0  hi  [0.0, 1.0]\n","1  hi  [0.24159249440204628, 0.7584075055979538]\n","2  hi  [0.004871216407527126, 0.9951287835924729]\n","3  hi  [0.175028266710803, 0.824971733289197]\n","4  hi  [0.1847494226154357, 0.8152505773845642]\n","5  hi  [0.20144853523611528, 0.7985514647638848]\n","6  hi  [0.19961482186035934, 0.8003851781396406]\n","7  hi  [0.22766450787540776, 0.7723354921245923]\n","8  hi  [0.23229663908529266, 0.7677033609147074]\n","9  hi  [1.0, 0.0]\n","10  hi  [1.0, 0.0]\n","11  hi  [0.8825677957884338, 0.11743220421156617]\n","12  hi  [0.9900281647128045, 0.009971835287195424]\n","13  hi  [1.0, 0.0]\n","14  hi  [0.25567452405823266, 0.7443254759417673]\n","15  hi  [0.9995495722994073, 0.0004504277005927193]\n","16  hi  [1.0, 0.0]\n","17  hi  [0.5778224547298542, 0.4221775452701459]\n","10 [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n","0  hi  [0.0016488687646522802, 0.9983511312353477]\n","1  hi  [0.018460196862322257, 0.9815398031376776]\n","2  hi  [0.004871216407527126, 0.9951287835924729]\n","3  hi  [0.175028266710803, 0.824971733289197]\n","4  hi  [0.1847494226154357, 0.8152505773845642]\n","5  hi  [0.20144853523611528, 0.7985514647638848]\n","6  hi  [0.19961482186035934, 0.8003851781396406]\n","7  hi  [0.18350243062333196, 0.8164975693766681]\n","8  hi  [0.2041541827326361, 0.7958458172673639]\n","9  hi  [1.0, 0.0]\n","10  hi  [0.9937298227295384, 0.006270177270461695]\n","11  hi  [0.9890917356567331, 0.010908264343266898]\n","12  hi  [0.9900281647128045, 0.009971835287195424]\n","13  hi  [0.7396257454482329, 0.2603742545517671]\n","14  hi  [0.25567452405823266, 0.7443254759417673]\n","15  hi  [0.9995495722994073, 0.0004504277005927193]\n","16  hi  [1.0, 0.0]\n","17  hi  [1.0, 0.0]\n","11 [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n","0  hi  [0.0, 1.0]\n","1  hi  [0.018460196862322257, 0.9815398031376776]\n","2  hi  [0.004871216407527126, 0.9951287835924729]\n","3  hi  [0.175028266710803, 0.824971733289197]\n","4  hi  [0.1847494226154357, 0.8152505773845642]\n","5  hi  [0.2068992955770772, 0.7931007044229228]\n","6  hi  [0.20884735893137524, 0.7911526410686248]\n","7  hi  [0.18350243062333196, 0.8164975693766681]\n","8  hi  [0.2041541827326361, 0.7958458172673639]\n","9  hi  [0.9888974158037442, 0.011102584196255823]\n","10  hi  [0.9784503092024023, 0.021549690797597762]\n","11  hi  [0.9752050444070056, 0.024794955592994446]\n","12  hi  [0.9900281647128045, 0.009971835287195424]\n","13  hi  [0.900607784914838, 0.099392215085162]\n","14  hi  [0.7027866983555314, 0.2972133016444686]\n","15  hi  [0.6275875299028435, 0.37241247009715645]\n","16  hi  [1.0, 0.0]\n","17  hi  [1.0, 0.0]\n","12 [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n","0  hi  [0.0, 1.0]\n","1  hi  [0.0, 1.0]\n","2  hi  [0.017006124679352502, 0.9829938753206475]\n","3  hi  [0.175028266710803, 0.824971733289197]\n","4  hi  [0.1847494226154357, 0.8152505773845642]\n","5  hi  [0.2068992955770772, 0.7931007044229228]\n","6  hi  [0.20884735893137524, 0.7911526410686248]\n","7  hi  [0.19436347801071147, 0.8056365219892886]\n","8  hi  [0.1893766359856791, 0.8106233640143209]\n","9  hi  [0.960195474719055, 0.03980452528094507]\n","10  hi  [0.9784503092024023, 0.021549690797597762]\n","11  hi  [0.9898258090478446, 0.010174190952155462]\n","12  hi  [0.9900281647128045, 0.009971835287195424]\n","13  hi  [0.900607784914838, 0.099392215085162]\n","14  hi  [0.7631582807964512, 0.2368417192035488]\n","15  hi  [0.9590583842931069, 0.04094161570689317]\n","16  hi  [1.0, 0.0]\n","17  hi  [1.0, 0.0]\n","13 [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n","0  hi  [0.0, 1.0]\n","1  hi  [0.0, 1.0]\n","2  hi  [0.0, 1.0]\n","3  hi  [0.1585119124393698, 0.8414880875606302]\n","4  hi  [0.1847494226154357, 0.8152505773845642]\n","5  hi  [0.005061362135771862, 0.9949386378642281]\n","6  hi  [0.014766401876882737, 0.9852335981231173]\n","7  hi  [0.19436347801071147, 0.8056365219892886]\n","8  hi  [0.0, 1.0]\n","9  hi  [0.960195474719055, 0.03980452528094507]\n","10  hi  [0.9875537431249375, 0.012446256875062572]\n","11  hi  [0.9898258090478446, 0.010174190952155462]\n","12  hi  [0.9900281647128045, 0.009971835287195424]\n","13  hi  [0.900607784914838, 0.099392215085162]\n","14  hi  [0.7631582807964512, 0.2368417192035488]\n","15  hi  [0.9863166592640857, 0.013683340735914302]\n","16  hi  [0.6689548313151944, 0.3310451686848056]\n","17  hi  [1.0, 0.0]\n","14 [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n","0  hi  [0.0, 1.0]\n","1  hi  [0.0, 1.0]\n","2  hi  [0.0, 1.0]\n","3  hi  [0.1585119124393698, 0.8414880875606302]\n","4  hi  [0.1847494226154357, 0.8152505773845642]\n","5  hi  [0.0, 1.0]\n","6  hi  [0.012909367073021975, 0.9870906329269781]\n","7  hi  [0.19436347801071147, 0.8056365219892886]\n","8  hi  [0.0, 1.0]\n","9  hi  [0.960195474719055, 0.03980452528094507]\n","10  hi  [0.9952372580866746, 0.004762741913325271]\n","11  hi  [0.9906434078229278, 0.009356592177072246]\n","12  hi  [0.9995920905196151, 0.00040790948038497217]\n","13  hi  [0.900607784914838, 0.099392215085162]\n","14  hi  [0.9373941435930093, 0.06260585640699065]\n","15  hi  [0.9863166592640857, 0.013683340735914302]\n","16  hi  [1.0, 0.0]\n","17  hi  [1.0, 0.0]\n","15 [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n","0  hi  [0.0, 1.0]\n","1  hi  [0.0, 1.0]\n","2  hi  [0.0, 1.0]\n","3  hi  [0.1585119124393698, 0.8414880875606302]\n","4  hi  [0.1847494226154357, 0.8152505773845642]\n","5  hi  [0.0, 1.0]\n","6  hi  [0.012909367073021975, 0.9870906329269781]\n","7  hi  [0.19436347801071147, 0.8056365219892886]\n","8  hi  [0.0, 1.0]\n","9  hi  [0.960195474719055, 0.03980452528094507]\n","10  hi  [0.9952372580866746, 0.004762741913325271]\n","11  hi  [0.9906434078229278, 0.009356592177072246]\n","12  hi  [0.9995920905196151, 0.00040790948038497217]\n","13  hi  [0.900607784914838, 0.099392215085162]\n","14  hi  [0.9373941435930093, 0.06260585640699065]\n","15  hi  [0.9863166592640857, 0.013683340735914302]\n","16  hi  [1.0, 0.0]\n","17  hi  [1.0, 0.0]\n","16 [2, 2, 2, 2, 4, 2, 2, 4, 4, 4, 4, 2, 4, 4, 4, 2, 2, 4]\n","0  hi  [0.0, 0.48807385554115423, 0.5078491198023631, 0.0]\n","1  hi  [0.0, 0.4867019935952211, 0.513298006404779, 0.0]\n","2  hi  [0.0, 0.49390763286311534, 0.5060923671368847, 0.0]\n","3  hi  [0.0, 0.5065415424561994, 0.49245503309499916, 0.0]\n","4  hi  [0.06904052742865026, 0.40728630199970395, 0.4503989082319986, 0.07327426233964729]\n","5  hi  [0.0, 0.4914055277691155, 0.5085944722308845, 0.0]\n","6  hi  [0.0, 0.5157770373801221, 0.480294433100658, 0.0]\n","7  hi  [0.0, 0.4597758461052946, 0.4975019624129801, 0.04272219148172515]\n","8  hi  [0.0055102186583230696, 0.4456669069867958, 0.505502718695823, 0.043320155659058245]\n","9  hi  [0.40869826891500555, 0.012173068380864458, 0.07778185401067524, 0.5013468086934547]\n","10  hi  [0.41364806489573747, 0.001199455414786383, 0.07670399680147282, 0.5084484828880034]\n","11  hi  [0.5046511329138715, 0.0, 0.0, 0.4953488670861285]\n","12  hi  [0.41991177988286965, 0.0, 0.07537104533353656, 0.5047171747835938]\n","13  hi  [0.4106903737404913, 0.02682538495890901, 0.07404535521984897, 0.4884388860807508]\n","14  hi  [0.4219576427134267, 0.0, 0.08048569143133878, 0.4975566658552345]\n","15  hi  [0.5577369863652935, 0.0, 0.0, 0.43234381217884366]\n","16  hi  [0.5560088164707951, 0.0, 0.0, 0.44399118352920497]\n","17  hi  [0.39084044188644185, 0.0, 0.11736057159379543, 0.4917989865197626]\n","17 [2, 2, 2, 2, 3, 2, 2, 4, 2, 4, 4, 2, 4, 2, 2, 2, 2, 2]\n","0  hi  [0.0, 0.48807385554115423, 0.5078491198023631, 0.0]\n","1  hi  [0.0, 0.4867019935952211, 0.513298006404779, 0.0]\n","2  hi  [0.0, 0.4832877612600352, 0.5167122387399649, 0.0]\n","3  hi  [0.0, 0.5065415424561994, 0.49245503309499916, 0.0]\n","4  hi  [0.11148607293531183, 0.4385195343841193, 0.44999439268056873, 0.0]\n","5  hi  [0.0, 0.4732734188403621, 0.5037441593372787, 0.0]\n","6  hi  [0.0, 0.4981492552894252, 0.5018507447105749, 0.0]\n","7  hi  [0.0, 0.4597758461052946, 0.4975019624129801, 0.04272219148172515]\n","8  hi  [0.0, 0.48100980121187764, 0.5076033028300617, 0.0]\n","9  hi  [0.40869826891500555, 0.012173068380864458, 0.07778185401067524, 0.5013468086934547]\n","10  hi  [0.41364806489573747, 0.001199455414786383, 0.07670399680147282, 0.5084484828880034]\n","11  hi  [0.5046511329138715, 0.0, 0.0, 0.4953488670861285]\n","12  hi  [0.41991177988286965, 0.0, 0.07537104533353656, 0.5047171747835938]\n","13  hi  [0.48267633953116224, 0.0, 0.0, 0.4842723406538445]\n","14  hi  [0.5013527481735437, 0.0, 0.0, 0.48612418819105346]\n","15  hi  [0.48286414651985093, 0.0, 0.0, 0.49861288118242886]\n","16  hi  [0.5560088164707951, 0.0, 0.0, 0.44399118352920497]\n","17  hi  [0.5022122467089497, 0.0, 0.0, 0.47849130919078126]\n","18 [2, 2, 2, 2, 3, 2, 2, 4, 2, 4, 2, 2, 2, 2, 2, 2, 2, 2]\n","0  hi  [0.0, 0.4794072395262148, 0.5067632158806542, 0.0]\n","1  hi  [0.0, 0.4867019935952211, 0.513298006404779, 0.0]\n","2  hi  [0.0, 0.4832877612600352, 0.5167122387399649, 0.0]\n","3  hi  [0.0, 0.5065415424561994, 0.49245503309499916, 0.0]\n","4  hi  [0.11148607293531183, 0.4385195343841193, 0.44999439268056873, 0.0]\n","5  hi  [0.0, 0.49675820282877203, 0.49455673323222316, 0.0]\n","6  hi  [0.0, 0.5134820477998013, 0.4865179522001988, 0.0]\n","7  hi  [0.0, 0.4597758461052946, 0.4975019624129801, 0.04272219148172515]\n","8  hi  [0.0, 0.5198209105248219, 0.4801790894751782, 0.0]\n","9  hi  [0.40869826891500555, 0.012173068380864458, 0.07778185401067524, 0.5013468086934547]\n","10  hi  [0.5126513174078813, 0.0, 0.0, 0.4788891661594166]\n","11  hi  [0.49174256457476406, 0.0, 0.0, 0.48148794957179175]\n","12  hi  [0.4917142472731099, 0.0, 0.0, 0.4966028810529357]\n","13  hi  [0.48267633953116224, 0.0, 0.0, 0.4842723406538445]\n","14  hi  [0.5013527481735437, 0.0, 0.0, 0.48612418819105346]\n","15  hi  [0.5116484418166757, 0.0, 0.0, 0.4729573817364559]\n","16  hi  [0.5062107551224296, 0.0, 0.0, 0.4937892448775704]\n","17  hi  [0.5022122467089497, 0.0, 0.0, 0.47849130919078126]\n","19 [2, 2, 2, 2, 2, 2, 2, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n","0  hi  [0.0, 0.4794072395262148, 0.5067632158806542, 0.0]\n","1  hi  [0.0, 0.4867019935952211, 0.513298006404779, 0.0]\n","2  hi  [0.0, 0.4832877612600352, 0.5167122387399649, 0.0]\n","3  hi  [0.0, 0.49284908395001825, 0.5033400700746969, 0.0]\n","4  hi  [0.0, 0.49236424706119153, 0.5076357529388085, 0.0]\n","5  hi  [0.0, 0.4896686930174427, 0.49347820791635966, 0.0]\n","6  hi  [0.0, 0.5185881655554313, 0.48141183444456864, 0.0]\n","7  hi  [0.0, 0.4597758461052946, 0.4975019624129801, 0.04272219148172515]\n","8  hi  [0.0, 0.5198209105248219, 0.4801790894751782, 0.0]\n","9  hi  [0.49414367241466095, 0.0, 0.0, 0.4973729500820841]\n","10  hi  [0.4925962870732955, 0.0, 0.0, 0.4979187721131479]\n","11  hi  [0.4927496971615669, 0.0, 0.0, 0.507250302838433]\n","12  hi  [0.4969445758525514, 0.0, 0.0, 0.4989144393981777]\n","13  hi  [0.48267633953116224, 0.0, 0.0, 0.4842723406538445]\n","14  hi  [0.5013527481735437, 0.0, 0.0, 0.48612418819105346]\n","15  hi  [0.5116484418166757, 0.0, 0.0, 0.4729573817364559]\n","16  hi  [0.5062107551224296, 0.0, 0.0, 0.4937892448775704]\n","17  hi  [0.4838640068158448, 0.0, 0.0, 0.5161359931841553]\n","20 [2, 2, 2, 2, 2, 2, 2, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n","0  hi  [0.0, 0.4811652930820966, 0.5188347069179035, 0.0]\n","1  hi  [0.0, 0.4867019935952211, 0.513298006404779, 0.0]\n","2  hi  [0.0, 0.5063055798326503, 0.49369442016734966, 0.0]\n","3  hi  [0.0, 0.49284908395001825, 0.5033400700746969, 0.0]\n","4  hi  [0.0, 0.49236424706119153, 0.5076357529388085, 0.0]\n","5  hi  [0.0, 0.4896686930174427, 0.49347820791635966, 0.0]\n","6  hi  [0.0, 0.500322592571639, 0.49080375792065456, 0.0]\n","7  hi  [0.0, 0.4597758461052946, 0.4975019624129801, 0.04272219148172515]\n","8  hi  [0.0, 0.5076605176559, 0.4862509007627767, 0.0]\n","9  hi  [0.4654650589900231, 0.0, 0.0, 0.5128506153054851]\n","10  hi  [0.4925962870732955, 0.0, 0.0, 0.4979187721131479]\n","11  hi  [0.5173275657533563, 0.0, 0.0, 0.4826724342466437]\n","12  hi  [0.4969445758525514, 0.0, 0.0, 0.4989144393981777]\n","13  hi  [0.48267633953116224, 0.0, 0.0, 0.4842723406538445]\n","14  hi  [0.4919626610406297, 0.0, 0.0, 0.5002211824118497]\n","15  hi  [0.512955933119297, 0.0, 0.0, 0.48704406688070306]\n","16  hi  [0.5367198173061541, 0.0, 0.0, 0.45706176678268007]\n","17  hi  [0.4838640068158448, 0.0, 0.0, 0.5161359931841553]\n","21 [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n","0  hi  [0.0, 0.4811652930820966, 0.5188347069179035, 0.0]\n","1  hi  [0.0, 0.4867019935952211, 0.513298006404779, 0.0]\n","2  hi  [0.0, 0.4902760130585873, 0.49205168351821355, 0.0]\n","3  hi  [0.0, 0.49284908395001825, 0.5033400700746969, 0.0]\n","4  hi  [0.0, 0.5045670512004197, 0.47610339747149866, 0.0]\n","5  hi  [0.0, 0.4896686930174427, 0.49347820791635966, 0.0]\n","6  hi  [0.0, 0.500322592571639, 0.49080375792065456, 0.0]\n","7  hi  [0.0, 0.4971796894477157, 0.5028203105522844, 0.0]\n","8  hi  [0.0, 0.5184133702817216, 0.4815866297182784, 0.0]\n","9  hi  [0.5087414747956139, 0.0, 0.0, 0.46995635894379695]\n","10  hi  [0.4925962870732955, 0.0, 0.0, 0.4979187721131479]\n","11  hi  [0.5173275657533563, 0.0, 0.0, 0.4826724342466437]\n","12  hi  [0.4969445758525514, 0.0, 0.0, 0.4989144393981777]\n","13  hi  [0.48267633953116224, 0.0, 0.0, 0.4842723406538445]\n","14  hi  [0.5075703701304232, 0.0, 0.0, 0.4861280599122067]\n","15  hi  [0.5125795605608057, 0.0, 0.0, 0.4874204394391943]\n","16  hi  [0.48824148282150315, 0.0, 0.0, 0.4812040229162767]\n","17  hi  [0.5262863976912672, 0.0, 0.0, 0.4708359032482847]\n","22 [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n","0  hi  [0.0, 0.5088788392324094, 0.49112116076759066, 0.0]\n","1  hi  [0.0, 0.4886233227641489, 0.5113766772358511, 0.0]\n","2  hi  [0.0, 0.5094622338153576, 0.49053776618464245, 0.0]\n","3  hi  [0.0, 0.5092724984728777, 0.45783372346625345, 0.0]\n","4  hi  [0.0, 0.5090978998284782, 0.4909021001715219, 0.0]\n","5  hi  [0.0, 0.5167521207345823, 0.4832478792654176, 0.0]\n","6  hi  [0.0, 0.49633876610970495, 0.503661233890295, 0.0]\n","7  hi  [0.0, 0.4971796894477157, 0.5028203105522844, 0.0]\n","8  hi  [0.0, 0.5184133702817216, 0.4815866297182784, 0.0]\n","9  hi  [0.47269129599411747, 0.0, 0.0, 0.4873210288778817]\n","10  hi  [0.4925962870732955, 0.0, 0.0, 0.4979187721131479]\n","11  hi  [0.5173275657533563, 0.0, 0.0, 0.4826724342466437]\n","12  hi  [0.4969445758525514, 0.0, 0.0, 0.4989144393981777]\n","13  hi  [0.48267633953116224, 0.0, 0.0, 0.4842723406538445]\n","14  hi  [0.5075703701304232, 0.0, 0.0, 0.4861280599122067]\n","15  hi  [0.5125795605608057, 0.0, 0.0, 0.4874204394391943]\n","16  hi  [0.48824148282150315, 0.0, 0.0, 0.4812040229162767]\n","17  hi  [0.4738263383187884, 0.0, 0.0, 0.4850736136030248]\n","23 [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n","0  hi  [0.0, 0.5088788392324094, 0.49112116076759066, 0.0]\n","1  hi  [0.0, 0.5036152255916642, 0.49577030659793325, 0.0]\n","2  hi  [0.0, 0.5094152128158748, 0.4905847871841251, 0.0]\n","3  hi  [0.0, 0.5092724984728777, 0.45783372346625345, 0.0]\n","4  hi  [0.0, 0.5090978998284782, 0.4909021001715219, 0.0]\n","5  hi  [0.0, 0.5167521207345823, 0.4832478792654176, 0.0]\n","6  hi  [0.0, 0.49633876610970495, 0.503661233890295, 0.0]\n","7  hi  [0.0, 0.4967658628948361, 0.46599632659867285, 0.0]\n","8  hi  [0.0, 0.5107534234366783, 0.4892465765633216, 0.0]\n","9  hi  [0.47269129599411747, 0.0, 0.0, 0.4873210288778817]\n","10  hi  [0.4925962870732955, 0.0, 0.0, 0.4979187721131479]\n","11  hi  [0.4918650439080891, 0.0, 0.0, 0.49455484604417915]\n","12  hi  [0.5090976417583096, 0.0, 0.0, 0.49090235824169043]\n","13  hi  [0.48857908097416575, 0.0, 0.0, 0.5114209190258343]\n","14  hi  [0.5075703701304232, 0.0, 0.0, 0.4861280599122067]\n","15  hi  [0.5125795605608057, 0.0, 0.0, 0.4874204394391943]\n","16  hi  [0.5185827279464333, 0.0, 0.0, 0.48141727205356677]\n","17  hi  [0.5015774291063717, 0.0, 0.0, 0.49842257089362835]\n","24 [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n","0  hi  [0.0, 0.5029076139689396, 0.4970923860310605, 0.0]\n","1  hi  [0.0, 0.5036152255916642, 0.49577030659793325, 0.0]\n","2  hi  [0.0, 0.5094152128158748, 0.4905847871841251, 0.0]\n","3  hi  [0.0, 0.49436617232027613, 0.5056338276797239, 0.0]\n","4  hi  [0.0, 0.5011573147837792, 0.45780377866242544, 0.0]\n","5  hi  [0.0, 0.5056379169012475, 0.4899764455116518, 0.0]\n","6  hi  [0.0, 0.49633876610970495, 0.503661233890295, 0.0]\n","7  hi  [0.0, 0.4967658628948361, 0.46599632659867285, 0.0]\n","8  hi  [0.0, 0.5107534234366783, 0.4892465765633216, 0.0]\n","9  hi  [0.47269129599411747, 0.0, 0.0, 0.4873210288778817]\n","10  hi  [0.4925962870732955, 0.0, 0.0, 0.4979187721131479]\n","11  hi  [0.5066780497825524, 0.0, 0.0, 0.48340466427512957]\n","12  hi  [0.5029169617820457, 0.0, 0.0, 0.4970830382179542]\n","13  hi  [0.48857908097416575, 0.0, 0.0, 0.5114209190258343]\n","14  hi  [0.4936724652985496, 0.0, 0.0, 0.5063275347014504]\n","15  hi  [0.4950126207501027, 0.0, 0.0, 0.5049873792498975]\n","16  hi  [0.511084186222178, 0.0, 0.0, 0.47970880952857]\n","17  hi  [0.5015774291063717, 0.0, 0.0, 0.49842257089362835]\n","25 [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n","0  hi  [0.0, 0.5029076139689396, 0.4970923860310605, 0.0]\n","1  hi  [0.0, 0.5036152255916642, 0.49577030659793325, 0.0]\n","2  hi  [0.0, 0.5094152128158748, 0.4905847871841251, 0.0]\n","3  hi  [0.0, 0.49436617232027613, 0.5056338276797239, 0.0]\n","4  hi  [0.0, 0.5011573147837792, 0.45780377866242544, 0.0]\n","5  hi  [0.0, 0.5056379169012475, 0.4899764455116518, 0.0]\n","6  hi  [0.0, 0.49633876610970495, 0.503661233890295, 0.0]\n","7  hi  [0.0, 0.4967658628948361, 0.46599632659867285, 0.0]\n","8  hi  [0.0, 0.5107534234366783, 0.4892465765633216, 0.0]\n","9  hi  [0.47269129599411747, 0.0, 0.0, 0.4873210288778817]\n","10  hi  [0.4925962870732955, 0.0, 0.0, 0.4979187721131479]\n","11  hi  [0.5066780497825524, 0.0, 0.0, 0.48340466427512957]\n","12  hi  [0.5029169617820457, 0.0, 0.0, 0.4970830382179542]\n","13  hi  [0.48857908097416575, 0.0, 0.0, 0.5114209190258343]\n","14  hi  [0.4936724652985496, 0.0, 0.0, 0.5063275347014504]\n","15  hi  [0.4950126207501027, 0.0, 0.0, 0.5049873792498975]\n","16  hi  [0.511084186222178, 0.0, 0.0, 0.47970880952857]\n","17  hi  [0.5015774291063717, 0.0, 0.0, 0.49842257089362835]\n","26 [2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 2, 3, 3, 3]\n","0  hi  [0.0, 0.4796274938234701, 0.47321483386575175, 0.0, 0.0, 0.0, 0.0, 0.0]\n","1  hi  [0.0, 0.4983810859631893, 0.4851505981194047, 0.0, 0.0, 0.0, 0.0, 0.0]\n","2  hi  [0.0, 0.3423493180244539, 0.326094209902583, 0.0, 0.0, 0.0, 0.3258864046981502, 0.0]\n","3  hi  [0.0, 0.5016923123653099, 0.4983076876346902, 0.0, 0.0, 0.0, 0.0, 0.0]\n","4  hi  [0.0, 0.4735861774316847, 0.42921884587711473, 0.0, 0.0, 0.0, 0.0, 0.0]\n","5  hi  [0.0, 0.47168817368910465, 0.47997846235835534, 0.0, 0.0, 0.0, 0.0, 0.0]\n","6  hi  [0.0, 0.500019830188898, 0.499980169811102, 0.0, 0.0, 0.0, 0.0, 0.0]\n","7  hi  [0.0, 0.498645090959477, 0.49348487293455595, 0.0, 0.0, 0.0, 0.0, 0.0]\n","8  hi  [0.0, 0.4938835284166369, 0.48498150078474545, 0.0, 0.0, 0.0, 0.0, 0.0]\n","9  hi  [0.4955621575752494, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5044378424247505]\n","10  hi  [0.4704486033613025, 0.0, 0.0, 0.4496690831961733, 0.0, 0.0, 0.0, 0.05589007877434917]\n","11  hi  [0.47250612628932587, 0.0, 0.0, 0.44194934429447585, 0.0, 0.0, 0.0, 0.07589517837933893]\n","12  hi  [0.36149830164250396, 0.0, 0.0, 0.3264798844120783, 0.0, 0.0, 0.0, 0.3120218139454177]\n","13  hi  [0.47754786007298683, 0.0, 0.0, 0.46131839251889895, 0.0, 0.0, 0.0, 0.061008514018794806]\n","14  hi  [0.46216974938046296, 0.0, 0.0, 0.4440231451975281, 0.0, 0.0, 0.0, 0.0]\n","15  hi  [0.45038502706831457, 0.0, 0.0, 0.4439822903753295, 0.0, 0.0, 0.0, 0.10563268255635586]\n","16  hi  [0.44414312601042194, 0.0, 0.0, 0.40784872871241623, 0.0, 0.0, 0.0, 0.1371359786167062]\n","17  hi  [0.4452368786678797, 0.0, 0.0, 0.46208249083492176, 0.0, 0.0, 0.0, 0.09268063049719853]\n","27 [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 2, 2, 2, 3, 3, 2]\n","0  hi  [0.0, 0.4796274938234701, 0.47321483386575175, 0.0, 0.0, 0.0, 0.0, 0.0]\n","1  hi  [0.0, 0.49018179194418854, 0.46680266414651234, 0.0, 0.0, 0.0, 0.0, 0.0]\n","2  hi  [0.0, 0.5065005560902953, 0.4779443945351304, 0.0, 0.0, 0.0, 0.0, 0.0]\n","3  hi  [0.0, 0.47251856723274255, 0.47867001302917495, 0.0, 0.0, 0.0, 0.0, 0.0]\n","4  hi  [0.0, 0.4735861774316847, 0.42921884587711473, 0.0, 0.0, 0.0, 0.0, 0.0]\n","5  hi  [0.0, 0.47168817368910465, 0.47997846235835534, 0.0, 0.0, 0.0, 0.0, 0.0]\n","6  hi  [0.0, 0.500019830188898, 0.499980169811102, 0.0, 0.0, 0.0, 0.0, 0.0]\n","7  hi  [0.0, 0.4975143074221591, 0.5024856925778408, 0.0, 0.0, 0.0, 0.0, 0.0]\n","8  hi  [0.0, 0.4938835284166369, 0.48498150078474545, 0.0, 0.0, 0.0, 0.0, 0.0]\n","9  hi  [0.4955621575752494, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5044378424247505]\n","10  hi  [0.4704486033613025, 0.0, 0.0, 0.4496690831961733, 0.0, 0.0, 0.0, 0.05589007877434917]\n","11  hi  [0.33198509796191256, 0.0, 0.0, 0.3162555078990247, 0.0, 0.0, 0.0, 0.35175939413906276]\n","12  hi  [0.5024555927829262, 0.0, 0.0, 0.4538920602977585, 0.0, 0.0, 0.0, 0.0]\n","13  hi  [0.4822662166674983, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.43316947984054455]\n","14  hi  [0.46216974938046296, 0.0, 0.0, 0.4440231451975281, 0.0, 0.0, 0.0, 0.0]\n","15  hi  [0.45038502706831457, 0.0, 0.0, 0.4439822903753295, 0.0, 0.0, 0.0, 0.10563268255635586]\n","16  hi  [0.3524043607631479, 0.0, 0.0, 0.32462186122659115, 0.0, 0.0, 0.0, 0.32273730617357654]\n","17  hi  [0.4821445135956766, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4846111262139901]\n","28 [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 3, 2]\n","0  hi  [0.0, 0.4796274938234701, 0.47321483386575175, 0.0, 0.0, 0.0, 0.0, 0.0]\n","1  hi  [0.0, 0.49018179194418854, 0.46680266414651234, 0.0, 0.0, 0.0, 0.0, 0.0]\n","2  hi  [0.0, 0.5065005560902953, 0.4779443945351304, 0.0, 0.0, 0.0, 0.0, 0.0]\n","3  hi  [0.0, 0.4916573885424741, 0.5083426114575259, 0.0, 0.0, 0.0, 0.0, 0.0]\n","4  hi  [0.0, 0.512328594819111, 0.487671405180889, 0.0, 0.0, 0.0, 0.0, 0.0]\n","5  hi  [0.0, 0.47168817368910465, 0.47997846235835534, 0.0, 0.0, 0.0, 0.0, 0.0]\n","6  hi  [0.0, 0.500019830188898, 0.499980169811102, 0.0, 0.0, 0.0, 0.0, 0.0]\n","7  hi  [0.0, 0.4694690127953474, 0.47620904520419405, 0.0, 0.0, 0.0, 0.0, 0.0]\n","8  hi  [0.0, 0.4938835284166369, 0.48498150078474545, 0.0, 0.0, 0.0, 0.0, 0.0]\n","9  hi  [0.6117339385877107, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.38826606141228925]\n","10  hi  [0.5082797938331897, 0.0, 0.0, 0.48426217720151155, 0.0, 0.0, 0.0, 0.0]\n","11  hi  [0.33198509796191256, 0.0, 0.0, 0.3162555078990247, 0.0, 0.0, 0.0, 0.35175939413906276]\n","12  hi  [0.5061548699592096, 0.0, 0.0, 0.45701069549907725, 0.0, 0.0, 0.0, 0.0]\n","13  hi  [0.5745700198881218, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4254299801118782]\n","14  hi  [0.46216974938046296, 0.0, 0.0, 0.4440231451975281, 0.0, 0.0, 0.0, 0.0]\n","15  hi  [0.490500972406815, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5094990275931849]\n","16  hi  [0.3524043607631479, 0.0, 0.0, 0.32462186122659115, 0.0, 0.0, 0.0, 0.32273730617357654]\n","17  hi  [0.5210216700855487, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.39996317433600004]\n","29 [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n","0  hi  [0.0, 0.5013071470162307, 0.49869285298376936, 0.0, 0.0, 0.0, 0.0, 0.0]\n","1  hi  [0.0, 0.49018179194418854, 0.46680266414651234, 0.0, 0.0, 0.0, 0.0, 0.0]\n","2  hi  [0.0, 0.4635876858516559, 0.41527421183528784, 0.0, 0.0, 0.0, 0.0, 0.0]\n","3  hi  [0.0, 0.4916573885424741, 0.5083426114575259, 0.0, 0.0, 0.0, 0.0, 0.0]\n","4  hi  [0.0, 0.5134237303862856, 0.4865762696137144, 0.0, 0.0, 0.0, 0.0, 0.0]\n","5  hi  [0.0, 0.47168817368910465, 0.47997846235835534, 0.0, 0.0, 0.0, 0.0, 0.0]\n","6  hi  [0.0, 0.510783081427585, 0.4892169185724151, 0.0, 0.0, 0.0, 0.0, 0.0]\n","7  hi  [0.0, 0.5016913473234141, 0.4983086526765859, 0.0, 0.0, 0.0, 0.0, 0.0]\n","8  hi  [0.0, 0.4938835284166369, 0.48498150078474545, 0.0, 0.0, 0.0, 0.0, 0.0]\n","9  hi  [0.6117339385877107, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.38826606141228925]\n","10  hi  [0.49856873614542097, 0.0, 0.0, 0.501431263854579, 0.0, 0.0, 0.0, 0.0]\n","11  hi  [0.49658477617250774, 0.0, 0.0, 0.4777132380590143, 0.0, 0.0, 0.0, 0.0]\n","12  hi  [0.4937408071923654, 0.0, 0.0, 0.5062591928076345, 0.0, 0.0, 0.0, 0.0]\n","13  hi  [0.5745700198881218, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4254299801118782]\n","14  hi  [0.46216974938046296, 0.0, 0.0, 0.4440231451975281, 0.0, 0.0, 0.0, 0.0]\n","15  hi  [0.490500972406815, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5094990275931849]\n","16  hi  [0.5221861220525469, 0.0, 0.0, 0.4753262914795674, 0.0, 0.0, 0.0, 0.0]\n","17  hi  [0.5210216700855487, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.39996317433600004]\n","30 [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n","0  hi  [0.0, 0.5013071470162307, 0.49869285298376936, 0.0, 0.0, 0.0, 0.0, 0.0]\n","1  hi  [0.0, 0.49018179194418854, 0.46680266414651234, 0.0, 0.0, 0.0, 0.0, 0.0]\n","2  hi  [0.0, 0.4635876858516559, 0.41527421183528784, 0.0, 0.0, 0.0, 0.0, 0.0]\n","3  hi  [0.0, 0.4916573885424741, 0.5083426114575259, 0.0, 0.0, 0.0, 0.0, 0.0]\n","4  hi  [0.0, 0.5134237303862856, 0.4865762696137144, 0.0, 0.0, 0.0, 0.0, 0.0]\n","5  hi  [0.0, 0.47168817368910465, 0.47997846235835534, 0.0, 0.0, 0.0, 0.0, 0.0]\n","6  hi  [0.0, 0.510783081427585, 0.4892169185724151, 0.0, 0.0, 0.0, 0.0, 0.0]\n","7  hi  [0.0, 0.5016913473234141, 0.4983086526765859, 0.0, 0.0, 0.0, 0.0, 0.0]\n","8  hi  [0.0, 0.4938835284166369, 0.48498150078474545, 0.0, 0.0, 0.0, 0.0, 0.0]\n","9  hi  [0.6117339385877107, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.38826606141228925]\n","10  hi  [0.49856873614542097, 0.0, 0.0, 0.501431263854579, 0.0, 0.0, 0.0, 0.0]\n","11  hi  [0.49658477617250774, 0.0, 0.0, 0.4777132380590143, 0.0, 0.0, 0.0, 0.0]\n","12  hi  [0.4937408071923654, 0.0, 0.0, 0.5062591928076345, 0.0, 0.0, 0.0, 0.0]\n","13  hi  [0.5745700198881218, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4254299801118782]\n","14  hi  [0.46216974938046296, 0.0, 0.0, 0.4440231451975281, 0.0, 0.0, 0.0, 0.0]\n","15  hi  [0.490500972406815, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5094990275931849]\n","16  hi  [0.5221861220525469, 0.0, 0.0, 0.4753262914795674, 0.0, 0.0, 0.0, 0.0]\n","17  hi  [0.5210216700855487, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.39996317433600004]\n","31 [2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 3, 2, 3, 2, 2]\n","0  hi  [0.0, 0.499635371847961, 0.5003646281520391, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","1  hi  [0.0, 0.52211128200454, 0.47788871799545996, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","2  hi  [0.0, 0.4927171320157475, 0.4780502846871047, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","3  hi  [0.0, 0.5148338870241906, 0.48516611297580936, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","4  hi  [0.0, 0.43550072521752853, 0.4061199746039822, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","5  hi  [0.0, 0.5122366755730484, 0.48776332442695153, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","6  hi  [0.0, 0.4928292505088943, 0.5071707494911056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","7  hi  [0.0, 0.48510914292075924, 0.5148908570792409, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","8  hi  [0.0, 0.4024715209079011, 0.3657515973823442, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","9  hi  [0.45901071529762505, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4224442498169557, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11854503488541926]\n","10  hi  [0.4478338525088709, 0.0, 0.0, 0.40440197021278634, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","11  hi  [0.5312221877548656, 0.0, 0.0, 0.46877781224513443, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","12  hi  [0.35646139252063164, 0.0, 0.0, 0.37625783334489005, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","13  hi  [0.34808629300511756, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.331722051569348, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3201916554255345]\n","14  hi  [0.5059626697252328, 0.0, 0.0, 0.4940373302747672, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","15  hi  [0.4840204502603208, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.33076827675020365, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12742998997693666]\n","16  hi  [0.5095971915266726, 0.0, 0.0, 0.49040280847332735, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","17  hi  [0.5104064534681518, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4895935465318482, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","32 [2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2]\n","0  hi  [0.0, 0.499635371847961, 0.5003646281520391, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","1  hi  [0.0, 0.52211128200454, 0.47788871799545996, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","2  hi  [0.0, 0.4927171320157475, 0.4780502846871047, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","3  hi  [0.0, 0.5148338870241906, 0.48516611297580936, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","4  hi  [0.0, 0.43550072521752853, 0.4061199746039822, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","5  hi  [0.0, 0.46311526246126017, 0.4605071193922774, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","6  hi  [0.0, 0.5053874035998731, 0.49461259640012684, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","7  hi  [0.0, 0.48510914292075924, 0.5148908570792409, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","8  hi  [0.0, 0.48907261251555306, 0.4855949329502408, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","9  hi  [0.45901071529762505, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4224442498169557, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11854503488541926]\n","10  hi  [0.4985508946252957, 0.0, 0.0, 0.5014491053747042, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","11  hi  [0.5113307808835811, 0.0, 0.0, 0.4886692191164188, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","12  hi  [0.35646139252063164, 0.0, 0.0, 0.37625783334489005, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","13  hi  [0.5253803796366875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.47461962036331257, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","14  hi  [0.5141922903226256, 0.0, 0.0, 0.4729708925849199, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","15  hi  [0.4645591769343533, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4318717857986983, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","16  hi  [0.5095971915266726, 0.0, 0.0, 0.49040280847332735, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","17  hi  [0.46717133511692543, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4461097166206844, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","33 [2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2]\n","0  hi  [0.0, 0.4584109213333444, 0.4586802028109216, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","1  hi  [0.0, 0.52211128200454, 0.47788871799545996, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","2  hi  [0.0, 0.4927171320157475, 0.4780502846871047, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","3  hi  [0.0, 0.49910605449081585, 0.5008939455091842, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","4  hi  [0.0, 0.49854328910204365, 0.5014567108979564, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","5  hi  [0.0, 0.46311526246126017, 0.4605071193922774, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","6  hi  [0.0, 0.5028366176389953, 0.4841551635388098, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","7  hi  [0.0, 0.4450014857571879, 0.4553832379792356, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","8  hi  [0.0, 0.48907261251555306, 0.4855949329502408, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","9  hi  [0.45901071529762505, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4224442498169557, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11854503488541926]\n","10  hi  [0.5245617952378484, 0.0, 0.0, 0.47543820476215154, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","11  hi  [0.5113307808835811, 0.0, 0.0, 0.4886692191164188, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","12  hi  [0.3509876158137005, 0.0, 0.0, 0.37624497968809667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","13  hi  [0.5253803796366875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.47461962036331257, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","14  hi  [0.5141922903226256, 0.0, 0.0, 0.4729708925849199, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","15  hi  [0.4148703798312734, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4122600870647899, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","16  hi  [0.5095971915266726, 0.0, 0.0, 0.49040280847332735, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","17  hi  [0.4463228278976502, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4300488987142651, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","34 [2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2]\n","0  hi  [0.0, 0.4584109213333444, 0.4586802028109216, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","1  hi  [0.0, 0.52211128200454, 0.47788871799545996, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","2  hi  [0.0, 0.48651762974961194, 0.4613512966723496, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","3  hi  [0.0, 0.44014718393276303, 0.40791815349193045, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","4  hi  [0.0, 0.5035035445100221, 0.49649645548997784, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","5  hi  [0.0, 0.5036801705024206, 0.49631982949757936, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","6  hi  [0.0, 0.5292127883845814, 0.47078721161541864, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","7  hi  [0.0, 0.4450014857571879, 0.4553832379792356, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","8  hi  [0.0, 0.4519659368501697, 0.45441918776964735, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","9  hi  [0.31026713052265836, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.280365617200878, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.23599100508707585]\n","10  hi  [0.5071120613522602, 0.0, 0.0, 0.4928879386477397, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","11  hi  [0.5113307808835811, 0.0, 0.0, 0.4886692191164188, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","12  hi  [0.3509876158137005, 0.0, 0.0, 0.37624497968809667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","13  hi  [0.5253803796366875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.47461962036331257, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","14  hi  [0.5141922903226256, 0.0, 0.0, 0.4729708925849199, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","15  hi  [0.4148703798312734, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4122600870647899, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","16  hi  [0.5325499267843312, 0.0, 0.0, 0.46745007321566884, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","17  hi  [0.4463228278976502, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4300488987142651, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","35 [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n","0  hi  [0.0, 0.4584109213333444, 0.4586802028109216, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","1  hi  [0.0, 0.5138616940197868, 0.48613830598021324, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","2  hi  [0.0, 0.48651762974961194, 0.4613512966723496, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","3  hi  [0.0, 0.49292663210229715, 0.4805205709200136, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","4  hi  [0.0, 0.5035035445100221, 0.49649645548997784, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","5  hi  [0.0, 0.5036801705024206, 0.49631982949757936, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","6  hi  [0.0, 0.5292127883845814, 0.47078721161541864, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","7  hi  [0.0, 0.4392903649335963, 0.41452134821806785, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","8  hi  [0.0, 0.4811171326960514, 0.4773368740279773, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","9  hi  [0.30544288060395314, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2788625238341743, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","10  hi  [0.5071120613522602, 0.0, 0.0, 0.4928879386477397, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","11  hi  [0.49090964600087955, 0.0, 0.0, 0.5090903539991204, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","12  hi  [0.3509876158137005, 0.0, 0.0, 0.37624497968809667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","13  hi  [0.5253803796366875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.47461962036331257, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","14  hi  [0.5220024205427329, 0.0, 0.0, 0.4779975794572672, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","15  hi  [0.47040826288319726, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.43864500416119195, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","16  hi  [0.4560736857316916, 0.0, 0.0, 0.43921064467248966, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","17  hi  [0.4463228278976502, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4300488987142651, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","36 [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n","0  hi  [0.0, 0.4414052074621969, 0.4460054753023508, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","1  hi  [0.0, 0.5064725888428511, 0.49352741115714893, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","2  hi  [0.0, 0.48651762974961194, 0.4613512966723496, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","3  hi  [0.0, 0.49292663210229715, 0.4805205709200136, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","4  hi  [0.0, 0.42907623024337554, 0.4374179172147196, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","5  hi  [0.0, 0.5036801705024206, 0.49631982949757936, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","6  hi  [0.0, 0.5072171449559878, 0.49278285504401215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","7  hi  [0.0, 0.4392903649335963, 0.41452134821806785, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","8  hi  [0.0, 0.5027571098674192, 0.4972428901325807, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","9  hi  [0.5090603591047832, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.49093964089521686, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","10  hi  [0.47913228542039077, 0.0, 0.0, 0.4521247431109813, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","11  hi  [0.49090964600087955, 0.0, 0.0, 0.5090903539991204, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","12  hi  [0.44122071460591117, 0.0, 0.0, 0.40348838480762705, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","13  hi  [0.5253803796366875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.47461962036331257, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","14  hi  [0.5220024205427329, 0.0, 0.0, 0.4779975794572672, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","15  hi  [0.47040826288319726, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.43864500416119195, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","16  hi  [0.5086430897876303, 0.0, 0.0, 0.4817046773865462, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","17  hi  [0.4463228278976502, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4300488987142651, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","37 [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n","0  hi  [0.0, 0.4414052074621969, 0.4460054753023508, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","1  hi  [0.0, 0.5064725888428511, 0.49352741115714893, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","2  hi  [0.0, 0.48651762974961194, 0.4613512966723496, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","3  hi  [0.0, 0.44876233877560584, 0.42467707959178946, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","4  hi  [0.0, 0.42907623024337554, 0.4374179172147196, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","5  hi  [0.0, 0.5036801705024206, 0.49631982949757936, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","6  hi  [0.0, 0.5072171449559878, 0.49278285504401215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","7  hi  [0.0, 0.4694939640068784, 0.45598798180824435, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","8  hi  [0.0, 0.5033684820392932, 0.49663151796070687, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","9  hi  [0.48245514198014766, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5175448580198523, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","10  hi  [0.47913228542039077, 0.0, 0.0, 0.4521247431109813, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","11  hi  [0.4928889954835667, 0.0, 0.0, 0.47119034602106713, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","12  hi  [0.4403175897366365, 0.0, 0.0, 0.4391933176683605, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","13  hi  [0.5214535889006872, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4785464110993128, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","14  hi  [0.5220024205427329, 0.0, 0.0, 0.4779975794572672, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","15  hi  [0.47040826288319726, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.43864500416119195, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","16  hi  [0.46449667074917206, 0.0, 0.0, 0.4265153751956213, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","17  hi  [0.5015546385718032, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4984453614281967, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","38 [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n","0  hi  [0.0, 0.4788734236964745, 0.5211265763035254, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","1  hi  [0.0, 0.4840367794164923, 0.4685688415245556, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","2  hi  [0.0, 0.47484181989809504, 0.4674813049889905, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","3  hi  [0.0, 0.44876233877560584, 0.42467707959178946, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","4  hi  [0.0, 0.42907623024337554, 0.4374179172147196, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","5  hi  [0.0, 0.4161545610709442, 0.375017155524848, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","6  hi  [0.0, 0.5072171449559878, 0.49278285504401215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","7  hi  [0.0, 0.4694939640068784, 0.45598798180824435, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","8  hi  [0.0, 0.5005615856339046, 0.49540142093476147, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","9  hi  [0.48245514198014766, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5175448580198523, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","10  hi  [0.4459932836996044, 0.0, 0.0, 0.44552119201888346, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","11  hi  [0.4932253850177677, 0.0, 0.0, 0.5067746149822323, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","12  hi  [0.47211971176824363, 0.0, 0.0, 0.44761587848111156, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","13  hi  [0.5214535889006872, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4785464110993128, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","14  hi  [0.5017475461499808, 0.0, 0.0, 0.4982524538500191, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","15  hi  [0.47040826288319726, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.43864500416119195, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","16  hi  [0.46449667074917206, 0.0, 0.0, 0.4265153751956213, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","17  hi  [0.5015546385718032, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4984453614281967, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","39 [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n","0  hi  [0.0, 0.49023399438687726, 0.5097660056131227, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","1  hi  [0.0, 0.4840367794164923, 0.4685688415245556, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","2  hi  [0.0, 0.47484181989809504, 0.4674813049889905, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","3  hi  [0.0, 0.44876233877560584, 0.42467707959178946, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","4  hi  [0.0, 0.42907623024337554, 0.4374179172147196, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","5  hi  [0.0, 0.49120219055824765, 0.4811728088795652, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","6  hi  [0.0, 0.5072171449559878, 0.49278285504401215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","7  hi  [0.0, 0.437230560763531, 0.45076349087938367, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","8  hi  [0.0, 0.5005615856339046, 0.49540142093476147, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","9  hi  [0.408000665946188, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3516868658867655, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","10  hi  [0.4626925386466432, 0.0, 0.0, 0.46020520099261253, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","11  hi  [0.4932253850177677, 0.0, 0.0, 0.5067746149822323, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","12  hi  [0.4946935014950182, 0.0, 0.0, 0.4849191426365764, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","13  hi  [0.5214535889006872, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4785464110993128, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","14  hi  [0.5030918331308564, 0.0, 0.0, 0.483414298270006, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","15  hi  [0.47040826288319726, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.43864500416119195, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","16  hi  [0.5051619622950817, 0.0, 0.0, 0.49483803770491824, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","17  hi  [0.5095225076153808, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.49047749238461924, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","40 [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n","0  hi  [0.0, 0.49023399438687726, 0.5097660056131227, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","1  hi  [0.0, 0.4840367794164923, 0.4685688415245556, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","2  hi  [0.0, 0.4117176156130249, 0.3792415127309694, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","3  hi  [0.0, 0.4211598812612783, 0.3912905695709423, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","4  hi  [0.0, 0.42907623024337554, 0.4374179172147196, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","5  hi  [0.0, 0.4631742612224091, 0.44688264413392936, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","6  hi  [0.0, 0.5142643180187186, 0.48573568198128136, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","7  hi  [0.0, 0.4837680471728892, 0.5162319528271109, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","8  hi  [0.0, 0.5005615856339046, 0.49540142093476147, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","9  hi  [0.44260301085318315, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.43287351558120485, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","10  hi  [0.4626925386466432, 0.0, 0.0, 0.46020520099261253, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","11  hi  [0.4899756792478414, 0.0, 0.0, 0.5100243207521586, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","12  hi  [0.5185882888583992, 0.0, 0.0, 0.48141171114160075, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","13  hi  [0.521751739956857, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.47824826004314297, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","14  hi  [0.5030918331308564, 0.0, 0.0, 0.483414298270006, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","15  hi  [0.47040826288319726, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.43864500416119195, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","16  hi  [0.5051619622950817, 0.0, 0.0, 0.49483803770491824, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","17  hi  [0.5095225076153808, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.49047749238461924, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","41 [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n","0  hi  [0.0, 0.49023399438687726, 0.5097660056131227, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","1  hi  [0.0, 0.4840367794164923, 0.4685688415245556, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","2  hi  [0.0, 0.5161536917685832, 0.4838463082314169, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","3  hi  [0.0, 0.4802525851001164, 0.4711736974728833, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","4  hi  [0.0, 0.42907623024337554, 0.4374179172147196, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","5  hi  [0.0, 0.4631742612224091, 0.44688264413392936, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","6  hi  [0.0, 0.5142643180187186, 0.48573568198128136, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","7  hi  [0.0, 0.4837680471728892, 0.5162319528271109, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","8  hi  [0.0, 0.5013349363997749, 0.4986650636002251, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","9  hi  [0.33604154389103635, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.36224667893736373, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","10  hi  [0.4626925386466432, 0.0, 0.0, 0.46020520099261253, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","11  hi  [0.4899756792478414, 0.0, 0.0, 0.5100243207521586, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","12  hi  [0.49420394400515044, 0.0, 0.0, 0.5057960559948497, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","13  hi  [0.4909320718208172, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5090679281791829, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","14  hi  [0.4964401300390302, 0.0, 0.0, 0.5035598699609698, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","15  hi  [0.47040826288319726, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.43864500416119195, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","16  hi  [0.5017274085250575, 0.0, 0.0, 0.49827259147494246, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","17  hi  [0.45262932596433214, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4412487897997544, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","42 [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n","0  hi  [0.0, 0.49023399438687726, 0.5097660056131227, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","1  hi  [0.0, 0.4840367794164923, 0.4685688415245556, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","2  hi  [0.0, 0.5161536917685832, 0.4838463082314169, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","3  hi  [0.0, 0.4802525851001164, 0.4711736974728833, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","4  hi  [0.0, 0.4903694585018986, 0.5096305414981014, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","5  hi  [0.0, 0.43958785358775854, 0.41751317433749896, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","6  hi  [0.0, 0.43988016649431844, 0.4531419007610223, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","7  hi  [0.0, 0.4837680471728892, 0.5162319528271109, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","8  hi  [0.0, 0.5013349363997749, 0.4986650636002251, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","9  hi  [0.4060464306636027, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3559784270080967, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","10  hi  [0.4501672743339247, 0.0, 0.0, 0.4513909722991191, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","11  hi  [0.4943793971510591, 0.0, 0.0, 0.5056206028489408, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","12  hi  [0.497876394150003, 0.0, 0.0, 0.5021236058499969, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","13  hi  [0.4909320718208172, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5090679281791829, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","14  hi  [0.4935308850689446, 0.0, 0.0, 0.5064691149310555, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","15  hi  [0.38524031796278224, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.38259690171735, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","16  hi  [0.5017274085250575, 0.0, 0.0, 0.49827259147494246, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","17  hi  [0.45262932596433214, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4412487897997544, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","43 [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n","0  hi  [0.0, 0.49023399438687726, 0.5097660056131227, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","1  hi  [0.0, 0.5033717680047277, 0.4966282319952723, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","2  hi  [0.0, 0.5161536917685832, 0.4838463082314169, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","3  hi  [0.0, 0.4983404234232176, 0.5016595765767824, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","4  hi  [0.0, 0.4903694585018986, 0.5096305414981014, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","5  hi  [0.0, 0.43958785358775854, 0.41751317433749896, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","6  hi  [0.0, 0.43988016649431844, 0.4531419007610223, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","7  hi  [0.0, 0.4837680471728892, 0.5162319528271109, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","8  hi  [0.0, 0.5013349363997749, 0.4986650636002251, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","9  hi  [0.5128915840498879, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4871084159501122, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","10  hi  [0.4501672743339247, 0.0, 0.0, 0.4513909722991191, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","11  hi  [0.5003615453759424, 0.0, 0.0, 0.49963845462405754, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","12  hi  [0.5024325766685944, 0.0, 0.0, 0.4975674233314055, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","13  hi  [0.48552665729466926, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5144733427053306, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","14  hi  [0.4989209272817282, 0.0, 0.0, 0.4851737075647043, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","15  hi  [0.38524031796278224, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.38259690171735, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","16  hi  [0.38808463046169706, 0.0, 0.0, 0.40275894963416287, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","17  hi  [0.4988434021812779, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5011565978187221, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","44 [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n","0  hi  [0.0, 0.49023399438687726, 0.5097660056131227, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","1  hi  [0.0, 0.4930644649385087, 0.5069355350614912, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","2  hi  [0.0, 0.5166720814939838, 0.48332791850601625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","3  hi  [0.0, 0.5071629038736153, 0.49283709612638466, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","4  hi  [0.0, 0.5006001703256395, 0.4993998296743605, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","5  hi  [0.0, 0.43958785358775854, 0.41751317433749896, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","6  hi  [0.0, 0.44210015895476595, 0.4550445875167324, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","7  hi  [0.0, 0.4837680471728892, 0.5162319528271109, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","8  hi  [0.0, 0.4853054514917583, 0.4790225924899602, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","9  hi  [0.5128915840498879, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4871084159501122, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","10  hi  [0.4817096565019788, 0.0, 0.0, 0.4600810938459754, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","11  hi  [0.5003615453759424, 0.0, 0.0, 0.49963845462405754, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","12  hi  [0.3972268675790771, 0.0, 0.0, 0.3518714045917937, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","13  hi  [0.48552665729466926, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5144733427053306, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","14  hi  [0.4989209272817282, 0.0, 0.0, 0.4851737075647043, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","15  hi  [0.38524031796278224, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.38259690171735, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","16  hi  [0.4343235764872637, 0.0, 0.0, 0.43604377417420076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","17  hi  [0.4988434021812779, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5011565978187221, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","Total training time: 4567.527480125427 seconds\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HfooNMNyTcte","colab_type":"code","colab":{}},"source":["model_id_lst = [0] \n","\n","## Device creation\n","devices = create_devices(net, trainset, validset, testset, train_idxs, valid_idxs, test_idxs_device, devices_bias, devices_archetype, num_devices=num_devices)\n","print('Devices', len(devices))\n","\n","## NON-IID Federated Learning\n","##  Control (no duplicating, no weighting)\n","##\n","##\n","start_time = time.time()\n","for round_num in range(rounds):\n","  \n","    # Part 1.3: Implement getting devices for each round here\n","    round_devices = get_devices_for_round(devices, device_pct)\n","    # print('Round Devices', round_devices)\n","\n","    print('--------------Round: ' + str(round_num) + \"-------------\")\n","    \n","    for device in round_devices:\n","        for model_id in model_id_lst:\n","            for local_epoch in range(local_epochs):\n","                train(local_epoch, device, model_id) \n","                validate(local_epoch, device, model_id) \n","                    # print()\n","                    # print(\"Device: \" + str(device.idx) + \" VALIDATION: model_id # \" + str(model_id))\n","                test_group(round_num, device, model_id, label_dict_valid, validset)   \n","\n","    # Part 1.3: Implement weight averaging here\n","    for model_id in model_id_lst:\n","        w_avg = model_average_weight(round_devices, model_id)\n","\n","        for device in devices:\n","            device.nets[model_id]['net'].load_state_dict(w_avg)\n","            device.nets[model_id]['optimizer'].zero_grad()\n","            device.nets[model_id]['optimizer'].step()\n","            device.nets[model_id]['scheduler'].step()\n","            \n","        # test accuracy with highest ranking model\n","        if((devices[0].active[model_id] == 1) and (devices[0].ranking[model_id] == max(devices[0].ranking))):\n","            print()\n","            print(\"ALL-TEST ACCURACY\")\n","            test(round_num, devices[0], model_id, arch_testset, test_dataloader)\n","            print(\"ALL-TEST TEST GROUPS ACCURACY\")\n","            test_group(round_num, devices[0], model_id, label_dict_test)\n","    \n","    for index in range(len(devices)):\n","        device = devices[index]\n","        max_model = device.ranking.index(max(device.ranking))\n","        print(\"TESTING WITH IID FROM DEVICE \", devices.index(device))\n","        test(round_num, device, max_model, device.device_testset, device.testloader)\n","\n","\n","total_time = time.time() - start_time\n","print('Total training time: {} seconds'.format(total_time))"],"execution_count":0,"outputs":[]}]}