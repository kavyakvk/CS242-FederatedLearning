{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Eric-Kavya-Jazz-CS242-Assignment-2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ed86822fea9148689a7d2f4b6d2c09c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ec2dc77ebb4b4f64b299fa315d5f71a0",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_24f5efe4f2dc4aed87779c5ceb87d0dd",
              "IPY_MODEL_a8c6431d919f4b6390d52d678eff3da6"
            ]
          }
        },
        "ec2dc77ebb4b4f64b299fa315d5f71a0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "24f5efe4f2dc4aed87779c5ceb87d0dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_27698cbff0ff436abb4cf6dbf77a225c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e335e6106d334bcc95815e699a8799b6"
          }
        },
        "a8c6431d919f4b6390d52d678eff3da6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9e4f2e62d2b8435a99ede1936a4841d5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170500096/? [00:30&lt;00:00, 17240372.84it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_068da22ba7894f5fb40efcbbbfa4636a"
          }
        },
        "27698cbff0ff436abb4cf6dbf77a225c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e335e6106d334bcc95815e699a8799b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9e4f2e62d2b8435a99ede1936a4841d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "068da22ba7894f5fb40efcbbbfa4636a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kavyakvk/CS242-FederatedLearning/blob/master/Eric_Kavya_Jazz_CS242_Assignment_2_copy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7ttBYeclcLI",
        "colab_type": "text"
      },
      "source": [
        "# CS242: Assignment 2\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQGm6JtgehIq",
        "colab_type": "text"
      },
      "source": [
        "> Harvard CS 242: Computing at Scale (Spring 2020)\n",
        "> \n",
        "> Instructor: Professor HT Kung\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRdPkqn04KqH",
        "colab_type": "text"
      },
      "source": [
        "### **Assignment Instructions**\n",
        "\n",
        "Read the following instructions carefully before starting the assignment and again before submitting your work:\n",
        "\n",
        "* This programming assignment must be completed with the same group used in Assignment 1.  **If you have any issues with this arrangement, please email Marcus.**\n",
        "* We expect this assignment to take more time than Assignment 1 (there is a more significant programming element as well as training time required for the models). Again, we suggest that you start your effort right away.\n",
        "* The assignment consists of two files: this Google Colab file (an .ipynb file) and a latex answer template ([download here](https://drive.google.com/open?id=1I-D2CCBZRICzCnEB881Ublp3tORN6Dq5)).\n",
        "* The Google Colab contains all assignment instructions and *Code Cells* that you will use to implement the programming components of the assignment (in Python).\n",
        "* We provide a significant amount of the code to make it easier to get started. In the *Code Cells*, please add comments to explain the purpose of each line of code in your implementation. **You will not receive credit for implementations that are not well documented.**\n",
        "* <font color='red'>**Deliverables highlighted in red**</font> are given in this Google Colab file. Use the latex answer template to write down answers for these deliverables.\n",
        "* Each group will submit both a PDF of your latex answers and your Google Colab file (.ipynb file) containing all completed *Code Cells* to \"Programming Assignment 2\" on Canvas. Only one submission per group. Check your .ipynb file using this [tool](https://htmtopdf.herokuapp.com/ipynbviewer/) before submitting to ensure that you completed all *Code Cells* (including detailed comments).\n",
        "* The assignment is due on April 13, 2020 at Noon EST.\n",
        "* Each part that you are asked to implement is relatively small in isolation, and should be easy to test.  We strongly recommend that you test each of these parts before training the large models so that you do not waste time training models when your implemention may have bugs.  For example, you should ensure that your sampling is being done correctly, as if you do incorrect sampling the model will still train, but your results will not be correct.  For a number of sections, we have provided checks you can run to ensure correctness prior to training the large model.\n",
        "\n",
        "-----\n",
        "The outline of this assignment with point values and training estimates is given below. Note that these training estimates represent a minimum running time which assume that your implementation is correct.\n",
        "\n",
        "1. Exploring Federated Learning (FL) [25 points] [Training Estimate: 2 hours]\n",
        "\n",
        "2. Non-IID Federated Learning and Fairness [30 points] [Training Estimate: 3 hours]\n",
        "\n",
        "3. Quantization of Local Models for Reduced Communication Cost [25 points] [Training Estimate: 3 hours]\n",
        "\n",
        "4. Extreme Anomaly Detection [20 points] [Training Estimate: 1.5 hours]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UvFA89jTuON",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "\n",
        "### **1. Exploring Federated Learning (FL)**\n",
        "\n",
        "---\n",
        "For consistency, the exact same dataset (CIFAR-10) and CNN model (`ConvNet`) will be used in this assignment as in Assignment 1. *Code Cell 1.1* creates the CIFAR-10 training and testing datasets. Additionally, it also contains the CNN (`ConvNet`)  that will be used throughout the assignment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9WL6HA_Lpe8",
        "colab_type": "code",
        "outputId": "92dd5289-d5d6-4404-ea1b-e7f6b40cfbe0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120,
          "referenced_widgets": [
            "ed86822fea9148689a7d2f4b6d2c09c5",
            "ec2dc77ebb4b4f64b299fa315d5f71a0",
            "24f5efe4f2dc4aed87779c5ceb87d0dd",
            "a8c6431d919f4b6390d52d678eff3da6",
            "27698cbff0ff436abb4cf6dbf77a225c",
            "e335e6106d334bcc95815e699a8799b6",
            "9e4f2e62d2b8435a99ede1936a4841d5",
            "068da22ba7894f5fb40efcbbbfa4636a"
          ]
        }
      },
      "source": [
        "## Code Cell 1.1\n",
        "\n",
        "import time\n",
        "import copy\n",
        "import sys\n",
        "from collections import OrderedDict\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Using CIFAR-10 again as in Assignment 1\n",
        "# Load training data\n",
        "transform_train = transforms.Compose([                                   \n",
        "    transforms.RandomCrop(32, padding=4),                                       \n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, \n",
        "                                        download=True,\n",
        "                                        transform=transform_train)\n",
        "\n",
        "# Load testing data\n",
        "transform_test = transforms.Compose([                                           \n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True,\n",
        "                                       transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False,\n",
        "                                         num_workers=2)\n",
        "\n",
        "\n",
        "# Using same ConvNet as in Assignment 1\n",
        "def conv_block(in_channels, out_channels, kernel_size=3, stride=1,\n",
        "               padding=1):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding,\n",
        "                  bias=False),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvNet, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            conv_block(3, 32),\n",
        "            conv_block(32, 32),\n",
        "            conv_block(32, 64, stride=2),\n",
        "            conv_block(64, 64),\n",
        "            conv_block(64, 64),\n",
        "            conv_block(64, 128, stride=2),\n",
        "            conv_block(128, 128),\n",
        "            conv_block(128, 256),\n",
        "            conv_block(256, 256),\n",
        "            nn.AdaptiveAvgPool2d(1)\n",
        "            )\n",
        "\n",
        "        self.classifier = nn.Linear(256, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.model(x)\n",
        "        B, C, _, _ = h.shape\n",
        "        h = h.view(B, C)\n",
        "        return self.classifier(h)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ed86822fea9148689a7d2f4b6d2c09c5",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjiB2IYp7B25",
        "colab_type": "text"
      },
      "source": [
        "**Federated Learning Overview**\n",
        "\n",
        "Federated Learning (FL) distributes the task of training a deep neural network (such as our CNN `ConvNet`) across multiple clients (each with a device). Each client has their own private data that they do not want to share with a central server. Therefore, instead of transmitting data, clients perform training locally and send the updated model parameters (e.g., convolutional weights) to the server. The server averages these parameters across multiple clients to update the centralized model. Finally, after the centralized model has been updated, the server sends the new version of the model to all clients.\n",
        "\n",
        "The figure below depicts this Federated Learning paradigm (taken from [Towards Federated Learning at Scale: System Design](https://arxiv.org/pdf/1902.01046.pdf)). At the beginning of a training round in the selection phase, a percentage of devices (i.e., clients) agree to participate.  By agreeing to participate, the client agrees to perform local training on its own dataset that resides on the device. During the configuration phase, the up-to-date centralized model is sent to the participating clients, which then perform local training. In the reporting phase, each client sends their updated model (trained on local data) to the server for aggregation. Note that in the figure, one of the clients fails to report back to the central server (either due to device or network failure). In this programming assignment, for simplicity we will assume that this type of device/network failure is not possible. \n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?id=1y8HAIxtNaZVLWetXHEzJ4UXWJ0yX_Jo0' />\n",
        "</figure>\n",
        "\n",
        "\n",
        "**Simulating Federated Learning**\n",
        "\n",
        "In this assignment, we will simulate this distributed Federated Learning environment on a single machine (a Colab instance). Each `device` will own a non-overlapping subset (or partition) of the dataset (e.g., 10% of the CIFAR-10 training set) and use it to train a local version of the model. The main difference between this simulated environment and a real system is the lack of networking between devices.\n",
        "\n",
        "You will use the `DatasetSplit` class in *Code Cell 1.2* to create subsets of the full training dataset. The `create_device` function creates a unique instance of `ConvNet`, an instance of the `DatasetSplit` dataloader, and an optimizer and scheduler for training. This function will be called multiple times (once per device) in order to create all the required device instances used for Federated Learning. The `train` and `test` functions are a modified version from Assignment 1 that take a device argument (the output from `create_device`).  The batch size during training is set to 128 throughout the assignment. This is passed into the `create_device` function as a default parameter value (i.e., `batch_size=128`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSx1GxV2j0iI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Code Cell 1.2\n",
        "\n",
        "class DatasetSplit(torch.utils.data.Dataset):\n",
        "    def __init__(self, dataset, idxs):\n",
        "        self.dataset = dataset\n",
        "        self.idxs = [int(i) for i in idxs]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idxs)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        image, label = self.dataset[self.idxs[item]]\n",
        "        return image, torch.tensor(label)\n",
        "  \n",
        "def create_device(net, device_id, trainset, data_idxs, lr=0.1,\n",
        "                  milestones=None, batch_size=128):\n",
        "    if milestones == None:\n",
        "        milestones = [25, 50, 75]\n",
        "\n",
        "    device_net = copy.deepcopy(net)\n",
        "    optimizer = torch.optim.SGD(device_net.parameters(), lr=lr, momentum=0.9,\n",
        "                                weight_decay=5e-4)\n",
        "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,\n",
        "                                                     milestones=milestones,\n",
        "                                                     gamma=0.1)\n",
        "    device_trainset = DatasetSplit(trainset, data_idxs)\n",
        "    device_trainloader = torch.utils.data.DataLoader(device_trainset,\n",
        "                                                     batch_size=batch_size,\n",
        "                                                     shuffle=True,\n",
        "                                                     num_workers=2)\n",
        "    return {\n",
        "        'net': device_net,\n",
        "        'id': device_id,\n",
        "        'dataloader': device_trainloader, \n",
        "        'optimizer': optimizer,\n",
        "        'scheduler': scheduler,\n",
        "        'train_loss_tracker': [],\n",
        "        'train_acc_tracker': [],\n",
        "        'test_loss_tracker': [],\n",
        "        'test_acc_tracker': [],\n",
        "        }\n",
        "  \n",
        "def train(epoch, device):\n",
        "    net.train()\n",
        "    train_loss, correct, total = 0, 0, 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(device['dataloader']):\n",
        "        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "        device['optimizer'].zero_grad()\n",
        "        outputs = device['net'](inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        device['optimizer'].step()\n",
        "        train_loss += loss.item()\n",
        "        device['train_loss_tracker'].append(loss.item())\n",
        "        loss = train_loss / (batch_idx + 1)\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "        acc = 100. * correct / total\n",
        "        dev_id = device['id']\n",
        "        sys.stdout.write(f'\\r(Device {dev_id}/Epoch {epoch}) ' + \n",
        "                         f'Train Loss: {loss:.3f} | Train Acc: {acc:.3f}')\n",
        "        sys.stdout.flush()\n",
        "    device['train_acc_tracker'].append(acc)\n",
        "    sys.stdout.flush()\n",
        "\n",
        "def test(epoch, device):\n",
        "    net.eval()\n",
        "    test_loss, correct, total = 0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.cuda(), targets.cuda()\n",
        "            outputs = device['net'](inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            test_loss += loss.item()\n",
        "            device['test_loss_tracker'].append(loss.item())\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "            loss = test_loss / (batch_idx + 1)\n",
        "            acc = 100.* correct / total\n",
        "    sys.stdout.write(f' | Test Loss: {loss:.3f} | Test Acc: {acc:.3f}\\n')\n",
        "    sys.stdout.flush()  \n",
        "    acc = 100.*correct/total\n",
        "    device['test_acc_tracker'].append(acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTEyxu91KNl-",
        "colab_type": "text"
      },
      "source": [
        "**Single Device Scenario**\n",
        "\n",
        "Before implementing Federated Learning, we will train a model for a single client device using only local data without sending updates to a central server. By doing this, the device is only able to look at a small percentage of the CIFAR-10 training set (10% in this case), and should perform poorly.\n",
        "\n",
        "---\n",
        "<font color='red'>**PART 1.1:**</font> [5 points]\n",
        "\n",
        "<font color='red'>**Deliverables**</font>\n",
        "1. In *Code Cell 1.3*, implement the `iid_sampler` function to generate **iid** (independent and identically distributed) samples from the CIFAR-10 training set. We will use this function to generate training subsets for multiple devices in PART 1.2.\n",
        "2. In *Code Cell 1.4*, create a single device using the `create_device` function. This device should have 10% of the CIFAR-10 training set using the `iid_sampler` function.\n",
        "3. Train the model for 1000 epochs using the specified parameters in *Code Cell 1.4* (similar to Assignment 1). The number of epochs is 10x greater due to the single device having 10x less data. Plot the test accuracy (`device['test_acc_tracker']`) and comment on the classification accuracy compared to using 100% of the dataset as in Assignment 1.\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-t76gDhjr-Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Code Cell 1.3\n",
        "import random #to use the random.sample method\n",
        "\n",
        "def iid_sampler(dataset, num_devices, data_pct):\n",
        "    '''\n",
        "    dataset: PyTorch Dataset (e.g., CIFAR-10 training set)\n",
        "    num_devices: integer number of devices to create subsets for\n",
        "    data_pct: percentage of training samples to give each device\n",
        "              e.g., 0.1 represents 10%\n",
        "\n",
        "    return: a dictionary of the following format:\n",
        "      {\n",
        "        0: [3, 65, 2233, ..., 22] // device 0 sample indexes\n",
        "        1: [0, 2, 4, ..., 583] // device 1 sample indexes\n",
        "        ...\n",
        "      }\n",
        "\n",
        "    iid (independent and identically distributed) means that the indexes\n",
        "    should be drawn independently in a uniformly random fashion.\n",
        "    '''\n",
        "\n",
        "    # total number of samples in the dataset\n",
        "    total_samples = len(dataset)\n",
        "\n",
        "    # Part 1.1: Implement!\n",
        "    arr = [i for i in range(total_samples)] #create an arrray of length total_samples\n",
        "    d = {} #initialize the dictonary\n",
        "    for i in range(num_devices): #for every device\n",
        "        d[i] = random.sample(list(arr), k=round(data_pct*total_samples)) #select data_pct*total_samples from the array, without replacement\n",
        "    return d #return the dictionary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifc4z-2Q8ab2",
        "colab_type": "text"
      },
      "source": [
        "Now, perform training using a single device on a subset of the training dataset using your `iid_sampler`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLd3fFeS8VEN",
        "colab_type": "code",
        "outputId": "9bba4ea7-48b9-4c09-a910-c4898dc26a0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        }
      },
      "source": [
        "## Code Cell 1.4\n",
        "\n",
        "data_pct = 0.1\n",
        "epochs = 1000\n",
        "num_devices = 1\n",
        "device_pct = 0.1\n",
        "net = ConvNet().cuda()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Part 1.1: Implement cifar_idd to generate data_idxs for create_device\n",
        "data_idxs = iid_sampler(trainset, num_devices, data_pct)\n",
        "\n",
        "# Part 1.1: Create the device\n",
        "device = create_device(net, 0, trainset, data_idxs[0],\n",
        "                       milestones=[250, 500, 750])\n",
        "\n",
        "# Part 1.1: Train the device model for 100 epochs and plot the result\n",
        "# Standard Training Loop\n",
        "start_time = time.time()\n",
        "for epoch in range(epochs):\n",
        "    train(epoch, device)\n",
        "    # To speed up running time, only evaluate the test set every 10 epochs\n",
        "    if epoch > 0 and epoch % 10 == 0:\n",
        "        test(epoch, device)\n",
        "    device['scheduler'].step()\n",
        "\n",
        "\n",
        "total_time = time.time() - start_time\n",
        "print('Total training time: {} seconds'.format(total_time))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(Device 0/Epoch 5) Train Loss: 1.664 | Train Acc: 36.719"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-a5c6ef6904fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0;31m# To speed up running time, only evaluate the test set every 10 epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-16347caea53d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, device)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dataloader'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mdevice\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'optimizer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    806\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 808\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    809\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    759\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 911\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    912\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m                 \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4waD2ZlQI4X2",
        "colab_type": "text"
      },
      "source": [
        "**Implementing Components for Federated Learning**\n",
        "\n",
        "In PART 1.1, you implemented an `iid_sampler`, created a 10% subset of the CIFAR-10 training set, and used it to train a single client device model. Since the client only had a 10% subset of the full CIFAR-10 training set, it performed significantly worse than the same model trained on 100% of the training set.  \n",
        "\n",
        "Federated Learning aims to improve the performance of these client devices by averaging the updates from multiple clients over the course of training. This way, a centralized server is able to be updated using the training data stored on local devices without having access to the training data.  By using more client devices, you will be able to get access to the original entire dataset.  In a way, this is simulating the traditional gradient descent, but with additional epochs performed on each client before averaging, where each epoch uses minibatches of size 128.  An additional benefit of federated learning is that the centralized server does not require large compute resources, as most of the training computation is performed on local devices. This makes the training computation \"free\" for the centralized server, as the clients are paying for the compute cost on their local devices.\n",
        "\n",
        "---\n",
        "<font color='red'>**PART 1.2:**</font> [10 points]\n",
        "\n",
        "Before implementing Federated Learning, you must implement two functions which will be used during the training process.\n",
        "\n",
        "The `average_weights` function takes in multiple device models, and computes the average for each model parameter across all models. This function will be called by the centralized server to aggregate the training performed by the end user devices.  This averaging is done in 32-bit floating point.\n",
        "\n",
        "The `get_devices_for_round` function will be used to simulate the device rejection phase shown earlier in the figure in the **Federated Learning Overview** section. This function will select a percentage of devices to participate in each training round.\n",
        "\n",
        "<font color='red'>**Deliverables**</font>\n",
        "1. In *Code Cell 1.5*, implement the `average_weights` function. We have provided test code which you can use to validate your implementation. This test code will also be useful for the full implementation of Federated Learning in PART 1.3.\n",
        "2. In *Code Cell 1.5*, implement the `get_devices_for_round` function. Try multiple `device_pct` settings to ensure that it is working properly.\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6X0fTWKg6hBY",
        "colab_type": "code",
        "outputId": "669fef29-c95e-4368-8449-64eb86c69266",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "## Code Cell 1.5\n",
        "\n",
        "\n",
        "def average_weights(devices):\n",
        "    '''\n",
        "    devices: a list of devices generated by create_devices\n",
        "    Returns an the average of the weights.\n",
        "    '''\n",
        "    # Part 1.2: Implement!\n",
        "    # Hint: device['net'].state_dict() will return an OrderedDict of all\n",
        "    #       tensors in the model. Return the average of each tensor using\n",
        "    #       and OrderedDict so that you can update the global model using\n",
        "    #       device['net'].load_state_dict(w_avg), where w_avg is the \n",
        "    #       averaged OrderedDict over all devices\n",
        "    global_tensors = copy.deepcopy(devices[0]['net'].state_dict()) #initialize a global tensor with the weights of the first device\n",
        "    for i in range(1, len(devices)):#iterate over the remaining devices\n",
        "        #for easy/ less complicated referencing, store the device and the state_dict\n",
        "        d = devices[i]\n",
        "        d_tensors = d['net'].state_dict()\n",
        "        for j in global_tensors.keys(): #add the tensors together by the key they are indexed by\n",
        "            global_tensors[j] += d_tensors[j]\n",
        "    for j in global_tensors.keys(): #average each tensor by the number of devices\n",
        "        global_tensors[j] = global_tensors[j]/len(devices)\n",
        "    return global_tensors #return the averaged weights\n",
        "\n",
        "\n",
        "def get_devices_for_round(devices, device_pct):\n",
        "    '''\n",
        "    This function will select a percentage of devices to participate in each training round.\n",
        "    '''\n",
        "    # Part 1.2: Implement!\n",
        "    #randomly choose device_pct*len(devices) devices from the devices array without replacement\n",
        "    arr = random.sample(devices, k=round(device_pct*len(devices)))\n",
        "    return arr\n",
        "\n",
        "# Test code for average_weights\n",
        "# Hint: This test may be useful for Part 1.3!\n",
        "class TestNetwork(nn.Module):\n",
        "    '''\n",
        "    A simple 2 layer MLP used for testing your average_weights implementation.\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        super(TestNetwork, self).__init__()\n",
        "        self.layer1 = nn.Linear(2, 2)\n",
        "        self.layer2 = nn.Linear(2, 4)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        h = F.relu(self.layer1(x))\n",
        "        return self.layer2(h)\n",
        "\n",
        "data_pct = 0.05\n",
        "num_devices = 2\n",
        "net = TestNetwork()\n",
        "data_idxs = iid_sampler(trainset, num_devices, data_pct)\n",
        "devices = [create_device(net, i, trainset, data_idxs[i])\n",
        "           for i in range(num_devices)]\n",
        "\n",
        "# Fixed seeding to compare against precomputed correct_weight_averages below\n",
        "torch.manual_seed(0)\n",
        "devices[0]['net'].layer1.weight.data.normal_()\n",
        "devices[0]['net'].layer1.bias.data.normal_()\n",
        "devices[0]['net'].layer2.weight.data.normal_()\n",
        "devices[0]['net'].layer2.bias.data.normal_()\n",
        "devices[1]['net'].layer1.weight.data.normal_()\n",
        "devices[1]['net'].layer1.bias.data.normal_()\n",
        "devices[1]['net'].layer2.weight.data.normal_()\n",
        "devices[1]['net'].layer2.bias.data.normal_()\n",
        "\n",
        "# Precomputed correct averages\n",
        "correct_weight_averages = OrderedDict(\n",
        "    [('layer1.weight', torch.tensor([[ 0.3245, -0.9013], [-0.9042,  1.0125]])),\n",
        "     ('layer1.bias', torch.tensor([-0.0724, -0.3119])),\n",
        "     ('layer2.weight', torch.tensor([[0.2976,  1.0509], [-1.0048, -0.5972],\n",
        "                                     [-0.3088, -0.2682], [-0.1690, -0.1060]])),\n",
        "     ('layer2.bias', torch.tensor([-0.4396,  0.3327, -1.3925,  0.3160]))\n",
        "    ])\n",
        "\n",
        "# Computed weight averages\n",
        "computed_weight_averages = average_weights(devices)\n",
        "\n",
        "mismatch_found = False\n",
        "for correct, computed in zip(correct_weight_averages.items(),\n",
        "                             computed_weight_averages.items()):\n",
        "    if not torch.allclose(correct[1], computed[1], atol=1e-2):\n",
        "        mismatch_found = True\n",
        "        print('Mismatch in tensor:', correct[0])\n",
        "        print(correct[1], computed[1]) #our debugging\n",
        "\n",
        "if not mismatch_found:\n",
        "    print('Implementation output matches!')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Implementation output matches!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgzNJfpM4kUj",
        "colab_type": "text"
      },
      "source": [
        "**Federated Learning Training**\n",
        "\n",
        "---\n",
        "<font color='red'>**PART 1.3:**</font> [10 points]\n",
        "\n",
        "We will now run the federated learning in the iid setting using the functions you wrote previously in this section.  The parameters are given to you in the code.  You will use 100 rounds of federated learning updates.  For each round, each device that participates in a given round will complete four epochs of local training.  10% of devices should participate in each round selected from the `get_devices_for_round` function you wrote previously.  Note that we use a static initialization for the models between all parts of the assignment.\n",
        "\n",
        "<font color='red'>**Deliverables**</font>\n",
        "1. In *Code Cell 1.6*, train a global model via federated learning.  Much of the code has been given to you, but you will need to fill in the parts using calls to the functions you wrote above.  \n",
        "\n",
        "2. Graph the accuracy of the global model over 100 rounds.  Discuss the accuracy difference between the global model trained here and the individual local model you trained in PART 1.1.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8OZxTJK4m8S",
        "colab_type": "code",
        "outputId": "ad898d84-01f5-4c48-c231-ce7970770334",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        }
      },
      "source": [
        "## Code Cell 1.6\n",
        "\n",
        "# use these parameters\n",
        "rounds = 100\n",
        "local_epochs = 4\n",
        "num_devices = 50\n",
        "device_pct = 0.1\n",
        "data_pct = 0.1\n",
        "net = ConvNet().cuda()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "data_idxs = iid_sampler(trainset, num_devices, data_pct)\n",
        "\n",
        "# Part 1.3: Implement device creation here\n",
        "devices = [create_device(net, i, trainset, data_idxs[i])\n",
        "           for i in range(num_devices)]\n",
        "\n",
        "## IID Federated Learning\n",
        "start_time = time.time()\n",
        "for round_num in range(rounds):\n",
        "  \n",
        "    # Part 1.3: Implement getting devices for each round here\n",
        "    round_devices = get_devices_for_round(devices, device_pct)\n",
        "\n",
        "    print('Round: ', round_num)\n",
        "    for device in round_devices:\n",
        "        for local_epoch in range(local_epochs):\n",
        "            train(local_epoch, device)\n",
        "\n",
        "    # Part 1.3: Implement weight averaging here\n",
        "    w_avg = average_weights(round_devices)\n",
        "\n",
        "    for device in devices:\n",
        "        device['net'].load_state_dict(w_avg)\n",
        "        device['optimizer'].zero_grad()\n",
        "        device['optimizer'].step()\n",
        "        device['scheduler'].step()\n",
        "\n",
        "    # test accuracy after aggregation\n",
        "    test(round_num, devices[0])\n",
        "\n",
        "\n",
        "total_time = time.time() - start_time\n",
        "print('Total training time: {} seconds'.format(total_time))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Round:  0\n",
            "(Device 11/Epoch 1) Train Loss: 2.122 | Train Acc: 21.094"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-35a050afeff0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mround_devices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlocal_epoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# Part 1.3: Implement weight averaging here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-16347caea53d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, device)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dataloader'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mdevice\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'optimizer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    806\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 808\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    809\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    759\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 911\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    912\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m                 \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KNmDbPh5b0h",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "\n",
        "### **2. Non-IID Federated Learning and Fairness**\n",
        "\n",
        "---\n",
        "**Overview**\n",
        "\n",
        "In PART 1, you implemented a Federated Learning pipeline that operated over iid data.  While this iid assumption may hold in some applications, it does not hold in many other settings.  For example, a group of similar users may have data that is fundamentally different from that of another group of users.  As a result, the data as a whole that federated learning operates over will be non-iid in nature.\n",
        "\n",
        "In PART 2 of the assignment, you will explore using Federated Learning in this non-iid setting.  In this part of the assignment, you will create groups of devices such that the inter-group data is non-iid and the intra-group data is iid.  To do this, you will re-implement many of the functions you implemented in PART 1 for this group, non-iid setting.\n",
        "\n",
        "For all experiments in this section, we fix that there are three groups.  Each group is assigned a different subset of classes in the dataset (Group 0 is assigned data from classes 0-3, Group 1 is assigned data from classes 4-6, and Group 2 is assigned data from classes 7-9).  We also fix that each group has 20 devices, although you will vary per-group participation rates in each round in PART 2.4.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yUOGnkZCbp-",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "**Non-iid Sampling**\n",
        "\n",
        "<font color='red'>**PART 2.1:**</font> [10 points]\n",
        "\n",
        "We will first start by implementing `noniid_group_sampler`, a new, non-iid group version of the `iid_sampler` you implemented in PART 1.  We will use this function to generate training subsets for multiple devices in PART 2.4.  \n",
        "\n",
        "<font color='red'>**Deliverables**</font>\n",
        "1. In *Code Cell 2.1*, implement the `noniid_group_sampler` function to generate **non-iid** samples from the CIFAR-10 training set.  As input, the function should take the dataset and number of training samples each device should be assigned.  As was the case in the `iid_sampler` you implemented previously, the function should return a dictionary where each key is the ID number of the particular device and each value is a `set` of the indexes of training samples in the dataset assigned to the device.  You may want to have the function return other data as well, depending on how you implement functions in later parts of the assignment.  We have provided the mapping stating which classes are mapped to each group.  Within a given group, you should sample the data in an iid fashion.\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VajsGz0wiL05",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Code Cell 2.1\n",
        "\n",
        "# creates noniid TRAINING datasets for each group\n",
        "def noniid_group_sampler(dataset, num_items_per_device):\n",
        "    '''\n",
        "    dataset: PyTorch Dataset (e.g., CIFAR-10 training set)\n",
        "    num_devices: integer number of devices to create subsets for\n",
        "    num_items_per_device: how many samples to assign to each device\n",
        "\n",
        "    return: a dictionary of the following format:\n",
        "      {\n",
        "        0: [3, 65, 2233, ..., 22] // device 0 sample indexes\n",
        "        1: [0, 2, 4, ..., 583] // device 1 sample indexes\n",
        "        ...\n",
        "      }\n",
        "\n",
        "    '''\n",
        "\n",
        "    # how many devices per non-iid group\n",
        "    devices_per_group = [20, 20, 20]\n",
        "\n",
        "    # label assignment per group\n",
        "    dict_group_classes = {}\n",
        "    dict_group_classes[0] = [0,1,2,3]\n",
        "    dict_group_classes[1] = [4,5,6]\n",
        "    dict_group_classes[2] = [7,8,9]\n",
        "\n",
        "    # Part 2.1: Implement!\n",
        "    #label dict stores the indexes of the dataset examples that fall into the ith group\n",
        "    label_dict = {}\n",
        "    label_dict[0] = []\n",
        "    label_dict[1] = []\n",
        "    label_dict[2] = []\n",
        "    for i in range(len(dataset)):\n",
        "        label = dataset[i][1]\n",
        "        #find which group the label belongs to and append the index of the example to the correct array\n",
        "        if(label in dict_group_classes[0]):\n",
        "            label_dict[0].append(i)\n",
        "        elif(label in dict_group_classes[1]):\n",
        "            label_dict[1].append(i)\n",
        "        else:\n",
        "            label_dict[2].append(i)\n",
        "    \n",
        "    num_devices = sum(devices_per_group)\n",
        "\n",
        "    final_dict = {} #final dict is to be returned\n",
        "    #device_group[i] returns the group of the ith device\n",
        "    device_group = [0 for i in range(devices_per_group[0])]+[1 for i in range(devices_per_group[1])]+[2 for i in range(devices_per_group[2])]\n",
        "    for i in range(num_devices):\n",
        "        group = device_group[i] #determine the group of the ith device\n",
        "        final_dict[i] = random.sample(label_dict[group], num_items_per_device) #randomly sample num_items_per_device examples without replacement\n",
        "    return final_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EY1H2bOELzX5",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "**Group-based Device Rejection**\n",
        "\n",
        "<font color='red'>**PART 2.2:**</font> [5 points]\n",
        "\n",
        "We will now implement `get_devices_for_round_GROUP`, a new group-based version of the `get_devices_for_round` you implemented in PART 1.  We will use this function in PART 2.4 to simulate the device rejection phase shown earlier on a per-group basis.  \n",
        "\n",
        "<font color='red'>**Deliverables**</font>\n",
        "1. In *Code Cell 2.2*, implement the `get_devices_for_round_GROUP` function to generate a list of devices that will participate in each round of federated learning.  The function should take as input 1) the list of all devices, and 2) how many devices from each group should participate in a given round.  It should return a list of devices that will participate in a given round.  You may want to add additional parameters to the function definition based on your implementation strategy.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akRqxN7mMjfa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Code Cell 2.2\n",
        "\n",
        "# get which devices in each group should participate in a current round\n",
        "# by explicitly saying number of each devices desired for each group \n",
        "def get_devices_for_round_GROUP(devices, device_nums, user_group_idxs=[[i for i in range(0,20)],[i for i in range(20,40)],[i for i in range(40,60)]]):\n",
        "    #  Part 2.2: Implement!\n",
        "\n",
        "    #Note: We assume devices 0-19 are group 0, 20-39 are group 1, and 40-59 are group 2\n",
        "    arr = [] #initialize the return array\n",
        "    for group in range(len(device_nums)): #iterate over all the groups\n",
        "        #randomly sample the indexes of a certain number of devices from a certain group\n",
        "        choices = random.sample(user_group_idxs[group], device_nums[group]) \n",
        "        #print(choices)\n",
        "        for i in choices:\n",
        "            arr.append(devices[i]) #append the chosen devices to the return array\n",
        "    return arr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBHBZVehNZNI",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "**Group-based Testing**\n",
        "\n",
        "<font color='red'>**PART 2.3:**</font> [5 points]\n",
        "\n",
        "We will now implement the testing functions needed to evaluate the global model learned via Federated Learning on a per-group basis.  This will require two functions.\n",
        "\n",
        "The `cifar_noniid_group_test` function divides the test dataset into three sub-datasets, one for each group.  \n",
        "\n",
        "The `test_group` function gets per-group classification accuracy for the global model.  You will likely want to start with the `test` function in Code Cell 1.2 and then modify it to work on a per-group basis. \n",
        "\n",
        "<font color='red'>**Deliverables**</font>\n",
        "1. In *Code Cell 2.3*, implement the `cifar_noniid_group_test` function to create a test dataset for each group.  It should take the full CIFAR-10 test dataset as input, and return a dictionary where each key is a group ID, and each value is a `set` of the indexes for all test samples for that group. \n",
        "\n",
        "2.  In *Code Cell 2.3*, implement the `test_group` function to output the per-group classification accuracy of the global model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99S3opJONpeW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Code Cell 2.3\n",
        "\n",
        "# creates noniid TEST datasets for each group\n",
        "def cifar_noniid_group_test(dataset):\n",
        "\n",
        "    dict_group_classes = {}\n",
        "    dict_group_classes[0] = [0,1,2,3]\n",
        "    dict_group_classes[1] = [4,5,6]\n",
        "    dict_group_classes[2] = [7,8,9]\n",
        "\n",
        "    # Part 2.3: Implement!\n",
        "    label_dict = {}\n",
        "    label_dict[0] = []\n",
        "    label_dict[1] = []\n",
        "    label_dict[2] = []\n",
        "    for i in range(len(dataset)): \n",
        "        label = dataset[i][1] #determine the label of the ith data point\n",
        "        #determine which group the label is in using dict_group_classes\n",
        "        #and append it to the correct array\n",
        "        if(label in dict_group_classes[0]):\n",
        "            label_dict[0].append(i)\n",
        "        elif(label in dict_group_classes[1]):\n",
        "            label_dict[1].append(i)\n",
        "        else:\n",
        "            label_dict[2].append(i)\n",
        "    return label_dict\n",
        "\n",
        "# gets per-group accuracy of global model\n",
        "def test_group(epoch, device, group_idxs_dict):\n",
        "    # Part 2.3: Implement!\n",
        "    # Hint: refer to test function in PART 1\n",
        "    net.eval() #turn the net into evaluaton mode\n",
        "    sys.stdout.write(' | Test accuracy: ')\n",
        "    with torch.no_grad():\n",
        "        # Iterate through the groups\n",
        "        for group in range (0, len(group_idxs_dict.keys())): #iterate over all of the groups\n",
        "            # Initialize the counters at the beginning of each group\n",
        "            test_loss, correct, total = 0, 0, 0\n",
        "            # Use DatasetSplit to generate new testloader for each group\n",
        "            new_testset = DatasetSplit(testset, group_idxs_dict[group])\n",
        "            testloader = torch.utils.data.DataLoader(new_testset, batch_size=128, shuffle=False,\n",
        "                                            num_workers=2)\n",
        "            # Code from PART 1\n",
        "            for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "                inputs, targets = inputs.cuda(), targets.cuda()\n",
        "                outputs = device['net'](inputs)\n",
        "                loss = criterion(outputs, targets)\n",
        "                test_loss += loss.item()\n",
        "                device['test_loss_tracker'].append(loss.item())\n",
        "                _, predicted = outputs.max(1)\n",
        "                total += targets.size(0)\n",
        "                correct += predicted.eq(targets).sum().item()\n",
        "            # Compute and print loss and accuracy at the end of the group\n",
        "            loss = test_loss / (batch_idx + 1)\n",
        "            acc = 100.* correct / total\n",
        "            sys.stdout.write(f'{acc:.3f} | ')\n",
        "    sys.stdout.write('\\n')\n",
        "    sys.stdout.flush()  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtkYyNF5Oo9J",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "**Federated Learning Results in Non-IID Setting**\n",
        "\n",
        "<font color='red'>**PART 2.4:**</font> [10 points]\n",
        "\n",
        "We will now run federated learning in the non-iid setting using the functions you wrote previously in this section.  We will examine two different settings.\n",
        "\n",
        "**Fair Device Participation:** Run federated learning on the CIFAR-10 dataset with the three groups.  Each group should have exactly one device participate in each round.  \n",
        "\n",
        "**Unfair Device Participation:** Run federated learning on the CIFAR-10 dataset with the three groups.  Group 0 should have five devices participate in each round, and Groups 1 and 2 should each have one device participate in each round.\n",
        "\n",
        "<font color='red'>**Deliverables**</font>\n",
        "1. In *Code Cell 2.4*, train a global model via federated learning for the group-based non-iid setting.  Much of the code has been given to you, but you will need to fill in the parts using calls to the group-based, non-iid functions you wrote above.  You will likely be able to re-use parts of the code you wrote in Part 1.3.\n",
        "\n",
        "2. Graph the per-group test accuracy over 100 rounds in the Fair Device Participation setting scenario.  Each group should have its own line in the graph.\n",
        "\n",
        "3.  Graph the per-group test accuracy over 100 rounds in the Unfair Device Participation setting scenario.  Each group should have its own line in the graph.\n",
        "\n",
        "4.  Describe the differences you see between the two scenarios.  How can you explain what you are seeing?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eia5zgWx6ew3",
        "colab_type": "code",
        "outputId": "d0700ecf-38c3-417a-bbb2-0d69b6083e1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "## Code Cell 2.4\n",
        "\n",
        "rounds = 100\n",
        "local_epochs = 4\n",
        "num_items_per_device = 5000\n",
        "device_nums = [5,1,1]\n",
        "net = ConvNet().cuda()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Part 2.4: Implement non-iid sampling\n",
        "data_idxs = noniid_group_sampler(trainset, num_items_per_device)\n",
        "\n",
        "# Part 2.4: Implement device creation here\n",
        "num_devices = 60\n",
        "devices = [create_device(net, i, trainset, data_idxs[i])\n",
        "           for i in range(num_devices)]\n",
        "\n",
        "## Non-IID Federated Learning\n",
        "start_time = time.time()\n",
        "for round_num in range(rounds):\n",
        "\n",
        "    # Part 2.4: Implement getting devices for each round here\n",
        "    round_devices = get_devices_for_round_GROUP(devices, device_nums)\n",
        "\n",
        "    print('Round: ', round_num)\n",
        "    for device in round_devices:\n",
        "        for local_epoch in range(local_epochs):\n",
        "            train(local_epoch, device)\n",
        "\n",
        "    # Part 2.4: Implement weight averaging here\n",
        "    w_avg = average_weights(round_devices)\n",
        "\n",
        "    for device in devices:\n",
        "        device['net'].load_state_dict(w_avg)\n",
        "        device['optimizer'].zero_grad()\n",
        "        device['optimizer'].step()\n",
        "        device['scheduler'].step()\n",
        "\n",
        "    # Part 2.4: Implement test accuracy here\n",
        "    group_idxs_dict = cifar_noniid_group_test(testset)\n",
        "    test_group(round_num, devices[0], group_idxs_dict)\n",
        "\n",
        "total_time = time.time() - start_time\n",
        "print('Total training time: {} seconds'.format(total_time))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Round:  0\n",
            "(Device 42/Epoch 3) Train Loss: 0.544 | Train Acc: 79.520 | Test accuracy: 59.400 | 0.000 | 0.000 | \n",
            "Round:  1\n",
            "(Device 50/Epoch 3) Train Loss: 0.586 | Train Acc: 75.380 | Test accuracy: 40.675 | 0.000 | 0.000 | \n",
            "Round:  2\n",
            "(Device 53/Epoch 3) Train Loss: 0.406 | Train Acc: 83.940 | Test accuracy: 68.300 | 0.000 | "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f6570616630>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
            "    w.join()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "AssertionError: can only join a child process\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.000 | \n",
            "Round:  3\n",
            "(Device 13/Epoch 0) Train Loss: 0.777 | Train Acc: 72.266"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f6570616630>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
            "    w.join()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "AssertionError: can only join a child process\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\r(Device 13/Epoch 0) Train Loss: 0.754 | Train Acc: 73.438"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f6570616630>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
            "    w.join()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "AssertionError: can only join a child process\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(Device 13/Epoch 1) Train Loss: 0.516 | Train Acc: 82.031"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f6570616630>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
            "    w.join()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f6570616630>>\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "Traceback (most recent call last):\n",
            "AssertionError: can only join a child process\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
            "    w.join()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\r(Device 13/Epoch 1) Train Loss: 0.598 | Train Acc: 76.953"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "AssertionError: can only join a child process\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(Device 13/Epoch 2) Train Loss: 0.478 | Train Acc: 82.812"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f6570616630>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
            "    self._shutdown_workers()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\r(Device 13/Epoch 2) Train Loss: 0.492 | Train Acc: 81.250"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
            "    w.join()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "AssertionError: can only join a child process\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(Device 13/Epoch 2) Train Loss: 0.592 | Train Acc: 76.719"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f6570616630>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
            "    w.join()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "AssertionError: can only join a child process\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(Device 13/Epoch 2) Train Loss: 0.590 | Train Acc: 76.580"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f6570616630>>\n",
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f6570616630>>\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
            "    self._shutdown_workers()\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
            "    w.join()\n",
            "    w.join()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "AssertionError: can only join a child process\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "AssertionError: can only join a child process\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(Device 8/Epoch 0) Train Loss: 0.812 | Train Acc: 68.750"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f6570616630>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
            "    w.join()\n",
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f6570616630>>\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "AssertionError: can only join a child process\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
            "    w.join()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "AssertionError: can only join a child process\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(Device 8/Epoch 1) Train Loss: 0.604 | Train Acc: 74.862"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f6570616630>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
            "    w.join()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "AssertionError: can only join a child process\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(Device 8/Epoch 1) Train Loss: 0.631 | Train Acc: 74.438"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f6570616630>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
            "    w.join()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "AssertionError: can only join a child process\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(Device 8/Epoch 1) Train Loss: 0.639 | Train Acc: 74.120"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f6570616630>>\n",
            "Traceback (most recent call last):\n",
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f6570616630>>\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
            "    w.join()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "Traceback (most recent call last):\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "AssertionError: can only join a child process\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
            "    w.join()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "AssertionError: can only join a child process\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(Device 8/Epoch 3) Train Loss: 0.654 | Train Acc: 71.484"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f6570616630>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
            "    w.join()\n",
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f6570616630>>\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "Traceback (most recent call last):\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "AssertionError: can only join a child process\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\r(Device 8/Epoch 3) Train Loss: 0.669 | Train Acc: 71.094"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "    w.join()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "AssertionError: can only join a child process\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(Device 8/Epoch 3) Train Loss: 0.592 | Train Acc: 76.300"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f6570616630>>\n",
            "Traceback (most recent call last):\n",
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f6570616630>>\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
            "Traceback (most recent call last):\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
            "    self._shutdown_workers()\n",
            "    w.join()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "    w.join()\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "AssertionError: can only join a child process\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "AssertionError: can only join a child process\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(Device 16/Epoch 0) Train Loss: 0.648 | Train Acc: 74.540"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f6570616630>>\n",
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f6570616630>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
            "Traceback (most recent call last):\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
            "    w.join()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "    w.join()\n",
            "AssertionError: can only join a child process\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "AssertionError: can only join a child process\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(Device 16/Epoch 2) Train Loss: 0.518 | Train Acc: 79.688"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f6570616630>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f6570616630>>\n",
            "    w.join()\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
            "    self._shutdown_workers()\n",
            "AssertionError: can only join a child process\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
            "    w.join()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "AssertionError: can only join a child process\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(Device 16/Epoch 2) Train Loss: 0.603 | Train Acc: 76.220"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f6570616630>>\n",
            "Traceback (most recent call last):\n",
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f6570616630>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
            "    self._shutdown_workers()\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
            "    w.join()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
            "    w.join()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "AssertionError: can only join a child process\n",
            "AssertionError: can only join a child process\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(Device 16/Epoch 3) Train Loss: 0.592 | Train Acc: 76.300"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f6570616630>>\n",
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f6570616630>>\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
            "    self._shutdown_workers()\n",
            "    w.join()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "    w.join()\n",
            "AssertionError: can only join a child process\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "AssertionError: can only join a child process\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(Device 19/Epoch 1) Train Loss: 0.657 | Train Acc: 75.781"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f6570616630>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
            "    w.join()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "AssertionError: can only join a child process\n",
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f6570616630>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
            "    w.join()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\r(Device 19/Epoch 1) Train Loss: 0.634 | Train Acc: 78.125"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "AssertionError: can only join a child process\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(Device 19/Epoch 1) Train Loss: 0.598 | Train Acc: 76.000"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f6570616630>>\n",
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f6570616630>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
            "    self._shutdown_workers()\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
            "    w.join()\n",
            "    w.join()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "AssertionError: can only join a child process\n",
            "AssertionError: can only join a child process\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(Device 19/Epoch 2) Train Loss: 0.612 | Train Acc: 75.860"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f6570616630>>\n",
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f6570616630>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
            "    self._shutdown_workers()\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
            "    w.join()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "    self._shutdown_workers()\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "AssertionError: can only join a child process\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
            "    w.join()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "AssertionError: can only join a child process\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(Device 5/Epoch 0) Train Loss: 0.744 | Train Acc: 69.922"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f6570616630>>\n",
            "Traceback (most recent call last):\n",
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f6570616630>>\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
            "    w.join()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "AssertionError: can only join a child process\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
            "    w.join()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\r(Device 5/Epoch 0) Train Loss: 0.733 | Train Acc: 70.833"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "AssertionError: can only join a child process\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(Device 5/Epoch 0) Train Loss: 0.670 | Train Acc: 73.740"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f6570616630>>\n",
            "Traceback (most recent call last):\n",
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f6570616630>>\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
            "Traceback (most recent call last):\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
            "    self._shutdown_workers()\n",
            "    w.join()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "    w.join()\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "AssertionError: can only join a child process\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "AssertionError: can only join a child process\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(Device 5/Epoch 1) Train Loss: 0.682 | Train Acc: 72.240"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f6570616630>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f6570616630>>\n",
            "    w.join()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "AssertionError: can only join a child process\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
            "    w.join()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "AssertionError: can only join a child process\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(Device 5/Epoch 3) Train Loss: 0.684 | Train Acc: 73.438"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f6570616630>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
            "    w.join()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "AssertionError: can only join a child process\n",
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f6570616630>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\r(Device 5/Epoch 3) Train Loss: 0.745 | Train Acc: 71.615"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
            "    w.join()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "AssertionError: can only join a child process\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(Device 5/Epoch 3) Train Loss: 0.644 | Train Acc: 75.360"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f6570616630>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f6570616630>>\n",
            "    self._shutdown_workers()\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
            "    w.join()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "    self._shutdown_workers()\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
            "AssertionError: can only join a child process\n",
            "    w.join()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "AssertionError: can only join a child process\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(Device 38/Epoch 0) Train Loss: 1.234 | Train Acc: 49.120"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f6570616630>>\n",
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f6570616630>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
            "    self._shutdown_workers()\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
            "    w.join()\n",
            "    w.join()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "AssertionError: can only join a child process\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "AssertionError: can only join a child process\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(Device 38/Epoch 1) Train Loss: 0.767 | Train Acc: 67.220"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f6570616630>>\n",
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f6570616630>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
            "    self._shutdown_workers()\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
            "    w.join()\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
            "    w.join()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "AssertionError: can only join a child process\n",
            "AssertionError: can only join a child process\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(Device 38/Epoch 2) Train Loss: 0.701 | Train Acc: 70.920"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f6570616630>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
            "    w.join()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "AssertionError: can only join a child process\n",
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f6570616630>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
            "    w.join()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "AssertionError: can only join a child process\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(Device 38/Epoch 3) Train Loss: 0.644 | Train Acc: 73.420"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f6570616630>>\n",
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f6570616630>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
            "    w.join()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "    w.join()\n",
            "AssertionError: can only join a child process\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "AssertionError: can only join a child process\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(Device 49/Epoch 0) Train Loss: 0.747 | Train Acc: 75.060"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f6570616630>>\n",
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f6570616630>>\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
            "    self._shutdown_workers()\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
            "    w.join()\n",
            "    w.join()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "AssertionError: can only join a child process\n",
            "AssertionError: can only join a child process\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(Device 49/Epoch 1) Train Loss: 0.390 | Train Acc: 84.600"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f6570616630>>\n",
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f6570616630>>\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
            "Traceback (most recent call last):\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
            "    w.join()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "AssertionError: can only join a child process\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
            "    w.join()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "AssertionError: can only join a child process\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(Device 49/Epoch 2) Train Loss: 0.335 | Train Acc: 87.460"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f6570616630>>\n",
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f6570616630>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
            "    self._shutdown_workers()\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
            "    w.join()\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "    w.join()\n",
            "AssertionError: can only join a child process\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
            "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
            "AssertionError: can only join a child process\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(Device 49/Epoch 3) Train Loss: 0.331 | Train Acc: 87.240 | Test accuracy: 71.700 | 0.000 | 0.000 | \n",
            "Round:  4\n",
            "(Device 43/Epoch 3) Train Loss: 0.380 | Train Acc: 85.780 | Test accuracy: 74.550 | 0.000 | 0.000 | \n",
            "Round:  5\n",
            "(Device 44/Epoch 3) Train Loss: 0.336 | Train Acc: 87.120 | Test accuracy: 73.050 | 0.000 | 0.000 | \n",
            "Round:  6\n",
            "(Device 57/Epoch 3) Train Loss: 0.254 | Train Acc: 90.440 | Test accuracy: 74.625 | 0.000 | 0.000 | \n",
            "Round:  7\n",
            "(Device 44/Epoch 3) Train Loss: 0.238 | Train Acc: 90.820 | Test accuracy: 77.000 | 0.000 | 0.000 | \n",
            "Round:  8\n",
            "(Device 53/Epoch 3) Train Loss: 0.239 | Train Acc: 92.300 | Test accuracy: 79.100 | 0.000 | 0.000 | \n",
            "Round:  9\n",
            "(Device 42/Epoch 3) Train Loss: 0.232 | Train Acc: 92.340 | Test accuracy: 79.725 | 0.100 | 0.000 | \n",
            "Round:  10\n",
            "(Device 48/Epoch 3) Train Loss: 0.144 | Train Acc: 94.520 | Test accuracy: 83.775 | 0.000 | 0.000 | \n",
            "Round:  11\n",
            "(Device 54/Epoch 3) Train Loss: 0.154 | Train Acc: 94.560 | Test accuracy: 82.700 | 0.167 | 0.033 | \n",
            "Round:  12\n",
            "(Device 47/Epoch 3) Train Loss: 0.155 | Train Acc: 94.400 | Test accuracy: 85.025 | 0.000 | 0.000 | \n",
            "Round:  13\n",
            "(Device 46/Epoch 3) Train Loss: 0.137 | Train Acc: 94.940 | Test accuracy: 83.225 | 0.267 | 0.233 | \n",
            "Round:  14\n",
            "(Device 45/Epoch 3) Train Loss: 0.154 | Train Acc: 94.540 | Test accuracy: 84.475 | 0.000 | 0.000 | \n",
            "Round:  15\n",
            "(Device 58/Epoch 3) Train Loss: 0.195 | Train Acc: 93.540 | Test accuracy: 84.525 | 1.633 | 2.600 | \n",
            "Round:  16\n",
            "(Device 45/Epoch 3) Train Loss: 0.142 | Train Acc: 94.500 | Test accuracy: 85.750 | 0.000 | 0.000 | \n",
            "Round:  17\n",
            "(Device 43/Epoch 3) Train Loss: 0.117 | Train Acc: 95.360 | Test accuracy: 86.425 | 3.267 | 2.767 | \n",
            "Round:  18\n",
            "(Device 40/Epoch 3) Train Loss: 0.113 | Train Acc: 96.000 | Test accuracy: 86.150 | 0.000 | 0.000 | \n",
            "Round:  19\n",
            "(Device 57/Epoch 3) Train Loss: 0.177 | Train Acc: 93.760 | Test accuracy: 87.000 | 4.500 | 7.233 | \n",
            "Round:  20\n",
            "(Device 42/Epoch 3) Train Loss: 0.161 | Train Acc: 93.920 | Test accuracy: 88.225 | 0.000 | 0.000 | \n",
            "Round:  21\n",
            "(Device 45/Epoch 3) Train Loss: 0.149 | Train Acc: 94.880 | Test accuracy: 87.550 | 8.733 | 9.300 | \n",
            "Round:  22\n",
            "(Device 55/Epoch 3) Train Loss: 0.210 | Train Acc: 94.700 | Test accuracy: 88.250 | 0.000 | 0.000 | \n",
            "Round:  23\n",
            "(Device 48/Epoch 3) Train Loss: 0.119 | Train Acc: 96.400 | Test accuracy: 85.100 | 11.700 | 19.000 | \n",
            "Round:  24\n",
            "(Device 59/Epoch 3) Train Loss: 0.091 | Train Acc: 96.720 | Test accuracy: 87.675 | 0.000 | 0.000 | \n",
            "Round:  25\n",
            "(Device 58/Epoch 3) Train Loss: 0.129 | Train Acc: 95.780 | Test accuracy: 89.650 | 0.267 | 0.000 | \n",
            "Round:  26\n",
            "(Device 41/Epoch 3) Train Loss: 0.114 | Train Acc: 96.560 | Test accuracy: 90.425 | 1.767 | 0.267 | \n",
            "Round:  27\n",
            "(Device 53/Epoch 3) Train Loss: 0.096 | Train Acc: 97.080 | Test accuracy: 90.575 | 4.333 | 1.433 | \n",
            "Round:  28\n",
            "(Device 46/Epoch 3) Train Loss: 0.094 | Train Acc: 97.300 | Test accuracy: 90.775 | 7.767 | 2.933 | \n",
            "Round:  29\n",
            "(Device 49/Epoch 3) Train Loss: 0.088 | Train Acc: 97.120 | Test accuracy: 90.700 | 10.600 | 5.233 | \n",
            "Round:  30\n",
            "(Device 45/Epoch 3) Train Loss: 0.100 | Train Acc: 97.520 | Test accuracy: 91.050 | 12.967 | 7.767 | \n",
            "Round:  31\n",
            "(Device 49/Epoch 3) Train Loss: 0.076 | Train Acc: 97.800 | Test accuracy: 91.125 | 14.600 | 10.367 | \n",
            "Round:  32\n",
            "(Device 54/Epoch 3) Train Loss: 0.102 | Train Acc: 97.480 | Test accuracy: 90.675 | 18.100 | 14.400 | \n",
            "Round:  33\n",
            "(Device 43/Epoch 3) Train Loss: 0.084 | Train Acc: 97.800 | Test accuracy: 91.150 | 19.733 | 17.233 | \n",
            "Round:  34\n",
            "(Device 40/Epoch 3) Train Loss: 0.097 | Train Acc: 96.700 | Test accuracy: 90.675 | 22.700 | 23.300 | \n",
            "Round:  35\n",
            "(Device 47/Epoch 3) Train Loss: 0.066 | Train Acc: 97.960 | Test accuracy: 91.000 | 19.433 | 16.867 | \n",
            "Round:  36\n",
            "(Device 41/Epoch 3) Train Loss: 0.075 | Train Acc: 97.560 | Test accuracy: 90.300 | 29.567 | 35.467 | \n",
            "Round:  37\n",
            "(Device 51/Epoch 3) Train Loss: 0.099 | Train Acc: 96.680 | Test accuracy: 91.375 | 18.133 | 9.200 | \n",
            "Round:  38\n",
            "(Device 49/Epoch 3) Train Loss: 0.058 | Train Acc: 98.500 | Test accuracy: 89.650 | 38.067 | 48.500 | \n",
            "Round:  39\n",
            "(Device 50/Epoch 3) Train Loss: 0.098 | Train Acc: 97.020 | Test accuracy: 91.525 | 17.033 | 7.233 | \n",
            "Round:  40\n",
            "(Device 59/Epoch 3) Train Loss: 0.076 | Train Acc: 97.560 | Test accuracy: 85.350 | 47.267 | 62.467 | \n",
            "Round:  41\n",
            "(Device 47/Epoch 3) Train Loss: 0.076 | Train Acc: 97.620 | Test accuracy: 91.725 | 15.100 | 4.267 | \n",
            "Round:  42\n",
            "(Device 50/Epoch 3) Train Loss: 0.080 | Train Acc: 98.100 | Test accuracy: 88.750 | 46.633 | 57.500 | \n",
            "Round:  43\n",
            "(Device 50/Epoch 3) Train Loss: 0.062 | Train Acc: 98.380 | Test accuracy: 91.650 | 21.967 | 10.833 | \n",
            "Round:  44\n",
            "(Device 40/Epoch 3) Train Loss: 0.065 | Train Acc: 97.680 | Test accuracy: 89.700 | 46.833 | 57.767 | \n",
            "Round:  45\n",
            "(Device 55/Epoch 3) Train Loss: 0.049 | Train Acc: 98.540 | Test accuracy: 91.900 | 21.500 | 10.133 | \n",
            "Round:  46\n",
            "(Device 41/Epoch 3) Train Loss: 0.055 | Train Acc: 98.300 | Test accuracy: 89.225 | 43.933 | 59.367 | \n",
            "Round:  47\n",
            "(Device 47/Epoch 3) Train Loss: 0.096 | Train Acc: 96.940 | Test accuracy: 92.250 | 30.533 | 14.233 | \n",
            "Round:  48\n",
            "(Device 50/Epoch 3) Train Loss: 0.047 | Train Acc: 98.400 | Test accuracy: 89.750 | 53.533 | 64.400 | \n",
            "Round:  49\n",
            "(Device 52/Epoch 3) Train Loss: 0.056 | Train Acc: 98.220 | Test accuracy: 92.250 | 26.733 | 13.533 | \n",
            "Round:  50\n",
            "(Device 59/Epoch 3) Train Loss: 0.114 | Train Acc: 97.940 | Test accuracy: 92.000 | 47.467 | 40.833 | \n",
            "Round:  51\n",
            "(Device 46/Epoch 3) Train Loss: 0.110 | Train Acc: 98.060 | Test accuracy: 92.100 | 51.333 | 42.400 | \n",
            "Round:  52\n",
            "(Device 45/Epoch 3) Train Loss: 0.111 | Train Acc: 97.700 | Test accuracy: 92.150 | 54.100 | 44.733 | \n",
            "Round:  53\n",
            "(Device 54/Epoch 3) Train Loss: 0.110 | Train Acc: 97.860 | Test accuracy: 92.150 | 56.933 | 45.333 | \n",
            "Round:  54\n",
            "(Device 48/Epoch 3) Train Loss: 0.116 | Train Acc: 97.760 | Test accuracy: 92.250 | 57.733 | 46.967 | \n",
            "Round:  55\n",
            "(Device 40/Epoch 3) Train Loss: 0.107 | Train Acc: 97.580 | Test accuracy: 92.275 | 58.667 | 47.000 | \n",
            "Round:  56\n",
            "(Device 55/Epoch 3) Train Loss: 0.098 | Train Acc: 98.040 | Test accuracy: 92.225 | 59.700 | 49.300 | \n",
            "Round:  57\n",
            "(Device 40/Epoch 3) Train Loss: 0.105 | Train Acc: 97.800 | Test accuracy: 92.100 | 60.667 | 50.833 | \n",
            "Round:  58\n",
            "(Device 48/Epoch 3) Train Loss: 0.116 | Train Acc: 97.780 | Test accuracy: 92.025 | 61.167 | 51.133 | \n",
            "Round:  59\n",
            "(Device 47/Epoch 3) Train Loss: 0.094 | Train Acc: 98.180 | Test accuracy: 92.075 | 61.833 | 52.733 | \n",
            "Round:  60\n",
            "(Device 44/Epoch 3) Train Loss: 0.097 | Train Acc: 97.740 | Test accuracy: 91.925 | 62.633 | 50.533 | \n",
            "Round:  61\n",
            "(Device 52/Epoch 3) Train Loss: 0.103 | Train Acc: 97.680 | Test accuracy: 91.950 | 63.833 | 54.600 | \n",
            "Round:  62\n",
            "(Device 51/Epoch 3) Train Loss: 0.129 | Train Acc: 97.940 | Test accuracy: 92.075 | 65.167 | 54.600 | \n",
            "Round:  63\n",
            "(Device 58/Epoch 3) Train Loss: 0.119 | Train Acc: 97.820 | Test accuracy: 92.175 | 65.300 | 57.933 | \n",
            "Round:  64\n",
            "(Device 54/Epoch 3) Train Loss: 0.092 | Train Acc: 98.080 | Test accuracy: 92.025 | 64.967 | 54.967 | \n",
            "Round:  65\n",
            "(Device 54/Epoch 3) Train Loss: 0.091 | Train Acc: 98.080 | Test accuracy: 92.025 | 63.967 | 56.433 | \n",
            "Round:  66\n",
            "(Device 49/Epoch 3) Train Loss: 0.091 | Train Acc: 98.180 | Test accuracy: 91.950 | 65.133 | 57.133 | \n",
            "Round:  67\n",
            "(Device 56/Epoch 3) Train Loss: 0.120 | Train Acc: 97.860 | Test accuracy: 91.900 | 63.800 | 56.233 | \n",
            "Round:  68\n",
            "(Device 41/Epoch 3) Train Loss: 0.097 | Train Acc: 98.440 | Test accuracy: 91.925 | 65.167 | 60.267 | \n",
            "Round:  69\n",
            "(Device 49/Epoch 3) Train Loss: 0.128 | Train Acc: 98.060 | Test accuracy: 91.925 | 67.433 | 61.300 | \n",
            "Round:  70\n",
            "(Device 46/Epoch 3) Train Loss: 0.091 | Train Acc: 98.180 | Test accuracy: 91.800 | 67.967 | 60.567 | \n",
            "Round:  71\n",
            "(Device 54/Epoch 3) Train Loss: 0.081 | Train Acc: 98.340 | Test accuracy: 91.875 | 68.267 | 60.333 | \n",
            "Round:  72\n",
            "(Device 40/Epoch 3) Train Loss: 0.092 | Train Acc: 97.920 | Test accuracy: 91.825 | 68.300 | 61.567 | \n",
            "Round:  73\n",
            "(Device 46/Epoch 3) Train Loss: 0.090 | Train Acc: 98.420 | Test accuracy: 91.825 | 68.633 | 61.967 | \n",
            "Round:  74\n",
            "(Device 57/Epoch 3) Train Loss: 0.087 | Train Acc: 98.280 | Test accuracy: 91.875 | 69.500 | 63.433 | \n",
            "Round:  75\n",
            "(Device 46/Epoch 3) Train Loss: 0.163 | Train Acc: 96.700 | Test accuracy: 91.900 | 73.000 | 68.167 | \n",
            "Round:  76\n",
            "(Device 46/Epoch 3) Train Loss: 0.163 | Train Acc: 96.980 | Test accuracy: 91.825 | 74.067 | 69.933 | \n",
            "Round:  77\n",
            "(Device 57/Epoch 3) Train Loss: 0.174 | Train Acc: 96.580 | Test accuracy: 91.775 | 75.467 | 70.567 | \n",
            "Round:  78\n",
            "(Device 43/Epoch 3) Train Loss: 0.164 | Train Acc: 97.060 | Test accuracy: 91.775 | 76.333 | 71.033 | \n",
            "Round:  79\n",
            "(Device 45/Epoch 3) Train Loss: 0.178 | Train Acc: 96.060 | Test accuracy: 91.775 | 76.733 | 70.900 | \n",
            "Round:  80\n",
            "(Device 42/Epoch 3) Train Loss: 0.170 | Train Acc: 96.440 | Test accuracy: 91.700 | 76.800 | 70.667 | \n",
            "Round:  81\n",
            "(Device 58/Epoch 3) Train Loss: 0.184 | Train Acc: 96.220 | Test accuracy: 91.800 | 77.100 | 71.667 | \n",
            "Round:  82\n",
            "(Device 44/Epoch 3) Train Loss: 0.173 | Train Acc: 96.940 | Test accuracy: 91.850 | 77.300 | 71.967 | \n",
            "Round:  83\n",
            "(Device 59/Epoch 3) Train Loss: 0.165 | Train Acc: 96.700 | Test accuracy: 91.900 | 77.433 | 71.567 | \n",
            "Round:  84\n",
            "(Device 41/Epoch 3) Train Loss: 0.167 | Train Acc: 97.040 | Test accuracy: 91.850 | 77.833 | 71.533 | \n",
            "Round:  85\n",
            "(Device 44/Epoch 3) Train Loss: 0.155 | Train Acc: 97.020 | Test accuracy: 91.800 | 78.100 | 72.533 | \n",
            "Round:  86\n",
            "(Device 57/Epoch 3) Train Loss: 0.183 | Train Acc: 96.680 | Test accuracy: 91.825 | 78.200 | 72.567 | \n",
            "Round:  87\n",
            "(Device 55/Epoch 3) Train Loss: 0.210 | Train Acc: 96.720 | Test accuracy: 91.825 | 78.267 | 72.800 | \n",
            "Round:  88\n",
            "(Device 51/Epoch 3) Train Loss: 0.162 | Train Acc: 96.640 | Test accuracy: 91.800 | 78.733 | 72.700 | \n",
            "Round:  89\n",
            "(Device 43/Epoch 3) Train Loss: 0.151 | Train Acc: 96.800 | Test accuracy: 91.800 | 78.733 | 73.100 | \n",
            "Round:  90\n",
            "(Device 51/Epoch 3) Train Loss: 0.159 | Train Acc: 96.760 | Test accuracy: 91.725 | 78.467 | 73.333 | \n",
            "Round:  91\n",
            "(Device 54/Epoch 3) Train Loss: 0.156 | Train Acc: 97.020 | Test accuracy: 91.825 | 78.833 | 73.433 | \n",
            "Round:  92\n",
            "(Device 47/Epoch 3) Train Loss: 0.160 | Train Acc: 97.040 | Test accuracy: 91.750 | 78.933 | 73.800 | \n",
            "Round:  93\n",
            "(Device 41/Epoch 3) Train Loss: 0.173 | Train Acc: 96.800 | Test accuracy: 91.725 | 78.833 | 73.267 | \n",
            "Round:  94\n",
            "(Device 50/Epoch 3) Train Loss: 0.157 | Train Acc: 97.160 | Test accuracy: 91.850 | 79.267 | 74.400 | \n",
            "Round:  95\n",
            "(Device 43/Epoch 3) Train Loss: 0.153 | Train Acc: 97.000 | Test accuracy: 91.750 | 79.367 | 73.800 | \n",
            "Round:  96\n",
            "(Device 47/Epoch 3) Train Loss: 0.158 | Train Acc: 96.620 | Test accuracy: 91.750 | 79.400 | 73.433 | \n",
            "Round:  97\n",
            "(Device 40/Epoch 3) Train Loss: 0.268 | Train Acc: 96.560 | Test accuracy: 91.775 | 79.400 | 74.033 | \n",
            "Round:  98\n",
            "(Device 43/Epoch 3) Train Loss: 0.155 | Train Acc: 97.000 | Test accuracy: 91.850 | 79.533 | 73.700 | \n",
            "Round:  99\n",
            "(Device 45/Epoch 3) Train Loss: 0.162 | Train Acc: 96.440 | Test accuracy: 91.825 | 79.600 | 73.633 | \n",
            "Total training time: 4620.072430849075 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQB2URXWqB6m",
        "colab_type": "text"
      },
      "source": [
        "----\n",
        "### **3. Quantization of Local Models for Reduced Communication Cost**\n",
        "-----\n",
        "Quantization refers to the process of reducing the number of bits used to represent a number. In the context of deep learning, the predominant numerical format used in research and deployment has been full precision (32-bit floating-point, [IEEE 754 Format](https://en.wikipedia.org/wiki/Single-precision_floating-point_format)). However, the desire for reduced model size and computation has led to research on using fewer bits to represent numbers in deep learning models.  This can impact a number of aspects of the pipeline, including computation, communication, and storage requirements.  For example, in the context of federated learning, quantizing a client model from full precision to 8-bit precision will reduce the model size by ~ 4Ã—.  This also reduces the storage requirements. Further, because the model size is reduced, the communication required for uploading a client model is also reduced by ~ 4Ã— as well.\n",
        "\n",
        "However, this quantization comes with trade-offs.  To see this, consider a full precision representation (32-bit floating-point).  This representation has a large dynamic range (from $-3.4\\times 10^{38}$ to $+3.4\\times10^{38}$) and high precision (about $7$ decimal digits). As a result, a full precision number can be seen as continuous data. In contrast, a n-bit fixed-point representation only has $2^n$ discrete values. As a result, n-bit representations of numbers can only be one of these $2^n$ values. n-bit quantization generally refers to projecting a full precision weight to one of these $2^n$ discrete values by finding its nearest neighbor.  \n",
        "\n",
        "--------\n",
        "<font color='red'>**PART 3.1:**</font> [5 points]\n",
        "\n",
        "In this part, we will write a function to project full precision numbers into n-bit fixed-point numbers.  For example, suppose we want to project full precision numbers in the range of $[0, 1]$ into an 8-bit fixed point representation, $\\frac{1}{2^8-1}\\times(0, 1, 2, 3, \\dots, 253, 254,255)$, where $\\frac{1}{2^8-1}$ is the **scale factor** of the 8-bit fixed point representation. \n",
        "\n",
        "<font color='red'>**Deliverables**</font>\n",
        "1. In *Code Cell 3.1*, implement a function that projects full precision numbers in the range of [0, 1] into n-bit fixed-point numbers. If your implementation is correct, it should return *'Output of Quantization Matches!'*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1D9azF-qBCF",
        "colab_type": "code",
        "outputId": "f3c8e299-e3a2-4dbd-b311-ff77cd965057",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "## Code Cell 3.1\n",
        "\n",
        "def quantizer(input, nbit):\n",
        "    '''\n",
        "    input: full precision tensor in the range [0, 1]\n",
        "    return: quantized tensor\n",
        "    '''\n",
        "    scale_factor = 1 / (2**nbit -  1)\n",
        "\n",
        "    # scale input by inverse of scale_factor and round to nearest integer\n",
        "    output = input / scale_factor\n",
        "    output = torch.round(output)\n",
        "\n",
        "    # scale rounded output back and return\n",
        "    output *= scale_factor\n",
        "    return output\n",
        "\n",
        "# Test Code\n",
        "test_data = torch.tensor([i/11 for i in range(11)])\n",
        "\n",
        "# ground truth results of 4-bit quantization\n",
        "ground_truth = torch.tensor([0.0000, 0.0667, 0.2000, 0.2667, 0.3333, 0.4667,\n",
        "                             0.5333, 0.6667, 0.7333, 0.8000, 0.9333])\n",
        "\n",
        "# output of your quantization function\n",
        "quantizer_output = quantizer(test_data, 4)\n",
        "\n",
        "if torch.allclose(quantizer_output, ground_truth, atol=1e-04):\n",
        "    print('Output of Quantization Matches!')\n",
        "else:\n",
        "    print('Output of Quantization DOES NOT Match!')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output of Quantization Matches!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrvX-UzbqY4J",
        "colab_type": "text"
      },
      "source": [
        "**Quantize Weights of Neural Networks**\n",
        "\n",
        "The quantizer in PART 3.1 will quantize any full precision number in the range of $[0, 1]$ into an n-bit fixed-point number. However, weights of neural networks, $w$, are not necessarily in the range of $[0, 1]$. \n",
        "\n",
        "To use the quantizer in PART 3.1, we will first use a scaling function to transform weights into the range of [0 ,1]:\n",
        "$$\\tilde{w} = \\frac{w}{2 \\max(|w|)} + \\frac{1}{2}$$ \n",
        "where $2 \\times\\max(|w|)$ is the **adaptive scale**.\n",
        "\n",
        "Then, we quantize the transformed weights:\n",
        "$$\\hat{w} = \\text{quantizer}_{\\text{n-bit}}(\\tilde{w})$$\n",
        "After quantization, a reverse scaling function is applied on $\\hat{w}$ to recover weights' original scale:\n",
        "$$w_q = 2\\max(|w|)\\times(\\hat{w}-\\frac{1}{2})$$\n",
        "\n",
        "Combining these three equations, the expression we will use to get the quantized weights $w_q$ is as follows:\n",
        "$$w_q = 2\\max(|w|)\\times[\\text{quantizer}_{\\text{n-bit}}(\\frac{w}{2\\max(|w|)} + \\frac{1}{2}) - \\frac{1}{2}]$$\n",
        "\n",
        "This equation is the **deterministic quantization function**. \n",
        "\n",
        "Following the method proposed by [DoReFa-Net](https://arxiv.org/abs/1606.06160), we enable *stochastic quantization* by adding extra noise $N(n) = \\frac{\\sigma}{2^n-1}$ to the transformed weights $\\tilde{w}$, where $\\sigma \\sim \\text{Uniform}(-0.5, 0.5)$ and $n$ is the number of bits. Generally, including such extra noise will encourage the model to explore more of the unexplored area in the loss surface, help the model escape local minima, and improve the model's generalizability.  \n",
        "\n",
        "The final **stochastic quantization function** we will use to quantize layers of local models is:\n",
        "\n",
        "$$w_q = 2\\max(|w|)\\times[\\text{quantizer}_{\\text{n-bit}}[\\frac{w}{2 \\times\\max(|w|)} + \\frac{1}{2} + N(n)] - \\frac{1}{2}]$$\n",
        "\n",
        "\n",
        "<font color='red'>**PART 3.2:**</font> [10 points]\n",
        "\n",
        "<font color='red'>**Deliverables**</font>\n",
        "1. In *Code Cell 3.2*, implement `dorefa_g(w, nbit, adaptive_scale=None)` with the formulation of the **stochastic quantization function** shown above. Again, if your implementation is correct, it should return *'Output of Quantization Matches!'*.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGIOhKnWqXXr",
        "colab_type": "code",
        "outputId": "e5fdd8e6-cfe0-4fc3-9159-bb5929b39741",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "## Code Cell 3.2\n",
        "\n",
        "def quantize_model(model, nbit):\n",
        "    '''\n",
        "    Used in Code Cell 3.3 to quantize the ConvNet model\n",
        "    '''\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
        "            m.weight.data, m.adaptive_scale = dorefa_g(m.weight, nbit)\n",
        "            if m.bias is not None:\n",
        "                m.bias.data,_ = dorefa_g(m.bias, nbit, m.adaptive_scale)\n",
        "\n",
        "def dorefa_g(w, nbit, adaptive_scale=None):\n",
        "    '''\n",
        "    w: a floating-point weight tensor to quantize\n",
        "    nbit: the number of bits in the quantized representation\n",
        "    adaptive_scale: the maximum scale value. if None, it is set to be the\n",
        "                    absolute maximum value in w.\n",
        "    '''\n",
        "    if adaptive_scale is None:\n",
        "        adaptive_scale = torch.max(torch.abs(w))\n",
        "    \n",
        "    # follows equations above\n",
        "    sigma = torch.rand(w.shape) - 0.5\n",
        "    noise = sigma / (2**nbit - 1)\n",
        "    # avoid type errors\n",
        "    noise = noise.type(w.type())\n",
        "    inp = w / (2*adaptive_scale) + 0.5 + noise\n",
        "    w_q = 2*adaptive_scale * (quantizer(inp, nbit) - 0.5)\n",
        "\n",
        "    return w_q, adaptive_scale\n",
        "\n",
        "\n",
        "# Test Code\n",
        "test_data = torch.tensor([i/11 for i in range(11)])\n",
        "\n",
        "# ground truth results of 4-bit quantization\n",
        "ground_truth = torch.tensor([-0.0606, 0.0606, 0.1818, 0.3030, 0.3030, 0.4242,\n",
        "                             0.5455, 0.5455, 0.7879, 0.7879, 0.9091])\n",
        "\n",
        "# output of your quantization function\n",
        "torch.manual_seed(43)\n",
        "quantizer_output, adaptive_scale = dorefa_g(test_data, 4)\n",
        "\n",
        "if torch.allclose(quantizer_output, ground_truth, atol=1e-04):\n",
        "    print('Output of Quantization Matches!')\n",
        "else:\n",
        "    print('Output of Quantization DOES NOT Match!')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output of Quantization Matches!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaaBm5RcqmMx",
        "colab_type": "text"
      },
      "source": [
        "**Reduce the Communication Overhead with Quantization**\n",
        "\n",
        "We will now explore the performance impact on federated learning using quantization.  We will use the iid-setting from PART 1. You will run the same federated learning code, but will quantize each local model with the `quantize_model` function you wrote above before uploading to the central server (*Line 27, Code Cell 3.3*).\n",
        "\n",
        "<font color='red'>**PART 3.3:**</font> [10 points]\n",
        "\n",
        "<font color='red'>**Deliverables**</font>\n",
        "1. In *Code Cell 3.3*, run federated learning with the following two different settings of quantization: `nbit = 16, 4`. Graph the accuracy of the global models over 100 rounds under different settings of bitwidth, 32 (the full precision baseline you ran previously), 16, and 4.  \n",
        "2. Discuss the accuracy difference between the global models under the three different settings of bit-width (32, 16, and 4). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUyOuVCpqrTI",
        "colab_type": "code",
        "outputId": "4e996432-2f2a-422d-8a05-21693a58c2e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        }
      },
      "source": [
        "## Code Cell 3.3\n",
        "\n",
        "# Part 3.2: Train two settings with nbit=16 and nbit=4.\n",
        "#           Compare against the floating-point performance\n",
        "#           of the final FL model trained in Part 1.3.\n",
        "nbit = 16\n",
        "\n",
        "rounds = 100\n",
        "local_epochs = 4\n",
        "num_devices = 50\n",
        "device_pct = 0.1\n",
        "data_pct = 0.1\n",
        "net = ConvNet().cuda()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "data_idxs = iid_sampler(trainset, num_devices, data_pct)\n",
        "devices = [create_device(net, i, trainset, data_idxs[i])\n",
        "           for i in range(num_devices)]\n",
        "\n",
        "## IID Federated Learning\n",
        "start_time = time.time()\n",
        "for round_num in range(rounds):\n",
        "    # Part 3.3: Implement!\n",
        "    # Hint: you can use your federated learning code from PART 1\n",
        "\n",
        "    # following PART 1\n",
        "    round_devices = get_devices_for_round(devices, device_pct)\n",
        "\n",
        "    print('Round: ', round_num)\n",
        "\n",
        "    # iterate over all devices\n",
        "    for device in round_devices:\n",
        "        for local_epoch in range(local_epochs):\n",
        "            train(local_epoch, device)\n",
        "        # after training, quantize the learned model\n",
        "        quantize_model(device['net'], nbit)\n",
        "\n",
        "    # take average over devices with already quantized models\n",
        "    w_avg = average_weights(round_devices)\n",
        "\n",
        "    for device in devices:\n",
        "        device['net'].load_state_dict(w_avg)\n",
        "        device['optimizer'].zero_grad()\n",
        "        device['optimizer'].step()\n",
        "        device['scheduler'].step()\n",
        "\n",
        "    # test accuracy after aggregation\n",
        "    test(round_num, devices[0])\n",
        "\n",
        "\n",
        "total_time = time.time() - start_time\n",
        "print('Total training time: {} seconds'.format(total_time))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Round:  0\n",
            "(Device 43/Epoch 1) Train Loss: 2.232 | Train Acc: 21.429"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-7c3d8f9bec5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mround_devices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlocal_epoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0;31m# after training, quantize the learned model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mquantize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'net'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-16347caea53d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, device)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dataloader'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mdevice\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'optimizer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0vWNB-HujyS",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "\n",
        "### **4. Extreme Anomaly Detection**\n",
        "\n",
        "In this part, you will explore the impact of malicious clients on Federated Learning. These malicious clients send fake weight updates to the centralized server in order to seriously degrade the classification accuracy of the global model and make convergence difficult to achieve during training. To mitigate the impact of these malicious clients, you will implement a secure version of the weight averaging scheme you implemented in PART 1 called `secure_average_weights`.\n",
        "\n",
        "In PART 4.1, you will simulate a group of malicious clients which participate in FL with the regular non-malicious clients. You will implement the fake weight updates (`generate_fake_weights`) used by these malicious clients to make FL unstable. Finally, you will observe the impact of these malicious clients on the classification accuracy of the global model.\n",
        "\n",
        "In PART 4.2, you will implement a secure weight update scheme (`secure_average_weights`) at the central server to detect the fake weights sent by malicious clients. Using this secure scheme, the malicious clients will be identified so that their weights are not used in the averaging step of the global model. Ultimately, this makes it so that these malicious clients do not impact the performance of the global model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXdbd1J8n0T8",
        "colab_type": "text"
      },
      "source": [
        "**Simulating Malicious Devices**\n",
        "\n",
        "In this part, you will write code to simulate malicious clients. We have slightly modify the FL training loop from PART 1 to support two different types of devices: normal (i.e., non-malicious) devices and malicious devices. We provide a function to select which devices are malicious (`get_malicious_devices`). When a malicious device is selected for a round of training, it will use fake weights drawn from a Gaussian distribution instead of actually performing local training.\n",
        "\n",
        "---\n",
        "<font color='red'>**PART 4.1:**</font> [10 points]\n",
        "\n",
        "Implement the `generate_fake_weights` function to sample fake weight values from a Gaussian distribution with a mean of zero and a standard deviation of 0.5. When a malicious device is selected for a round of training, it will use this function to send fake weights to the central server.\n",
        "\n",
        "<font color='red'>**Deliverables**</font>\n",
        "\n",
        "1. In *Code Cell 4.1*, implement the `generate_fake_weights` to generate fake weights for each layer in `ConvNet` which are used by malicious clients. \n",
        "2. Train FL using the same federated learning training settings as in PART 1.3, but with 1 out of 5 client devices used in each round being malicious.\n",
        "3. Graph the accuracy of the global model over 50 rounds (instead of 100 as in prior parts).  Discuss the accuracy difference between this model trained with the presense of malicious clients and the model from PART 1.3.  \n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASQvKnydn0oI",
        "colab_type": "code",
        "outputId": "b867397d-1971-4b20-ead8-8b7b9b8a148c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        }
      },
      "source": [
        "## Code Cell 4.1\n",
        "import torch.distributions as tdist\n",
        "\n",
        "def get_malicious_devices(devices):\n",
        "    # Creates a group of malicious devices\n",
        "    num_malicious_devices = 1\n",
        "    device_idxs = np.random.permutation(len(devices))[:num_malicious_devices]\n",
        "    return [devices[i] for i in device_idxs]\n",
        "\n",
        "def generate_fake_weights(device): \n",
        "    #define an empty dictionary to fill with the random model state tensors\n",
        "    w_mal = {}\n",
        "    for i in device['net'].state_dict().keys(): #iterate over every key in the model state dictionary for the device\n",
        "        temp = torch.ones(device['net'].state_dict()[i].shape, dtype=torch.float64) #create a temporary tensor of the correct size\n",
        "        temp.normal_(0, 0.5) #fill the tensor with data from the normal distribution, mean = 0 and sd = 0.5\n",
        "        w_mal[i] = temp.cuda() #make the tensor cuda so that it is in the right format\n",
        "    device['net'].load_state_dict(w_mal) #load the randomly generated state dict into the device\n",
        "    return device #return the device with the malicious random weights\n",
        "\n",
        "\n",
        "rounds = 50\n",
        "local_epochs = 4\n",
        "num_devices = 50\n",
        "device_pct = 0.1\n",
        "net = ConvNet().cuda()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "data_idxs = iid_sampler(trainset, num_devices, data_pct)\n",
        "devices = [create_device(net, i, trainset, data_idxs[i])\n",
        "           for i in range(num_devices)]\n",
        "\n",
        "# Part 4.1: Implement!\n",
        "# Hint: base this off of your federated learning code in PART 1\n",
        "# modified for the malicious case here.\n",
        "# We give you part of the code below, but you will likely need to plug\n",
        "# in code throughout below.\n",
        "\n",
        "start_time = time.time()\n",
        "for round_num in range(rounds):\n",
        "    \n",
        "    # 1/5 of the devices in each round are malicious\n",
        "    round_devices = get_devices_for_round(devices, device_pct)\n",
        "    malicious_devices = get_malicious_devices(round_devices)\n",
        "\n",
        "    print('Round: ', round_num)    \n",
        "    for device in round_devices:\n",
        "        if device not in malicious_devices:\n",
        "            for local_epoch in range(local_epochs):\n",
        "                train(local_epoch, device)\n",
        "        else:   # the device is malicious\n",
        "            # instead of training, get fake weights\n",
        "            device = generate_fake_weights(device) \n",
        "    \n",
        "    w_avg = average_weights(round_devices)\n",
        "\n",
        "    for device in devices:\n",
        "        device['net'].load_state_dict(w_avg)\n",
        "        device['optimizer'].zero_grad()\n",
        "        device['optimizer'].step()\n",
        "        device['scheduler'].step()\n",
        "\n",
        "    # test accuracy after aggregation\n",
        "    test(round_num, devices[0])\n",
        "\n",
        "total_time = time.time() - start_time\n",
        "print('Total training time: {} seconds'.format(total_time))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Round:  0\n",
            "(Device 23/Epoch 0) Train Loss: 2.107 | Train Acc: 21.245"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-05fe96905bd4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmalicious_devices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mlocal_epoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m                 \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m   \u001b[0;31m# the device is malicious\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;31m# instead of training, get fake weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-16347caea53d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, device)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mdevice\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'optimizer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mdevice\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_loss_tracker'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_idx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4n3Y4U0n1-a",
        "colab_type": "text"
      },
      "source": [
        "**Securing the Server against Malicious Clients**\n",
        "\n",
        "To prevent the malicious clients from degrading the global model performance observed in PART 4.1, you will implement a secure weight update scheme (`secure_average_weights`) used by the central server to detect the fake weights sent by malicious clients. Then, you will ensure that these fake weights are not used in the averaging step of the global model. This approach significantly improves the performance of the global model in the presense of malicious clients.\n",
        "\n",
        "\n",
        "---\n",
        "<font color='red'>**PART 4.2:**</font> [10 points]\n",
        "\n",
        "In *Code Cell 4.2*, implement `secure_average_weights`, which is a modified version of the `average_weights` function you implemented earlier in *Code Cell 1.5*. This `secure_average_weights` function has a simple anomaly detection algorithm using the $l_{2}$ distance between the individual weights reported by each client and the average weights over all clients. At each training round, the central server will receive a set of updated weights from $5$ clients. Let $w_{i}^{l}$ denote the layer $l$ weight tensor for client $i$. The detailed steps to implement `secure_average_weights` are as follows:\n",
        "\n",
        "\n",
        "1. Compute the normalized $l_{2}$ distance between the weights of each device ($w_i$) and the average weights across all devices ($w_{avg}$). This distance is computed across all layers $$a_{i} = \\sqrt{\\frac{\\sum_{l} ||w_{i}^{l} - w_{avg}^{l}||^{2}}{N}}$$ for each device $i$, where $N$ is the total number of parameters in the CNN, and $w_{avg}$ is defined as the output of the `average_weights` function you implemented in PART 1.2. Note that the resulting distance for a single device ($a_{i}$) is a scalar value.  \n",
        "2. Compute the average of the device weight distances $$a_{avg} = \\frac{\\sum_{i=1}^5 a_{i}}{5}$$\n",
        "3.  If $a_{i} > a_{avg} + \\epsilon$, where $\\epsilon$ is set to 0.3, then mark device $i$ as malicious, otherwise mark device $i$ as non-malicious. \n",
        "We observe through experimentation that devices with a $l_{2}$ distance greater than $a_{avg} + \\epsilon$ $(\\epsilon=0.3)$ are likely to be malicious.\n",
        "4. Recompute the average weights by including only the non-malicious devices.\n",
        "\n",
        "<font color='red'>**Deliverables**</font>\n",
        "\n",
        "1. In *Code Cell 4.2*, implement the `secure_average_weights` function using the detailed steps described above. \n",
        "2. Train FL using the same settings as in Part 4.1, but with `secure_average_weights` instead of `average_weights`.\n",
        "3. Graph the accuracy of the global model over 50 rounds. Compare against the model performance in PART 4.1.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3oKTu7qPn3xf",
        "colab_type": "code",
        "outputId": "93dabef3-6a98-46df-84a6-822a2990fab4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        }
      },
      "source": [
        "## Code Cell 4.2\n",
        "import math\n",
        "def secure_average_weights(devices):\n",
        "    \"\"\"\n",
        "    Returns the average of the weights.\n",
        "    \"\"\"\n",
        "    # Part 4.2: Implement!\n",
        "    num_devices = len(devices)\n",
        "\n",
        "    avg_weight = average_weights(devices)\n",
        "    a = [0 for i in range(num_devices)] \n",
        "    #compute L2 distance from device weights to average weight\n",
        "    for i in range(num_devices):\n",
        "        temp_counter = 0\n",
        "        device_state_dict = devices[i]['net'].state_dict()\n",
        "        N = 0\n",
        "        for k in avg_weight.keys():\n",
        "            #only use the parts of the state dict with floating point precision\n",
        "            if(device_state_dict[k].dtype == torch.float32): \n",
        "                N += 1\n",
        "                #add the L2 distance between the device's weight and the average weight for a particular key\n",
        "                temp_counter += torch.dist(device_state_dict[k], avg_weight[k], 2).item()\n",
        "        #average and square root as per the formula\n",
        "        temp_counter = temp_counter/N\n",
        "        temp_counter = math.sqrt(temp_counter)\n",
        "        a[i] = temp_counter\n",
        "\n",
        "    #compute the average of the device weight distances\n",
        "    a_avg = sum(a)/len(a)\n",
        "\n",
        "    new_devices = []\n",
        "    #Check if above the threshold\n",
        "    for i in range(num_devices):\n",
        "        #if not, append to the accepted devices\n",
        "        if(a[i] <= a_avg+0.3):\n",
        "            new_devices.append(devices[i])\n",
        "    assert (len(new_devices) == num_devices-1) #for debugging purposes, this should always be true\n",
        "\n",
        "    #return the averaged weights of the accepted/ non-malicious devices\n",
        "    return average_weights(new_devices)\n",
        "\n",
        "rounds = 50\n",
        "local_epochs = 4\n",
        "num_devices = 50\n",
        "device_pct = 0.1\n",
        "net = ConvNet().cuda()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Part 4.2: Implement!\n",
        "# You can reuse the training code you wrote in PART 4.1 but with replacing\n",
        "# average_weights with secure_average_weights\n",
        "\n",
        "data_idxs = iid_sampler(trainset, num_devices, data_pct)\n",
        "devices = [create_device(net, i, trainset, data_idxs[i])\n",
        "           for i in range(num_devices)]\n",
        "\n",
        "start_time = time.time()\n",
        "for round_num in range(rounds):\n",
        "    \n",
        "    # 1/5 of the devices in each round are malicious\n",
        "    round_devices = get_devices_for_round(devices, device_pct)\n",
        "    malicious_devices = get_malicious_devices(round_devices)\n",
        "\n",
        "    print('Round: ', round_num)    \n",
        "    for device in round_devices:\n",
        "        if device not in malicious_devices:\n",
        "            for local_epoch in range(local_epochs):\n",
        "                train(local_epoch, device)\n",
        "        else:   # the device is malicious\n",
        "            # instead of training, get fake weights\n",
        "            device = generate_fake_weights(device) \n",
        "    \n",
        "    # use new function secure_average_weights\n",
        "    malic = [0 if x in malicious_devices else 1 for x in round_devices]\n",
        "    #print (malic)\n",
        "    w_avg = secure_average_weights(round_devices)\n",
        "\n",
        "    for device in devices:\n",
        "        device['net'].load_state_dict(w_avg)\n",
        "        device['optimizer'].zero_grad()\n",
        "        device['optimizer'].step()\n",
        "        device['scheduler'].step()\n",
        "\n",
        "    # test accuracy after aggregation\n",
        "    test(round_num, devices[0])\n",
        "\n",
        "total_time = time.time() - start_time\n",
        "print('Total training time: {} seconds'.format(total_time))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Round:  0\n",
            "(Device 20/Epoch 1) Train Loss: 1.871 | Train Acc: 29.120"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-2cf7dbb2e2bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmalicious_devices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mlocal_epoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                 \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m   \u001b[0;31m# the device is malicious\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# instead of training, get fake weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-16347caea53d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, device)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dataloader'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mdevice\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'optimizer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    806\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 808\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    809\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    759\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 911\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    912\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m                 \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}